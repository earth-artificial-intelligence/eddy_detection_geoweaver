[{
  "history_id" : "k7d4m9y08cm",
  "history_input" : "# All frequently used dependency are here\n\nimport os\nimport numpy as np\nimport logging as logger\n\nfrom datetime import datetime\nfrom matplotlib import pyplot as plt\nfrom py_eddy_tracker import data\nfrom py_eddy_tracker.dataset.grid import RegularGridDataset\nfrom copy import deepcopy\n",
  "history_output" : "",
  "history_begin_time" : 1676678178045,
  "history_end_time" : 1676678182289,
  "history_notes" : null,
  "history_process" : "0ajbp0",
  "host_id" : "ycru82",
  "indicator" : "Done"
},{
  "history_id" : "2uoqutyiiwo",
  "history_input" : "from dependency import *\n\ndata_root = os.path.join(os.path.expanduser(\"~\"), \"ML_eddies\")\ntrain_folder = os.path.join(data_root, \"cds_ssh_1998-2018_10day_interval\")\ntest_folder = os.path.join(data_root, \"cds_ssh_2019_10day_interval\")\n\nexample_file = os.path.join(test_folder, \"dt_global_twosat_phy_l4_20190110_vDT2021.nc\")",
  "history_output" : "Running",
  "history_begin_time" : 1676678216699,
  "history_end_time" : 1676678219699,
  "history_notes" : null,
  "history_process" : "0ps7es",
  "host_id" : "ycru82",
  "indicator" : "Done"
},{
  "history_id" : "snicsekdys8",
  "history_input" : "#Fequently used plotting functions\n\n\nimport os.path\nfrom dependency import plt\n\n\n\n\ndef start_axes(title):\n    fig = plt.figure(figsize=(13, 5))\n    ax = fig.add_axes([0.03, 0.03, 0.90, 0.94])\n    ax.set_aspect(\"equal\")\n    ax.set_title(title, weight=\"bold\")\n    return ax, fig\n\n\ndef update_axes(ax, mappable=None):\n    ax.grid()\n    if mappable:\n        plt.colorbar(mappable, cax=ax.figure.add_axes([0.94, 0.05, 0.01, 0.9]))\n\n\n\n\ndef plot_variable(grid_object, var_name, ax_title, **kwargs):\n    ax,fig = start_axes(ax_title)\n    m = grid_object.display(ax, var_name, **kwargs)\n    update_axes(ax, m)\n    ax.set_xlim(grid_object.x_c.min(), grid_object.x_c.max())\n    ax.set_ylim(grid_object.y_c.min(), grid_object.y_c.max())\n    return ax, m, fig\n\ndef save_fig_and_relesase_memory(ax, m, fig):\n    # TODO: change the function to account for a relevant name\n    fig.savefig( os.path.join(\"/home/chetana/ML_eddies/plots/\", \"0.png\"))\n    ax.cla()\n    plt.close('all')\n\n",
  "history_output" : "",
  "history_begin_time" : 1676678182866,
  "history_end_time" : 1676678187000,
  "history_notes" : null,
  "history_process" : "ag4g86",
  "host_id" : "ycru82",
  "indicator" : "Done"
},{
  "history_id" : "itshf2v9kgf",
  "history_input" : "# sea surface height (SSH preprocessing)\n\n\nfrom dependency import *\nfrom plot_utils import plot_variable, save_fig_and_relesase_memory\nfrom data_loader import example_file\n\n\ndate = datetime(2019, 1, 1)\ng = RegularGridDataset(example_file, \"longitude\", \"latitude\")\n\nax, m, fig = plot_variable(\n    g,\n    \"adt\",\n    f\"ADT (m) before high-pass filter\",\n    vmin=-0.15,\n    vmax=0.15,\n)\n\nsave_fig_and_relesase_memory(ax, m, fig)\n\nwavelength_km = 700\ng_filtered = deepcopy(g)\ng_filtered.bessel_high_filter(\"adt\", wavelength_km)\n\nax, m, fig = plot_variable(\n    g_filtered,\n    \"adt\",\n    f\"ADT (m) filtered (Final: {wavelength_km} km)\",\n    vmin=-0.15,\n    vmax=0.15,\n)\n\nsave_fig_and_relesase_memory(ax, m, fig)\n\n",
  "history_output" : "We assume pixel position of grid is centered for /home/chetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190110_vDT2021.nc\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/itshf2v9kgf/ssh_map_preprocessing.py\", line 20, in <module>\n    save_fig_and_relesase_memory(ax, m, fig)\n  File \"/home/chetana/gw-workspace/itshf2v9kgf/plot_utils.py\", line 36, in save_fig_and_relesase_memory\n    fig.savefig( os.path.join(\"/home/chetana/ML_eddies/plots/\", \"0.png\"))\n  File \"/home/chetana/anaconda3/envs/ranjan/lib/python3.10/site-packages/matplotlib/figure.py\", line 3328, in savefig\n    self.canvas.print_figure(fname, **kwargs)\n  File \"/home/chetana/anaconda3/envs/ranjan/lib/python3.10/site-packages/matplotlib/backend_bases.py\", line 2362, in print_figure\n    result = print_method(\n  File \"/home/chetana/anaconda3/envs/ranjan/lib/python3.10/site-packages/matplotlib/backend_bases.py\", line 2228, in <lambda>\n    print_method = functools.wraps(meth)(lambda *args, **kwargs: meth(\n  File \"/home/chetana/anaconda3/envs/ranjan/lib/python3.10/site-packages/matplotlib/backends/backend_agg.py\", line 509, in print_png\n    self._print_pil(filename_or_obj, \"png\", pil_kwargs, metadata)\n  File \"/home/chetana/anaconda3/envs/ranjan/lib/python3.10/site-packages/matplotlib/backends/backend_agg.py\", line 458, in _print_pil\n    mpl.image.imsave(\n  File \"/home/chetana/anaconda3/envs/ranjan/lib/python3.10/site-packages/matplotlib/image.py\", line 1687, in imsave\n    image.save(fname, **pil_kwargs)\n  File \"/home/chetana/anaconda3/envs/ranjan/lib/python3.10/site-packages/PIL/Image.py\", line 2428, in save\n    fp = builtins.open(filename, \"w+b\")\nFileNotFoundError: [Errno 2] No such file or directory: '/home/chetana/ML_eddies/plots/0.png'\n",
  "history_begin_time" : 1676678220916,
  "history_end_time" : 1676678226572,
  "history_notes" : null,
  "history_process" : "nzlslh",
  "host_id" : "ycru82",
  "indicator" : "Failed"
},{
  "history_id" : "549j0f0s5av",
  "history_input" : "# Process data to generate ground truth using py-eddy-tracker\n\nfrom dependency import *\nfrom plot_utils import *\nfrom matplotlib.path import Path\nfrom py_eddy_tracker.poly import create_vertice\n\ndef generate_segmentation_mask_from_file(\n    gridded_ssh_file,\n    date,\n    ssh_var=\"adt\",\n    u_var=\"ugosa\",\n    v_var=\"vgosa\",\n    high_pass_wavelength_km=700,\n    x_offset=0,\n    y_offset=0,\n):\n    g, g_filtered, anticyclonic, cyclonic = identify_eddies(\n        gridded_ssh_file, date, ssh_var, u_var, v_var, high_pass_wavelength_km\n    )\n    mask = generate_segmentation_mask(\n        g_filtered, anticyclonic, cyclonic, x_offset, y_offset\n    )\n    var = g.grid(ssh_var)\n    var_filtered = g_filtered.grid(ssh_var)\n    return var, var_filtered, mask\n\n\ndef identify_eddies(\n    gridded_ssh_file,\n    date,\n    ssh_var=\"adt\",\n    u_var=\"ugosa\",\n    v_var=\"vgosa\",\n    high_pass_wavelength_km=700,\n):\n    g = RegularGridDataset(gridded_ssh_file, \"longitude\", \"latitude\")\n    g_filtered = deepcopy(g)  # make a copy so we don't alter the original\n    g_filtered.bessel_high_filter(ssh_var, high_pass_wavelength_km)\n    anticyclonic, cyclonic = g_filtered.eddy_identification(ssh_var, u_var, v_var, date)\n    return g, g_filtered, anticyclonic, cyclonic\n\n\ndef generate_segmentation_mask(\n    grid_dataset, anticyclonic, cyclonic, x_offset, y_offset, plot=False\n):\n    \"\"\"\n    Creates a numpy array to store the segmentation mask for the grid_dataset.\n    The mask contains classes 0: no eddy, 1: anticyclonic eddy, 2: cyclonic eddy.\n    \"\"\"\n    assert (\n        cyclonic.sign_legend == \"Cyclonic\"\n        and anticyclonic.sign_legend == \"Anticyclonic\"\n    ), \"Check whether the correct order for (anti)cyclonic observations were provided.\"\n    mask = np.zeros(grid_dataset.grid(\"adt\").shape, dtype=np.uint8)\n    # cyclonic should have the same: x_name = 'contour_lon_e', y_name = 'contour_lat_e'\n    x_name, y_name = anticyclonic.intern(False)\n    for eddy in anticyclonic:\n        x_list = (eddy[x_name] - x_offset) % 360 + x_offset\n        vertices = create_vertice(x_list, eddy[y_name] + y_offset)\n        i, j = Path(vertices).pixels_in(grid_dataset)\n        mask[i, j] = 1\n\n    for eddy in cyclonic:\n        x_list = (eddy[x_name] - x_offset) % 360 + x_offset\n        y_list = eddy[y_name] + y_offset\n        i, j = Path(create_vertice(x_list, y_list)).pixels_in(grid_dataset)\n        mask[i, j] = 2\n\n    if plot:\n        ax, m,fig = plot_variable(grid_dataset, mask, \"Segmentation Mask\", cmap=\"viridis\")\n    return mask",
  "history_output" : "Running",
  "history_begin_time" : 1676678179258,
  "history_end_time" : 1676678182354,
  "history_notes" : null,
  "history_process" : "jajowz",
  "host_id" : "ycru82",
  "indicator" : "Done"
},{
  "history_id" : "z8z0v1v2w29",
  "history_input" : "# Generate ground truth on a global scale helper functions\n\nimport multiprocessing\n\nfrom ground_truth_utils import *\n\ndef generate_masks_in_parallel(\n    files,\n    dates,\n    ssh_var=\"adt\",\n    u_var=\"ugosa\",\n    v_var=\"vgosa\",\n    high_pass_wavelength_km=700,\n    x_offset=-180,\n    y_offset=0,\n    num_processes=8,\n    plot=False,\n    save=True,\n):\n    args = [\n        (file, date, ssh_var, u_var, v_var, high_pass_wavelength_km, x_offset, y_offset)\n        for file, date in zip(files, dates)\n    ]\n    pool = multiprocessing.Pool(processes=num_processes)\n    results = pool.starmap(generate_segmentation_mask_from_file, args)\n\n    vars_ = []\n    vars_filtered = []\n    masks = []\n    for result in results:\n        vars_.append(result[0])\n        vars_filtered.append(result[1])\n        masks.append(result[2])\n\n    # concatenate list into single numpy array and return\n    masks = np.stack(masks, axis=0)\n    vars_ = np.stack(vars_, axis=0).astype(np.float32)\n    vars_filtered = np.stack(vars_filtered, axis=0).astype(np.float32)\n\n    if save:\n        # find common folder across all files\n        common_folder = os.path.commonpath(files)\n        years = sorted(set([date.year for date in dates]))\n        year_str = f\"{years[0]}\" if len(years) == 1 else f\"{min(years)}-{max(years)}\"\n        save_path = os.path.join(\n            common_folder, f\"global_pet_masks_with_{ssh_var}_{year_str}.npz\"\n        )\n        np.savez_compressed(\n            save_path,\n            masks=masks,\n            dates=dates,\n            var=vars_,\n            var_filtered=vars_filtered,\n        )\n        print(f\"Saved masks to {save_path}\")\n\n    return vars_, vars_filtered, masks\n\n\nfrom itertools import product\n\n\ndef get_dates_and_files(years, months, days, folder, file_pattern):\n    \"\"\"\n    Given a filename pattern and a list of years months and days,\n    fill in the filename pattern with the date and return\n    a list of filenames and a list of associated `datetime` objects.\n\n    Args:\n        years (list): list of years, e.g., [1993, 1994, 1995, 1996]\n        months (list): list of months, e.g., [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n        days (list): list of days, e.g., [1, 10, 20, 30] for every 10th day\n        folder (str): folder where the files are located\n        file_pattern (str): filename pattern, e.g.,\n            \"dt_global_twosat_phy_l4_{year:04d}{month:02d}{day:02d}_vDT2021.nc\"\n\n    Returns:\n        files (list): full/absolute path to each netCDF file in the list of dates\n        dates (list): list of `datetime` objects formed from the combination of years, months and days\n    \"\"\"\n    dates, files = [], []\n    for y, m, d in product(years, months, days):  # cartesian product\n        try:\n            date = datetime(y, m, d)\n            file = os.path.join(folder, file_pattern.format(year=y, month=m, day=d))\n            dates.append(date)\n            files.append(file)\n        # catch ValueError thrown by datetime if date is not valid\n        except ValueError:\n            pass\n    years = f\"{years[0]}\" if len(years) == 1 else f\"{min(years)}-{max(years)}\"\n    print(f\"Found {len(dates)} files for {years}.\")\n    return dates, files",
  "history_output" : "",
  "history_begin_time" : 1676678226734,
  "history_end_time" : 1676678230887,
  "history_notes" : null,
  "history_process" : "zhsdwn",
  "host_id" : "ycru82",
  "indicator" : "Done"
},{
  "history_id" : "ylp532wv5zf",
  "history_input" : "# Generate ground truth on a global scale helper functions\n\nimport multiprocessing\n\nfrom ground_truth_utils import *\n\ndef generate_masks_in_parallel(\n    files,\n    dates,\n    ssh_var=\"adt\",\n    u_var=\"ugosa\",\n    v_var=\"vgosa\",\n    high_pass_wavelength_km=700,\n    x_offset=-180,\n    y_offset=0,\n    num_processes=8,\n    plot=False,\n    save=True,\n):\n    args = [\n        (file, date, ssh_var, u_var, v_var, high_pass_wavelength_km, x_offset, y_offset)\n        for file, date in zip(files, dates)\n    ]\n    pool = multiprocessing.Pool(processes=num_processes)\n    results = pool.starmap(generate_segmentation_mask_from_file, args)\n\n    vars_ = []\n    vars_filtered = []\n    masks = []\n    for result in results:\n        vars_.append(result[0])\n        vars_filtered.append(result[1])\n        masks.append(result[2])\n\n    # concatenate list into single numpy array and return\n    masks = np.stack(masks, axis=0)\n    vars_ = np.stack(vars_, axis=0).astype(np.float32)\n    vars_filtered = np.stack(vars_filtered, axis=0).astype(np.float32)\n\n    if save:\n        # find common folder across all files\n        common_folder = os.path.commonpath(files)\n        years = sorted(set([date.year for date in dates]))\n        year_str = f\"{years[0]}\" if len(years) == 1 else f\"{min(years)}-{max(years)}\"\n        save_path = os.path.join(\n            common_folder, f\"global_pet_masks_with_{ssh_var}_{year_str}.npz\"\n        )\n        np.savez_compressed(\n            save_path,\n            masks=masks,\n            dates=dates,\n            var=vars_,\n            var_filtered=vars_filtered,\n        )\n        print(f\"Saved masks to {save_path}\")\n\n    return vars_, vars_filtered, masks\n\n\nfrom itertools import product\n\n\ndef get_dates_and_files(years, months, days, folder, file_pattern):\n    \"\"\"\n    Given a filename pattern and a list of years months and days,\n    fill in the filename pattern with the date and return\n    a list of filenames and a list of associated `datetime` objects.\n\n    Args:\n        years (list): list of years, e.g., [1993, 1994, 1995, 1996]\n        months (list): list of months, e.g., [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n        days (list): list of days, e.g., [1, 10, 20, 30] for every 10th day\n        folder (str): folder where the files are located\n        file_pattern (str): filename pattern, e.g.,\n            \"dt_global_twosat_phy_l4_{year:04d}{month:02d}{day:02d}_vDT2021.nc\"\n\n    Returns:\n        files (list): full/absolute path to each netCDF file in the list of dates\n        dates (list): list of `datetime` objects formed from the combination of years, months and days\n    \"\"\"\n    dates, files = [], []\n    for y, m, d in product(years, months, days):  # cartesian product\n        try:\n            date = datetime(y, m, d)\n            file = os.path.join(folder, file_pattern.format(year=y, month=m, day=d))\n            dates.append(date)\n            files.append(file)\n        # catch ValueError thrown by datetime if date is not valid\n        except ValueError:\n            pass\n    years = f\"{years[0]}\" if len(years) == 1 else f\"{min(years)}-{max(years)}\"\n    print(f\"Found {len(dates)} files for {years}.\")\n    return dates, files",
  "history_output" : "",
  "history_begin_time" : 1676678177503,
  "history_end_time" : 1676678181824,
  "history_notes" : null,
  "history_process" : "zhsdwn",
  "host_id" : "ycru82",
  "indicator" : "Done"
},{
  "history_id" : "7g7q9z1bddy",
  "history_input" : "# Take a subset of the masks and SSH map, and save to a compressed numpy (.npz) file\n\nfrom dependency import *\n\ndef subset_arrays(\n    masks,\n    var,\n    var_filtered,\n    dates,\n    lon_range,\n    lat_range,\n    resolution_deg,\n    plot=False,\n    ssh_var=\"adt\",\n    save_folder=None,\n):\n    \"\"\"\n    Subset the arrays to the given lon_range and lat_range.\n\n    Args:\n        masks (np.ndarray): Global eddy segmentation masks.\n            Can be masks from multiple dates concatenated into one array\n        var (np.ndarray): Global SSH value\n        var_filtered (np.ndarray): Global SSH value after high-pass filter\n        dates (list): List of `datetime` objects\n        lon_range (tuple): Longitude range to subset to (lon_start, lon_end)\n        lat_range (tuple): Latitude range to subset to (lat_start, lat_end)\n        resolution_deg (float): Resolution of the SSH map in degrees\n        plot (bool): Whether to plot a sample of the subsetted arrays\n        ssh_var (str): SSH variable name. Defaults to \"adt\". Only used if save_folder is not None.\n        save_folder (str): Folder to save the subsetted arrays to. Defaults to None.\n            If None, the arrays are not saved.\n\n    Returns:\n        mask_subset (np.ndarray): Subsetted masks\n        var_subset (np.ndarray): Subsetted var\n        var_filtered_subset (np.ndarray): Subsetted var_filtered\n        lon_subset (np.ndarray): Subsetted lon\n        lat_subset (np.ndarray): Subsetted lat\n    \"\"\"\n    lon_bounds = np.arange(-180, 180 + resolution_deg, resolution_deg)\n    lat_bounds = np.arange(-90, 90 + resolution_deg, resolution_deg)\n\n    # convert lon_range and lat_range to indices in the numpy arrays\n    lon_start, lon_end = lon_range\n    lat_start, lat_end = lat_range\n    lon_idx = lambda lon: np.argmin(np.abs(lon_bounds - lon))\n    lat_idx = lambda lat: np.argmin(np.abs(lat_bounds - lat))\n    lon_start_idx, lon_end_idx = lon_idx(lon_start), lon_idx(lon_end)\n    lat_start_idx, lat_end_idx = lat_idx(lat_start), lat_idx(lat_end)\n\n    mask_subset = masks[:, lon_start_idx:lon_end_idx, lat_start_idx:lat_end_idx]\n    var_subset = var[:, lon_start_idx:lon_end_idx, lat_start_idx:lat_end_idx]\n    var_filtered_subset = var_filtered[\n        :, lon_start_idx:lon_end_idx, lat_start_idx:lat_end_idx\n    ]\n    lon_subset = lon_bounds[lon_start_idx : lon_end_idx + 1]\n    lat_subset = lat_bounds[lat_start_idx : lat_end_idx + 1]\n    if plot:\n        fig, ax = plt.subplots()\n        if mask_subset.ndim == 3:\n            m = mask_subset[0]\n            v = var_subset[0]\n        elif mask_subset.ndim == 2:\n            m = mask_subset\n            v = var_subset\n        ax.pcolormesh(lon_subset, lat_subset, m.T, vmin=0, vmax=2, cmap=\"viridis\")\n        ax.set_xlim(lon_start, lon_end)\n        ax.set_ylim(lat_start, lat_end)\n        ax.set_aspect(abs((lon_end - lon_start) / (lat_start - lat_end)) * 1.0)\n\n    if save_folder is not None:\n        all_years = sorted(set([d.year for d in dates]))\n        year_str = (\n            f\"{all_years[0]}\"\n            if len(all_years) == 1\n            else f\"{min(all_years)}-{max(all_years)}\"\n        )\n        lat_str = lat_range_to_str(lat_range)\n        lon_str = lon_range_to_str(lon_range)\n        save_path = os.path.join(\n            save_folder,\n            f\"subset_pet_masks_with_{ssh_var}_{year_str}_lat{lat_str}_lon{lon_str}.npz\",\n        )\n        np.savez_compressed(\n            save_path,\n            masks=mask_subset,\n            dates=dates,\n            var=var_subset,\n            var_filtered=var_filtered_subset,\n            lon_subset=lon_subset,\n            lat_subset=lat_subset,\n        )\n        print(f\"Saved mask subset to {save_path}\")\n    return mask_subset, var_subset, var_filtered_subset, lon_subset, lat_subset\n\n\ndef lon_range_to_str(lon_range):\n    lon_start, lon_end = lon_range\n    lon_start = f\"{lon_start}E\" if lon_start >= 0 else f\"{abs(lon_start)}W\"\n    lon_end = f\"{lon_end}E\" if lon_end >= 0 else f\"{abs(lon_end)}W\"\n    return f\"{lon_start}-{lon_end}\"\n\n\ndef lat_range_to_str(lat_range):\n    lat_start, lat_end = lat_range\n    lat_start = f\"{lat_start}N\" if lat_start >= 0 else f\"{abs(lat_start)}S\"\n    lat_end = f\"{lat_end}N\" if lat_end >= 0 else f\"{abs(lat_end)}S\"\n    return f\"{lat_start}-{lat_end}\"",
  "history_output" : "",
  "history_begin_time" : 1676678178387,
  "history_end_time" : 1676678181859,
  "history_notes" : null,
  "history_process" : "g85teu",
  "host_id" : "ycru82",
  "indicator" : "Done"
},{
  "history_id" : "fl7qdvuic7y",
  "history_input" : "# This generates segmentation efficiently in parallel on a gloabal scale\n\n\nimport logging\n\nfrom generate_ground_truth_parallel_utils import *\nfrom data_loader import *\n\n\nlogging.getLogger(\"pet\").setLevel(logging.ERROR)\n\n# enter the AVISO filename pattern\n# year, month, and day in file_pattern will be filled in get_dates_and_files:\nfile_pattern = \"dt_global_twosat_phy_l4_{year:04d}{month:02d}{day:02d}_vDT2021.nc\"\n\n# training set: 1998 - 2018\ntrain_dates, train_files = get_dates_and_files(\n    range(1998, 2000), range(1, 13), [10], train_folder, file_pattern\n)\ntrain_adt, train_adt_filtered, train_masks = generate_masks_in_parallel(\n    train_files, train_dates\n)\n\n\n# test set: 2019\ntest_dates, test_files = get_dates_and_files(\n    [2019], range(1, 13), [10], test_folder, file_pattern\n)\ntest_adt, test_adt_filtered, test_masks = generate_masks_in_parallel(\n    test_files, test_dates\n)",
  "history_output" : "Found 24 files for 1998-1999.\nmultiprocessing.pool.RemoteTraceback: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/chetana/anaconda3/envs/ranjan/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/home/chetana/anaconda3/envs/ranjan/lib/python3.10/multiprocessing/pool.py\", line 51, in starmapstar\n    return list(itertools.starmap(args[0], args[1]))\n  File \"/home/chetana/gw-workspace/fl7qdvuic7y/ground_truth_utils.py\", line 18, in generate_segmentation_mask_from_file\n    g, g_filtered, anticyclonic, cyclonic = identify_eddies(\n  File \"/home/chetana/gw-workspace/fl7qdvuic7y/ground_truth_utils.py\", line 37, in identify_eddies\n    g = RegularGridDataset(gridded_ssh_file, \"longitude\", \"latitude\")\n  File \"/home/chetana/anaconda3/envs/ranjan/lib/python3.10/site-packages/py_eddy_tracker/dataset/grid.py\", line 1211, in __init__\n    super().__init__(*args, **kwargs)\n  File \"/home/chetana/anaconda3/envs/ranjan/lib/python3.10/site-packages/py_eddy_tracker/dataset/grid.py\", line 307, in __init__\n    self.populate()\n  File \"/home/chetana/anaconda3/envs/ranjan/lib/python3.10/site-packages/py_eddy_tracker/dataset/grid.py\", line 311, in populate\n    self.load_general_features()\n  File \"/home/chetana/anaconda3/envs/ranjan/lib/python3.10/site-packages/py_eddy_tracker/dataset/grid.py\", line 345, in load_general_features\n    with Dataset(self.filename) as h:\n  File \"src/netCDF4/_netCDF4.pyx\", line 2307, in netCDF4._netCDF4.Dataset.__init__\n  File \"src/netCDF4/_netCDF4.pyx\", line 1925, in netCDF4._netCDF4._ensure_nc_success\nFileNotFoundError: [Errno 2] No such file or directory: b'/home/chetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_19980610_vDT2021.nc'\n\"\"\"\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/fl7qdvuic7y/generate_segmentation_in_parallel.py\", line 20, in <module>\n    train_adt, train_adt_filtered, train_masks = generate_masks_in_parallel(\n  File \"/home/chetana/gw-workspace/fl7qdvuic7y/generate_ground_truth_parallel_utils.py\", line 25, in generate_masks_in_parallel\n    results = pool.starmap(generate_segmentation_mask_from_file, args)\n  File \"/home/chetana/anaconda3/envs/ranjan/lib/python3.10/multiprocessing/pool.py\", line 375, in starmap\n    return self._map_async(func, iterable, starmapstar, chunksize).get()\n  File \"/home/chetana/anaconda3/envs/ranjan/lib/python3.10/multiprocessing/pool.py\", line 774, in get\n    raise self._value\nFileNotFoundError: [Errno 2] No such file or directory: b'/home/chetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_19980610_vDT2021.nc'\n",
  "history_begin_time" : 1676678232603,
  "history_end_time" : 1676678236843,
  "history_notes" : null,
  "history_process" : "q20jvx",
  "host_id" : "ycru82",
  "indicator" : "Done"
},{
  "history_id" : "nfqv9wgw91a",
  "history_input" : "# Use the segmask_and_ssh_utils to generate compress file\n\nfrom data_loader import *\nfrom generate_segmentation_in_parallel import *\nfrom segmask_and_ssh_utils import *\n\nlon_range = (-166, -134)\nlat_range = (14, 46)\n\ntrain_subset = subset_arrays(\n    train_masks,\n    train_adt,\n    train_adt_filtered,\n    train_dates,\n    lon_range,\n    lat_range,\n    plot=False,\n    resolution_deg=0.25,\n    save_folder=train_folder,\n)\n\ntest_subset = subset_arrays(\n    test_masks,\n    test_adt,\n    test_adt_filtered,\n    test_dates,\n    lon_range,\n    lat_range,\n    plot=True,\n    resolution_deg=0.25,\n    save_folder=test_folder,\n)",
  "history_output" : "Found 24 files for 1998-1999.\nmultiprocessing.pool.RemoteTraceback: \n\"\"\"\nTraceback (most recent call last):\n  File \"/home/chetana/anaconda3/envs/ranjan/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/home/chetana/anaconda3/envs/ranjan/lib/python3.10/multiprocessing/pool.py\", line 51, in starmapstar\n    return list(itertools.starmap(args[0], args[1]))\n  File \"/home/chetana/gw-workspace/nfqv9wgw91a/ground_truth_utils.py\", line 18, in generate_segmentation_mask_from_file\n    g, g_filtered, anticyclonic, cyclonic = identify_eddies(\n  File \"/home/chetana/gw-workspace/nfqv9wgw91a/ground_truth_utils.py\", line 37, in identify_eddies\n    g = RegularGridDataset(gridded_ssh_file, \"longitude\", \"latitude\")\n  File \"/home/chetana/anaconda3/envs/ranjan/lib/python3.10/site-packages/py_eddy_tracker/dataset/grid.py\", line 1211, in __init__\n    super().__init__(*args, **kwargs)\n  File \"/home/chetana/anaconda3/envs/ranjan/lib/python3.10/site-packages/py_eddy_tracker/dataset/grid.py\", line 307, in __init__\n    self.populate()\n  File \"/home/chetana/anaconda3/envs/ranjan/lib/python3.10/site-packages/py_eddy_tracker/dataset/grid.py\", line 311, in populate\n    self.load_general_features()\n  File \"/home/chetana/anaconda3/envs/ranjan/lib/python3.10/site-packages/py_eddy_tracker/dataset/grid.py\", line 345, in load_general_features\n    with Dataset(self.filename) as h:\n  File \"src/netCDF4/_netCDF4.pyx\", line 2307, in netCDF4._netCDF4.Dataset.__init__\n  File \"src/netCDF4/_netCDF4.pyx\", line 1925, in netCDF4._netCDF4._ensure_nc_success\nFileNotFoundError: [Errno 2] No such file or directory: b'/home/chetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_19980110_vDT2021.nc'\n\"\"\"\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/nfqv9wgw91a/compress_segmask_and_ssh.py\", line 4, in <module>\n    from generate_segmentation_in_parallel import *\n  File \"/home/chetana/gw-workspace/nfqv9wgw91a/generate_segmentation_in_parallel.py\", line 20, in <module>\n    train_adt, train_adt_filtered, train_masks = generate_masks_in_parallel(\n  File \"/home/chetana/gw-workspace/nfqv9wgw91a/generate_ground_truth_parallel_utils.py\", line 25, in generate_masks_in_parallel\n    results = pool.starmap(generate_segmentation_mask_from_file, args)\n  File \"/home/chetana/anaconda3/envs/ranjan/lib/python3.10/multiprocessing/pool.py\", line 375, in starmap\n    return self._map_async(func, iterable, starmapstar, chunksize).get()\n  File \"/home/chetana/anaconda3/envs/ranjan/lib/python3.10/multiprocessing/pool.py\", line 774, in get\n    raise self._value\nFileNotFoundError: [Errno 2] No such file or directory: b'/home/chetana/ML_eddies/cds_ssh_1998-2018_10day_interval/dt_global_twosat_phy_l4_19980110_vDT2021.nc'\n",
  "history_begin_time" : 1676678236914,
  "history_end_time" : 1676678241070,
  "history_notes" : null,
  "history_process" : "yddm1o",
  "host_id" : "ycru82",
  "indicator" : "Failed"
},{
  "history_id" : "vag6sz7u2x8",
  "history_input" : "from dependency import *\nfrom unzip_utils import *\nfrom get_data import *\n\n\nos.chdir('/home/chetana/')\ncurrent_working_dir = os.getcwd()\nprint(current_working_dir)\n\n# Directory names\nroot_dir_name = \"ML_eddies\"\ntrain_dir_name = \"cds_ssh_1998-2018_10day_interval\"\ntest_dir_name = \"cds_ssh_2019_10day_interval\"\n\n# Build dir paths\nroot_path = os.path.join(current_working_dir, root_dir_name)\ntrain_path = os.path.join(root_path, train_dir_name)\ntest_path= os.path.join(root_path, test_dir_name)\n\n# Check if dir exists\nis_root_dir_exists = os.path.exists(root_path)\nis_train_dir_exists = os.path.exists(train_path)\nis_test_dir_exists = os.path.exists(test_path)\n\n\ndef create_directory(directory_name):\n    try:\n        os.mkdir(directory_name)\n        logger.info(\"Successfully created folder\")\n    except:\n        logger.error(\"Something went wrong while creating folder\")\n\n\n\n'''if is_root_dir_exists != True:\n    print(root_path)\n    create_directory(root_path)\n    print(\"created:\",root_path)\n    create_directory(train_path)\n    create_directory(test_path)\n    train_file, test_file = download_data()\n\n    unzip_file( os.path.join(current_working_dir,train_file), train_path)\n    unzip_file( os.path.join(current_working_dir,test_file), test_path)\n\n\nif is_root_dir_exists and is_train_dir_exists != True:\n    create_directory(\"cds_ssh_1998-2018_10day_interval\")\n    train_file = download_train_data()\n    unzip_file( os.path.join(current_working_dir,train_file), train_path)'''\n\nif  is_test_dir_exists != True:\n    create_directory(\"cds_ssh_2019_10day_interval\")\n    test_file = download_test_data()\n    unzip_file( os.path.join(current_working_dir,test_file), test_path)\n\n",
  "history_output" : "/home/chetana\n2023-02-17 23:56:32,230 INFO Successfully created folder\n2023-02-17 23:56:32,706 INFO Welcome to the CDS\n2023-02-17 23:56:32,707 INFO Sending request to https://cds.climate.copernicus.eu/api/v2/resources/satellite-sea-level-global\n2023-02-17 23:56:32,943 INFO Request is completed\n2023-02-17 23:56:32,943 INFO Downloading https://download-0011-clone.copernicus-climate.eu/cache-compute-0011/cache/data8/dataset-satellite-sea-level-global-f921d56b-47a1-434f-9155-5d252ef99366.zip to test_data.zip (438.1M)\n\n  0%|                                                | 0.00/438M [00:00<?, ?B/s]\n  0%|                                     | 8.00k/438M [00:00<1:35:19, 80.3kB/s]\n  0%|                                        | 40.0k/438M [00:00<35:05, 218kB/s]\n  0%|                                        | 80.0k/438M [00:00<25:46, 297kB/s]\n  0%|                                         | 176k/438M [00:00<13:54, 550kB/s]\n  0%|                                         | 320k/438M [00:00<08:54, 859kB/s]\n  0%|                                        | 664k/438M [00:00<04:28, 1.71MB/s]\n  0%|                                        | 952k/438M [00:00<03:41, 2.07MB/s]\n  0%|▏                                      | 1.77M/438M [00:00<01:52, 4.07MB/s]\n  1%|▏                                      | 2.68M/438M [00:00<01:20, 5.66MB/s]\n  1%|▍                                      | 4.79M/438M [00:01<00:43, 10.4MB/s]\n  2%|▌                                      | 6.96M/438M [00:01<00:32, 13.8MB/s]\n  2%|▊                                      | 9.19M/438M [00:01<00:27, 16.4MB/s]\n  3%|█                                      | 11.5M/438M [00:01<00:24, 18.4MB/s]\n  3%|█▏                                     | 13.8M/438M [00:01<00:22, 19.9MB/s]\n  4%|█▍                                     | 16.2M/438M [00:01<00:20, 21.1MB/s]\n  4%|█▋                                     | 18.7M/438M [00:01<00:19, 22.3MB/s]\n  5%|█▉                                     | 21.3M/438M [00:01<00:18, 23.1MB/s]\n  5%|██                                     | 23.7M/438M [00:01<00:18, 23.6MB/s]\n  6%|██▎                                    | 26.3M/438M [00:01<00:17, 24.1MB/s]\n  7%|██▌                                    | 28.8M/438M [00:02<00:17, 24.4MB/s]\n  7%|██▊                                    | 31.4M/438M [00:02<00:17, 24.9MB/s]\n  8%|███                                    | 33.8M/438M [00:02<00:17, 24.3MB/s]\n  8%|███▏                                   | 36.2M/438M [00:02<00:17, 24.5MB/s]\n  9%|███▍                                   | 38.7M/438M [00:02<00:16, 24.7MB/s]\n  9%|███▋                                   | 41.2M/438M [00:02<00:16, 24.9MB/s]\n 10%|███▉                                   | 43.7M/438M [00:02<00:16, 24.9MB/s]\n 11%|████                                   | 46.2M/438M [00:02<00:16, 24.9MB/s]\n 11%|████▎                                  | 48.7M/438M [00:02<00:16, 24.8MB/s]\n 12%|████▌                                  | 51.2M/438M [00:03<00:16, 24.9MB/s]\n 12%|████▊                                  | 54.0M/438M [00:03<00:15, 25.9MB/s]\n 13%|█████                                  | 56.9M/438M [00:03<00:14, 26.8MB/s]\n 14%|█████▎                                 | 59.5M/438M [00:03<00:14, 26.6MB/s]\n 14%|█████▌                                 | 62.1M/438M [00:03<00:15, 26.3MB/s]\n 15%|█████▊                                 | 64.8M/438M [00:03<00:14, 26.5MB/s]\n 15%|██████                                 | 67.6M/438M [00:03<00:14, 26.8MB/s]\n 16%|██████▎                                | 70.3M/438M [00:03<00:14, 27.0MB/s]\n 17%|██████▍                                | 72.9M/438M [00:03<00:14, 26.6MB/s]\n 17%|██████▋                                | 75.6M/438M [00:03<00:14, 26.6MB/s]\n 18%|██████▉                                | 78.4M/438M [00:04<00:14, 26.9MB/s]\n 19%|███████▏                               | 81.1M/438M [00:04<00:13, 27.0MB/s]\n 19%|███████▍                               | 83.9M/438M [00:04<00:13, 27.3MB/s]\n 20%|███████▋                               | 86.5M/438M [00:04<00:13, 27.0MB/s]\n 20%|███████▉                               | 89.3M/438M [00:04<00:13, 27.3MB/s]\n 21%|████████▏                              | 92.1M/438M [00:04<00:13, 27.3MB/s]\n 22%|████████▍                              | 94.8M/438M [00:04<00:13, 27.3MB/s]\n 22%|████████▋                              | 97.7M/438M [00:04<00:12, 27.8MB/s]\n 23%|█████████▏                              | 101M/438M [00:04<00:12, 28.0MB/s]\n 24%|█████████▍                              | 104M/438M [00:05<00:12, 28.5MB/s]\n 24%|█████████▋                              | 106M/438M [00:05<00:12, 27.2MB/s]\n 25%|█████████▉                              | 109M/438M [00:05<00:12, 27.0MB/s]\n 25%|██████████▏                             | 112M/438M [00:05<00:12, 27.6MB/s]\n 26%|██████████▍                             | 114M/438M [00:05<00:12, 26.8MB/s]\n 27%|██████████▋                             | 117M/438M [00:05<00:12, 27.2MB/s]\n 27%|██████████▉                             | 120M/438M [00:05<00:12, 27.5MB/s]\n 28%|███████████▏                            | 123M/438M [00:05<00:11, 28.0MB/s]\n 29%|███████████▍                            | 126M/438M [00:05<00:11, 28.8MB/s]\n 29%|███████████▋                            | 128M/438M [00:05<00:11, 28.2MB/s]\n 30%|███████████▉                            | 131M/438M [00:06<00:11, 28.3MB/s]\n 31%|████████████▏                           | 134M/438M [00:06<00:11, 27.6MB/s]\n 31%|████████████▍                           | 137M/438M [00:06<00:11, 27.7MB/s]\n 32%|████████████▋                           | 139M/438M [00:06<00:11, 26.6MB/s]\n 32%|████████████▉                           | 142M/438M [00:06<00:11, 26.3MB/s]\n 33%|█████████████▏                          | 144M/438M [00:06<00:11, 26.4MB/s]\n 34%|█████████████▍                          | 147M/438M [00:06<00:11, 27.2MB/s]\n 34%|█████████████▋                          | 150M/438M [00:06<00:10, 27.7MB/s]\n 35%|█████████████▉                          | 153M/438M [00:06<00:10, 28.0MB/s]\n 36%|██████████████▏                         | 156M/438M [00:07<00:10, 27.8MB/s]\n 36%|██████████████▍                         | 158M/438M [00:07<00:10, 27.4MB/s]\n 37%|██████████████▋                         | 161M/438M [00:07<00:10, 27.6MB/s]\n 37%|██████████████▉                         | 164M/438M [00:07<00:10, 27.8MB/s]\n 38%|███████████████▏                        | 167M/438M [00:07<00:10, 26.9MB/s]\n 39%|███████████████▍                        | 170M/438M [00:07<00:10, 27.6MB/s]\n 39%|███████████████▋                        | 172M/438M [00:07<00:09, 28.1MB/s]\n 40%|███████████████▉                        | 175M/438M [00:07<00:10, 27.5MB/s]\n 41%|████████████████▏                       | 178M/438M [00:07<00:10, 27.2MB/s]\n 41%|████████████████▍                       | 181M/438M [00:07<00:09, 27.9MB/s]\n 42%|████████████████▊                       | 184M/438M [00:08<00:09, 28.2MB/s]\n 43%|█████████████████                       | 187M/438M [00:08<00:09, 28.5MB/s]\n 43%|█████████████████▎                      | 189M/438M [00:08<00:09, 28.0MB/s]\n 44%|█████████████████▌                      | 192M/438M [00:08<00:09, 27.3MB/s]\n 44%|█████████████████▊                      | 195M/438M [00:08<00:09, 28.0MB/s]\n 45%|██████████████████                      | 197M/438M [00:08<00:09, 27.5MB/s]\n 46%|██████████████████▎                     | 200M/438M [00:08<00:09, 26.8MB/s]\n 46%|██████████████████▌                     | 203M/438M [00:08<00:09, 27.0MB/s]\n 47%|██████████████████▊                     | 206M/438M [00:08<00:08, 27.7MB/s]\n 48%|███████████████████                     | 209M/438M [00:09<00:08, 28.0MB/s]\n 48%|███████████████████▎                    | 211M/438M [00:09<00:08, 28.2MB/s]\n 49%|███████████████████▌                    | 214M/438M [00:09<00:08, 27.6MB/s]\n 49%|███████████████████▊                    | 217M/438M [00:09<00:08, 27.6MB/s]\n 50%|████████████████████                    | 220M/438M [00:09<00:08, 28.2MB/s]\n 51%|████████████████████▎                   | 223M/438M [00:09<00:07, 28.3MB/s]\n 51%|████████████████████▌                   | 226M/438M [00:09<00:07, 28.7MB/s]\n 52%|████████████████████▊                   | 228M/438M [00:09<00:07, 28.4MB/s]\n 53%|█████████████████████                   | 231M/438M [00:09<00:07, 28.6MB/s]\n 53%|█████████████████████▎                  | 234M/438M [00:09<00:07, 28.5MB/s]\n 54%|█████████████████████▌                  | 237M/438M [00:10<00:07, 27.6MB/s]\n 55%|█████████████████████▊                  | 240M/438M [00:10<00:07, 28.0MB/s]\n 55%|██████████████████████                  | 242M/438M [00:10<00:07, 26.6MB/s]\n 56%|██████████████████████▎                 | 245M/438M [00:10<00:07, 26.3MB/s]\n 56%|██████████████████████▌                 | 248M/438M [00:10<00:07, 27.0MB/s]\n 57%|██████████████████████▊                 | 250M/438M [00:10<00:07, 27.7MB/s]\n 58%|███████████████████████                 | 253M/438M [00:10<00:07, 27.4MB/s]\n 58%|███████████████████████▎                | 256M/438M [00:10<00:07, 26.9MB/s]\n 59%|███████████████████████▌                | 258M/438M [00:10<00:07, 26.7MB/s]\n 60%|███████████████████████▊                | 261M/438M [00:11<00:06, 27.4MB/s]\n 60%|████████████████████████                | 264M/438M [00:11<00:06, 26.7MB/s]\n 61%|████████████████████████▎               | 266M/438M [00:11<00:06, 26.8MB/s]\n 61%|████████████████████████▌               | 269M/438M [00:11<00:06, 27.5MB/s]\n 62%|████████████████████████▊               | 272M/438M [00:11<00:06, 28.0MB/s]\n 63%|█████████████████████████               | 275M/438M [00:11<00:06, 27.8MB/s]\n 63%|█████████████████████████▎              | 278M/438M [00:11<00:06, 27.7MB/s]\n 64%|█████████████████████████▌              | 280M/438M [00:11<00:06, 26.9MB/s]\n 65%|█████████████████████████▊              | 283M/438M [00:11<00:06, 26.3MB/s]\n 65%|██████████████████████████              | 285M/438M [00:11<00:06, 26.2MB/s]\n 66%|██████████████████████████▎             | 288M/438M [00:12<00:06, 26.0MB/s]\n 66%|██████████████████████████▌             | 291M/438M [00:12<00:05, 27.2MB/s]\n 67%|██████████████████████████▊             | 294M/438M [00:12<00:05, 27.5MB/s]\n 68%|███████████████████████████             | 296M/438M [00:12<00:05, 27.4MB/s]\n 68%|███████████████████████████▎            | 299M/438M [00:12<00:05, 27.8MB/s]\n 69%|███████████████████████████▌            | 302M/438M [00:12<00:05, 28.0MB/s]\n 70%|███████████████████████████▊            | 305M/438M [00:12<00:05, 27.4MB/s]\n 70%|████████████████████████████            | 307M/438M [00:12<00:05, 27.3MB/s]\n 71%|████████████████████████████▎           | 310M/438M [00:12<00:04, 27.3MB/s]\n 71%|████████████████████████████▌           | 313M/438M [00:13<00:04, 26.6MB/s]\n 72%|████████████████████████████▊           | 315M/438M [00:13<00:04, 27.0MB/s]\n 73%|█████████████████████████████           | 318M/438M [00:13<00:04, 27.2MB/s]\n 73%|█████████████████████████████▎          | 321M/438M [00:13<00:04, 27.2MB/s]\n 74%|█████████████████████████████▌          | 324M/438M [00:13<00:04, 26.9MB/s]\n 75%|█████████████████████████████▊          | 326M/438M [00:13<00:04, 27.4MB/s]\n 75%|██████████████████████████████          | 329M/438M [00:13<00:04, 27.7MB/s]\n 76%|██████████████████████████████▎         | 332M/438M [00:13<00:03, 28.2MB/s]\n 76%|██████████████████████████████▌         | 335M/438M [00:13<00:03, 27.9MB/s]\n 77%|██████████████████████████████▊         | 338M/438M [00:13<00:03, 28.4MB/s]\n 78%|███████████████████████████████         | 341M/438M [00:14<00:03, 27.4MB/s]\n 78%|███████████████████████████████▎        | 343M/438M [00:14<00:03, 27.2MB/s]\n 79%|███████████████████████████████▌        | 346M/438M [00:14<00:03, 27.1MB/s]\n 80%|███████████████████████████████▊        | 348M/438M [00:14<00:03, 26.9MB/s]\n 80%|████████████████████████████████        | 351M/438M [00:14<00:03, 27.5MB/s]\n 81%|████████████████████████████████▎       | 354M/438M [00:14<00:03, 26.8MB/s]\n 81%|████████████████████████████████▌       | 357M/438M [00:14<00:03, 27.0MB/s]\n 82%|████████████████████████████████▊       | 360M/438M [00:14<00:02, 27.8MB/s]\n 83%|█████████████████████████████████       | 363M/438M [00:14<00:02, 28.1MB/s]\n 83%|█████████████████████████████████▎      | 365M/438M [00:15<00:02, 28.5MB/s]\n 84%|█████████████████████████████████▋      | 368M/438M [00:15<00:02, 28.7MB/s]\n 85%|█████████████████████████████████▉      | 371M/438M [00:15<00:02, 28.8MB/s]\n 85%|██████████████████████████████████▏     | 374M/438M [00:15<00:03, 21.1MB/s]\n 86%|██████████████████████████████████▎     | 376M/438M [00:15<00:03, 17.3MB/s]\n 86%|██████████████████████████████████▌     | 379M/438M [00:15<00:03, 19.1MB/s]\n 87%|██████████████████████████████████▊     | 381M/438M [00:15<00:03, 18.8MB/s]\n 87%|██████████████████████████████████▉     | 383M/438M [00:16<00:03, 18.8MB/s]\n 88%|███████████████████████████████████▏    | 385M/438M [00:16<00:03, 18.5MB/s]\n 88%|███████████████████████████████████▎    | 387M/438M [00:16<00:03, 15.1MB/s]\n 89%|███████████████████████████████████▍    | 388M/438M [00:16<00:03, 15.4MB/s]\n 89%|███████████████████████████████████▌    | 390M/438M [00:16<00:03, 15.4MB/s]\n 89%|███████████████████████████████████▋    | 391M/438M [00:16<00:03, 15.4MB/s]\n 90%|███████████████████████████████████▊    | 393M/438M [00:16<00:03, 15.6MB/s]\n 90%|████████████████████████████████████    | 394M/438M [00:16<00:02, 15.6MB/s]\n 90%|████████████████████████████████████▏   | 396M/438M [00:16<00:02, 15.7MB/s]\n 91%|████████████████████████████████████▎   | 398M/438M [00:17<00:02, 15.8MB/s]\n 91%|████████████████████████████████████▍   | 399M/438M [00:17<00:02, 16.0MB/s]\n 92%|████████████████████████████████████▌   | 401M/438M [00:17<00:02, 16.1MB/s]\n 92%|████████████████████████████████████▊   | 403M/438M [00:17<00:02, 16.2MB/s]\n 92%|████████████████████████████████████▉   | 404M/438M [00:17<00:02, 16.3MB/s]\n 93%|█████████████████████████████████████   | 406M/438M [00:17<00:02, 16.4MB/s]\n 93%|█████████████████████████████████████▏  | 408M/438M [00:17<00:01, 16.5MB/s]\n 93%|█████████████████████████████████████▎  | 409M/438M [00:17<00:01, 16.6MB/s]\n 94%|█████████████████████████████████████▌  | 411M/438M [00:17<00:01, 16.6MB/s]\n 94%|█████████████████████████████████████▋  | 413M/438M [00:17<00:01, 16.7MB/s]\n 95%|█████████████████████████████████████▊  | 414M/438M [00:18<00:01, 16.8MB/s]\n 95%|█████████████████████████████████████▉  | 416M/438M [00:18<00:01, 16.8MB/s]\n 95%|██████████████████████████████████████▏ | 418M/438M [00:18<00:01, 16.9MB/s]\n 96%|██████████████████████████████████████▎ | 419M/438M [00:18<00:01, 16.9MB/s]\n 96%|██████████████████████████████████████▍ | 421M/438M [00:18<00:01, 17.0MB/s]\n 97%|██████████████████████████████████████▌ | 423M/438M [00:18<00:00, 17.1MB/s]\n 97%|██████████████████████████████████████▊ | 425M/438M [00:18<00:00, 17.2MB/s]\n 97%|██████████████████████████████████████▉ | 426M/438M [00:18<00:00, 17.2MB/s]\n 98%|███████████████████████████████████████ | 428M/438M [00:18<00:00, 17.2MB/s]\n 98%|███████████████████████████████████████▏| 430M/438M [00:19<00:00, 17.3MB/s]\n 99%|███████████████████████████████████████▍| 432M/438M [00:19<00:00, 17.3MB/s]\n 99%|███████████████████████████████████████▌| 433M/438M [00:19<00:00, 17.4MB/s]\n 99%|███████████████████████████████████████▋| 435M/438M [00:19<00:00, 17.4MB/s]\n100%|███████████████████████████████████████▉| 437M/438M [00:19<00:00, 17.5MB/s]\n                                                                                \n2023-02-17 23:56:53,627 INFO Download rate 21.2M/s\n",
  "history_begin_time" : 1676678188318,
  "history_end_time" : 1676678214885,
  "history_notes" : null,
  "history_process" : "dhjb5i",
  "host_id" : "ycru82",
  "indicator" : "Done"
},{
  "history_id" : "ylvjm9lvbgy",
  "history_input" : "from dependency import logger\nfrom zipfile import ZipFile\n\ndef unzip_file(zip_file_path, extract_to_path):\n    try:\n        with ZipFile(zip_file_path) as zip_file_object:          \n            zip_file_object.extractall(extract_to_path)\n            \n    except:\n        logger.error(\"Something went wrong while extracting File\" )\n",
  "history_output" : "",
  "history_begin_time" : 1676678183230,
  "history_end_time" : 1676678187444,
  "history_notes" : null,
  "history_process" : "zbt6sg",
  "host_id" : "ycru82",
  "indicator" : "Done"
},{
  "history_id" : "dfp6027ajjq",
  "history_input" : "\nfrom dependency import logger\n\nimport cdsapi\n\nclient = cdsapi.Client()\n\ndef download_train_data():\n    try:\n        client.retrieve(\n            'satellite-sea-level-global',\n            {\n                'version': 'vDT2021',\n                'variable': 'all',\n                'format': 'zip',\n                'year': [\n                    '1998', '1999', '2000',\n                    '2001', '2002', '2003',\n                    '2004', '2005', '2006',\n                    '2007', '2008', '2009',\n                    '2010', '2011', '2012',\n                    '2013', '2014', '2015',\n                    '2016', '2017', '2018',\n                ],\n                'month': [\n                '01', '02', '03',\n                '04', '05', '06',\n                '07', '08', '09',\n                '10', '11', '12',\n            ],\n                'day': ['01','10','20','30'],\n            },\n            'train_data.zip')\n        return 'train_data.zip'\n    except:\n        logger.error(\"Something went wrong while downloading training data\")\n\n\ndef download_test_data():\n    try:\n        client.retrieve(\n            'satellite-sea-level-global',\n            {\n                'version': 'vDT2021',\n                'variable': 'all',\n                'format': 'zip',\n                'year': [ '2019' ],\n                'month': [\n                    '01', '02', '03',\n                    '04', '05', '06',\n                    '07', '08', '09',\n                    '10', '11', '12',\n                ],\n                'day': ['01','10','20','30'],\n            },\n            'test_data.zip')\n        return 'test_data.zip'\n    except:\n        logger.error(\"Something went wrong while downloading test data\")\n\n\ndef download_data():\n    train_zip_file = download_train_data()\n    test_zip_file = download_test_data()\n    return train_zip_file, test_zip_file",
  "history_output" : "",
  "history_begin_time" : 1676678183833,
  "history_end_time" : 1676678187029,
  "history_notes" : null,
  "history_process" : "g7a3zf",
  "host_id" : "ycru82",
  "indicator" : "Done"
}]
