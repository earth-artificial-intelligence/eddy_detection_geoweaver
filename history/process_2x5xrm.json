[{
  "history_id" : "0xf6rpyr59t",
  "history_input" : null,
  "history_output" : "Exhausted available authentication methods",
  "history_begin_time" : 1678250140126,
  "history_end_time" : 1678250142781,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "ycru82",
  "indicator" : "Failed"
},{
  "history_id" : "i9LoOyVLxGxi",
  "history_input" : "from model_training_utils import add_hparams, EarlyStopping\nfrom get_device_config import *\nfrom loss_function import *\nfrom set_optmizer_and_scheduler import *\nfrom set_summary_writer import *\nfrom model_utils import *\nfrom torch_metrics_utils import *\nfrom tqdm.auto import tqdm\n\n\"\"\"\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nprint(train_m)\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"MulticlassAccuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"MulticlassAccuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\"\"\"\n\n# save model to tensorboard folder\nN = 2\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\nprint(model_path)\n#torch.save(model.state_dict(), model_path)",
  "history_output" : "Read 252 samples from /home/chetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 12 samples from /home/chetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\n======================================================================\nWriting Tensorboard logs to /home/chetana/tensorboard/2023-03-07_18-13\n======================================================================\n/home/chetana/tensorboard/2023-03-07_18-13/model_ckpt_3.pt\n",
  "history_begin_time" : 1678212810307,
  "history_end_time" : 1678212815277,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "QJCRHZFLRKl3",
  "history_input" : "from model_training_utils import add_hparams, EarlyStopping\nfrom get_device_config import *\nfrom loss_function import *\nfrom set_optmizer_and_scheduler import *\nfrom set_summary_writer import *\nfrom model_utils import *\nfrom torch_metrics_utils import *\nfrom tqdm.auto import tqdm\n\n\"\"\"\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nprint(train_m)\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"MulticlassAccuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"MulticlassAccuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\"\"\"\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\nprint(model_path)\n#torch.save(model.state_dict(), model_path)",
  "history_output" : "Read 252 samples from /home/chetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 12 samples from /home/chetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\n======================================================================\nWriting Tensorboard logs to /home/chetana/tensorboard/2023-03-07_18-12\n======================================================================\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/QJCRHZFLRKl3/run_model_training.py\", line 77, in <module>\n    model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\nNameError: name 'N' is not defined\n",
  "history_begin_time" : 1678212770065,
  "history_end_time" : 1678212775033,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "ctcLRcIBvznC",
  "history_input" : "from model_training_utils import add_hparams, EarlyStopping\nfrom get_device_config import *\nfrom loss_function import *\nfrom set_optmizer_and_scheduler import *\nfrom set_summary_writer import *\nfrom model_utils import *\nfrom torch_metrics_utils import *\nfrom tqdm.auto import tqdm\n\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nprint(train_m)\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"MulticlassAccuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"MulticlassAccuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 252 samples from /home/chetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 12 samples from /home/chetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\n======================================================================\nWriting Tensorboard logs to /home/chetana/tensorboard/2023-03-07_17-45\n======================================================================\n\nTraining:   0%|                                   | 0/250 [00:00<?, ?epoch(s)/s]\nTraining:   0%| | 0/250 [00:12<?, ?epoch(s)/s, train_multiclassaccuracy=0.244053\nTraining:   0%| | 1/250 [00:12<51:41, 12.46s/epoch(s), train_multiclassaccuracy=\nTraining:   0%| | 1/250 [00:22<51:41, 12.46s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 2/250 [00:22<46:35, 11.27s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 2/250 [00:33<46:35, 11.27s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 3/250 [00:33<45:00, 10.94s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 3/250 [00:43<45:00, 10.94s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 4/250 [00:43<43:49, 10.69s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 4/250 [00:54<43:49, 10.69s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 5/250 [00:54<43:06, 10.56s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 5/250 [01:04<43:06, 10.56s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 6/250 [01:04<42:40, 10.50s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 6/250 [01:14<42:40, 10.50s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 7/250 [01:14<42:11, 10.42s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 7/250 [01:24<42:11, 10.42s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 8/250 [01:24<41:40, 10.33s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 8/250 [01:35<41:40, 10.33s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 9/250 [01:35<41:34, 10.35s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 9/250 [01:45<41:34, 10.35s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 10/250 [01:45<40:52, 10.22s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 10/250 [01:55<40:52, 10.22s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 11/250 [01:55<40:31, 10.17s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 11/250 [02:05<40:31, 10.17s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 12/250 [02:05<40:15, 10.15s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 12/250 [02:15<40:15, 10.15s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 13/250 [02:15<40:16, 10.20s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 13/250 [02:26<40:16, 10.20s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 14/250 [02:26<40:21, 10.26s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 14/250 [02:35<40:21, 10.26s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 15/250 [02:35<39:45, 10.15s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 15/250 [02:46<39:45, 10.15s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 16/250 [02:46<39:38, 10.16s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 16/250 [02:56<39:38, 10.16s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 17/250 [02:56<39:19, 10.13s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 17/250 [03:06<39:19, 10.13s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 18/250 [03:06<39:18, 10.17s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 18/250 [03:16<39:18, 10.17s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 19/250 [03:16<39:05, 10.15s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 19/250 [03:26<39:05, 10.15s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 20/250 [03:26<39:05, 10.20s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 20/250 [03:36<39:05, 10.20s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 21/250 [03:36<38:50, 10.18s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 21/250 [03:47<38:50, 10.18s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 22/250 [03:47<38:40, 10.18s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 22/250 [03:57<38:40, 10.18s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 23/250 [03:57<38:32, 10.19s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 23/250 [04:07<38:32, 10.19s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 24/250 [04:07<38:07, 10.12s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 24/250 [04:17<38:07, 10.12s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 25/250 [04:17<38:01, 10.14s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 25/250 [04:27<38:01, 10.14s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 26/250 [04:27<37:38, 10.08s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 26/250 [04:37<37:38, 10.08s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 27/250 [04:37<37:42, 10.14s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 27/250 [04:47<37:42, 10.14s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 28/250 [04:47<37:16, 10.08s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 28/250 [04:58<37:16, 10.08s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 29/250 [04:58<37:30, 10.18s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 29/250 [05:07<37:30, 10.18s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 30/250 [05:07<36:56, 10.07s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 30/250 [05:18<36:56, 10.07s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 31/250 [05:18<36:53, 10.11s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 31/250 [05:28<36:53, 10.11s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 32/250 [05:28<36:51, 10.14s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 32/250 [05:38<36:51, 10.14s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 33/250 [05:38<36:40, 10.14s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 33/250 [05:48<36:40, 10.14s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 34/250 [05:48<36:39, 10.18s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 34/250 [05:59<36:39, 10.18s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 35/250 [05:59<36:35, 10.21s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 35/250 [06:09<36:35, 10.21s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 36/250 [06:09<36:29, 10.23s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 36/250 [06:19<36:29, 10.23s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 37/250 [06:19<36:06, 10.17s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 37/250 [06:29<36:06, 10.17s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 38/250 [06:29<35:53, 10.16s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 38/250 [06:39<35:53, 10.16s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 39/250 [06:39<35:37, 10.13s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 39/250 [06:49<35:37, 10.13s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 40/250 [06:49<35:35, 10.17s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 40/250 [06:59<35:35, 10.17s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 41/250 [06:59<35:12, 10.11s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 41/250 [07:10<35:12, 10.11s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 42/250 [07:10<35:12, 10.16s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 42/250 [07:20<35:12, 10.16s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 43/250 [07:20<35:00, 10.15s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 43/250 [07:30<35:00, 10.15s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 44/250 [07:30<35:00, 10.20s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 44/250 [07:40<35:00, 10.20s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 45/250 [07:40<34:55, 10.22s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 45/250 [07:50<34:55, 10.22s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 46/250 [07:50<34:37, 10.19s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 46/250 [08:01<34:37, 10.19s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 47/250 [08:01<34:29, 10.20s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 47/250 [08:11<34:29, 10.20s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 48/250 [08:11<34:11, 10.16s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 48/250 [08:21<34:11, 10.16s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 49/250 [08:21<34:04, 10.17s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 49/250 [08:31<34:04, 10.17s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 50/250 [08:31<33:41, 10.11s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 50/250 [08:41<33:41, 10.11s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 51/250 [08:41<33:33, 10.12s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 51/250 [08:51<33:33, 10.12s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 52/250 [08:51<33:17, 10.09s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 52/250 [09:01<33:17, 10.09s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 53/250 [09:01<33:11, 10.11s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 53/250 [09:11<33:11, 10.11s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 54/250 [09:11<33:00, 10.10s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 54/250 [09:21<33:00, 10.10s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 55/250 [09:21<32:54, 10.13s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 55/250 [09:31<32:54, 10.13s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 56/250 [09:31<32:39, 10.10s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 56/250 [09:42<32:39, 10.10s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 57/250 [09:42<32:32, 10.12s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 57/250 [09:52<32:32, 10.12s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 58/250 [09:52<32:17, 10.09s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 58/250 [10:02<32:17, 10.09s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 59/250 [10:02<32:12, 10.12s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 59/250 [10:12<32:12, 10.12s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 60/250 [10:12<32:03, 10.13s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 60/250 [10:22<32:03, 10.13s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 61/250 [10:22<31:45, 10.08s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 61/250 [10:32<31:45, 10.08s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▏| 62/250 [10:32<31:42, 10.12s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▏| 62/250 [10:42<31:42, 10.12s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▎| 63/250 [10:42<31:27, 10.10s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▎| 63/250 [10:52<31:27, 10.10s/epoch(s), train_multiclassaccuracyEarly stopping at epoch 63 with validation loss 0.000 and training loss 1.010\n\nTraining:  25%|▎| 63/250 [10:52<32:17, 10.36s/epoch(s), train_multiclassaccuracy\n{'MulticlassAccuracy': tensor(0.6377)}\n",
  "history_begin_time" : 1678211089162,
  "history_end_time" : 1678211770583,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "TP46kpenHUD0",
  "history_input" : "from model_training_utils import add_hparams, EarlyStopping\nfrom get_device_config import *\nfrom loss_function import *\nfrom set_optmizer_and_scheduler import *\nfrom set_summary_writer import *\nfrom model_utils import *\nfrom torch_metrics_utils import *\nfrom tqdm.auto import tqdm\n\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nprint(train_m)\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 252 samples from /home/chetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 12 samples from /home/chetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\n======================================================================\nWriting Tensorboard logs to /home/chetana/tensorboard/2023-03-06_19-15\n======================================================================\n\nTraining:   0%|                                   | 0/250 [00:00<?, ?epoch(s)/s]\nTraining:   0%| | 0/250 [00:11<?, ?epoch(s)/s, train_multiclassaccuracy=0.244053\nTraining:   0%| | 1/250 [00:11<46:09, 11.12s/epoch(s), train_multiclassaccuracy=\nTraining:   0%| | 1/250 [00:21<46:09, 11.12s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 2/250 [00:21<43:48, 10.60s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 2/250 [00:31<43:48, 10.60s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 3/250 [00:31<43:42, 10.62s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 3/250 [00:42<43:42, 10.62s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 4/250 [00:42<43:09, 10.53s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 4/250 [00:52<43:09, 10.53s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 5/250 [00:52<42:41, 10.45s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 5/250 [01:02<42:41, 10.45s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 6/250 [01:02<41:54, 10.31s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 6/250 [01:12<41:54, 10.31s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 7/250 [01:12<41:27, 10.24s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 7/250 [01:22<41:27, 10.24s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 8/250 [01:22<41:00, 10.17s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 8/250 [01:33<41:00, 10.17s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 9/250 [01:33<40:50, 10.17s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 9/250 [01:43<40:50, 10.17s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 10/250 [01:43<40:32, 10.13s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 10/250 [01:52<40:32, 10.13s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 11/250 [01:52<40:05, 10.06s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 11/250 [02:03<40:05, 10.06s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 12/250 [02:03<40:03, 10.10s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 12/250 [02:13<40:03, 10.10s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 13/250 [02:13<39:36, 10.03s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 13/250 [02:23<39:36, 10.03s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 14/250 [02:23<39:36, 10.07s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 14/250 [02:33<39:36, 10.07s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 15/250 [02:33<39:14, 10.02s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 15/250 [02:43<39:14, 10.02s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 16/250 [02:43<39:10, 10.05s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 16/250 [02:53<39:10, 10.05s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 17/250 [02:53<38:49, 10.00s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 17/250 [03:03<38:49, 10.00s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 18/250 [03:03<38:39, 10.00s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 18/250 [03:13<38:39, 10.00s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 19/250 [03:13<38:26,  9.99s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 19/250 [03:23<38:26,  9.99s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 20/250 [03:23<38:23, 10.02s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 20/250 [03:32<38:23, 10.02s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 21/250 [03:32<38:00,  9.96s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 21/250 [03:42<38:00,  9.96s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 22/250 [03:42<37:48,  9.95s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 22/250 [03:52<37:48,  9.95s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 23/250 [03:52<37:41,  9.96s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 23/250 [04:02<37:41,  9.96s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 24/250 [04:02<37:33,  9.97s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 24/250 [04:12<37:33,  9.97s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 25/250 [04:12<37:32, 10.01s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 25/250 [04:22<37:32, 10.01s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 26/250 [04:22<37:16,  9.98s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 26/250 [04:33<37:16,  9.98s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 27/250 [04:33<37:21, 10.05s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 27/250 [04:43<37:21, 10.05s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 28/250 [04:43<37:07, 10.03s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 28/250 [04:53<37:07, 10.03s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 29/250 [04:53<37:09, 10.09s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 29/250 [05:03<37:09, 10.09s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 30/250 [05:03<36:43, 10.02s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 30/250 [05:13<36:43, 10.02s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 31/250 [05:13<36:35, 10.03s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 31/250 [05:23<36:35, 10.03s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 32/250 [05:23<36:23, 10.02s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 32/250 [05:33<36:23, 10.02s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 33/250 [05:33<36:03,  9.97s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 33/250 [05:43<36:03,  9.97s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 34/250 [05:43<35:53,  9.97s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 34/250 [05:52<35:53,  9.97s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 35/250 [05:52<35:33,  9.92s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 35/250 [06:02<35:33,  9.92s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 36/250 [06:02<35:34,  9.97s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 36/250 [06:12<35:34,  9.97s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 37/250 [06:12<35:28,  9.99s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 37/250 [06:23<35:28,  9.99s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 38/250 [06:23<35:40, 10.09s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 38/250 [06:33<35:40, 10.09s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 39/250 [06:33<35:20, 10.05s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 39/250 [06:43<35:20, 10.05s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 40/250 [06:43<35:23, 10.11s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 40/250 [06:53<35:23, 10.11s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 41/250 [06:53<35:04, 10.07s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 41/250 [07:03<35:04, 10.07s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 42/250 [07:03<34:58, 10.09s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 42/250 [07:13<34:58, 10.09s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 43/250 [07:13<35:00, 10.15s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 43/250 [07:24<35:00, 10.15s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 44/250 [07:24<35:27, 10.33s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 44/250 [07:35<35:27, 10.33s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 45/250 [07:35<35:42, 10.45s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 45/250 [07:45<35:42, 10.45s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 46/250 [07:45<35:06, 10.33s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 46/250 [07:55<35:06, 10.33s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 47/250 [07:55<34:43, 10.26s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 47/250 [08:05<34:43, 10.26s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 48/250 [08:05<34:32, 10.26s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 48/250 [08:16<34:32, 10.26s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 49/250 [08:16<34:46, 10.38s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 49/250 [08:26<34:46, 10.38s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 50/250 [08:26<34:19, 10.30s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 50/250 [08:37<34:19, 10.30s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 51/250 [08:37<34:33, 10.42s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 51/250 [08:47<34:33, 10.42s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 52/250 [08:47<34:20, 10.41s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 52/250 [08:57<34:20, 10.41s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 53/250 [08:57<34:08, 10.40s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 53/250 [09:07<34:08, 10.40s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 54/250 [09:07<33:25, 10.23s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 54/250 [09:18<33:25, 10.23s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 55/250 [09:18<33:26, 10.29s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 55/250 [09:28<33:26, 10.29s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 56/250 [09:28<32:56, 10.19s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 56/250 [09:38<32:56, 10.19s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 57/250 [09:38<32:47, 10.20s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 57/250 [09:48<32:47, 10.20s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 58/250 [09:48<32:57, 10.30s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 58/250 [09:59<32:57, 10.30s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 59/250 [09:59<33:20, 10.47s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 59/250 [10:10<33:20, 10.47s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 60/250 [10:10<33:24, 10.55s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 60/250 [10:20<33:24, 10.55s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 61/250 [10:20<33:03, 10.49s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 61/250 [10:31<33:03, 10.49s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▏| 62/250 [10:31<32:27, 10.36s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▏| 62/250 [10:41<32:27, 10.36s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▎| 63/250 [10:41<32:11, 10.33s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▎| 63/250 [10:51<32:11, 10.33s/epoch(s), train_multiclassaccuracyEarly stopping at epoch 63 with validation loss 0.000 and training loss 1.010\n\nTraining:  25%|▎| 63/250 [10:51<32:13, 10.34s/epoch(s), train_multiclassaccuracy\n{'MulticlassAccuracy': tensor(0.6377)}\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/TP46kpenHUD0/run_model_training.py\", line 69, in <module>\n    \"train/Accuracy\": train_m[\"Accuracy\"],\nKeyError: 'Accuracy'\n",
  "history_begin_time" : 1678130122322,
  "history_end_time" : 1678130778806,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "26ktdkvhn11",
  "history_input" : "from model_training_utils import add_hparams, EarlyStopping\nfrom get_device_config import *\nfrom loss_function import *\nfrom set_optmizer_and_scheduler import *\nfrom set_summary_writer import *\nfrom model_utils import *\nfrom torch_metrics_utils import *\nfrom tqdm.auto import tqdm\n\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 252 samples from /home/chetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 12 samples from /home/chetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\n======================================================================\nWriting Tensorboard logs to /home/chetana/tensorboard/2023-03-06_18-43\n======================================================================\n\nTraining:   0%|                                   | 0/250 [00:00<?, ?epoch(s)/s]\nTraining:   0%| | 0/250 [00:11<?, ?epoch(s)/s, train_multiclassaccuracy=0.244053\nTraining:   0%| | 1/250 [00:11<46:06, 11.11s/epoch(s), train_multiclassaccuracy=\nTraining:   0%| | 1/250 [00:21<46:06, 11.11s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 2/250 [00:21<44:18, 10.72s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 2/250 [00:32<44:18, 10.72s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 3/250 [00:32<43:54, 10.67s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 3/250 [00:42<43:54, 10.67s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 4/250 [00:42<43:13, 10.54s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 4/250 [00:52<43:13, 10.54s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 5/250 [00:52<42:49, 10.49s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 5/250 [01:03<42:49, 10.49s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 6/250 [01:03<42:13, 10.38s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 6/250 [01:13<42:13, 10.38s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 7/250 [01:13<41:55, 10.35s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 7/250 [01:23<41:55, 10.35s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 8/250 [01:23<41:28, 10.28s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 8/250 [01:33<41:28, 10.28s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 9/250 [01:33<41:31, 10.34s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 9/250 [01:44<41:31, 10.34s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 10/250 [01:44<41:49, 10.46s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 10/250 [01:54<41:49, 10.46s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 11/250 [01:54<41:10, 10.34s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 11/250 [02:05<41:10, 10.34s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 12/250 [02:05<41:03, 10.35s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 12/250 [02:15<41:03, 10.35s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 13/250 [02:15<40:38, 10.29s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 13/250 [02:25<40:38, 10.29s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 14/250 [02:25<40:23, 10.27s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 14/250 [02:35<40:23, 10.27s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 15/250 [02:35<39:50, 10.17s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 15/250 [02:45<39:50, 10.17s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 16/250 [02:45<39:41, 10.18s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 16/250 [02:55<39:41, 10.18s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 17/250 [02:55<39:21, 10.13s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 17/250 [03:05<39:21, 10.13s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 18/250 [03:05<39:01, 10.09s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 18/250 [03:15<39:01, 10.09s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 19/250 [03:15<38:29, 10.00s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 19/250 [03:25<38:29, 10.00s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 20/250 [03:25<38:21, 10.01s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 20/250 [03:35<38:21, 10.01s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 21/250 [03:35<38:12, 10.01s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 21/250 [03:45<38:12, 10.01s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 22/250 [03:45<38:03, 10.01s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 22/250 [03:55<38:03, 10.01s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 23/250 [03:55<37:54, 10.02s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 23/250 [04:05<37:54, 10.02s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 24/250 [04:05<37:34,  9.98s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 24/250 [04:15<37:34,  9.98s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 25/250 [04:15<37:23,  9.97s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 25/250 [04:25<37:23,  9.97s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 26/250 [04:25<37:18,  9.99s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 26/250 [04:35<37:18,  9.99s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 27/250 [04:35<37:20, 10.05s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 27/250 [04:45<37:20, 10.05s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 28/250 [04:45<36:59, 10.00s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 28/250 [04:55<36:59, 10.00s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 29/250 [04:55<36:48,  9.99s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 29/250 [05:05<36:48,  9.99s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 30/250 [05:05<36:37,  9.99s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 30/250 [05:15<36:37,  9.99s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 31/250 [05:15<36:31, 10.01s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 31/250 [05:25<36:31, 10.01s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 32/250 [05:25<36:19, 10.00s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 32/250 [05:35<36:19, 10.00s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 33/250 [05:35<36:05,  9.98s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 33/250 [05:45<36:05,  9.98s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 34/250 [05:45<36:06, 10.03s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 34/250 [05:55<36:06, 10.03s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 35/250 [05:55<35:54, 10.02s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 35/250 [06:05<35:54, 10.02s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 36/250 [06:05<35:59, 10.09s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 36/250 [06:15<35:59, 10.09s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 37/250 [06:15<35:38, 10.04s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 37/250 [06:25<35:38, 10.04s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 38/250 [06:25<35:41, 10.10s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 38/250 [06:35<35:41, 10.10s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 39/250 [06:35<35:24, 10.07s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 39/250 [06:46<35:24, 10.07s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 40/250 [06:46<35:24, 10.12s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 40/250 [06:56<35:24, 10.12s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 41/250 [06:56<35:06, 10.08s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 41/250 [07:06<35:06, 10.08s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 42/250 [07:06<34:59, 10.09s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 42/250 [07:16<34:59, 10.09s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 43/250 [07:16<34:41, 10.06s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 43/250 [07:26<34:41, 10.06s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 44/250 [07:26<34:39, 10.09s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 44/250 [07:36<34:39, 10.09s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 45/250 [07:36<34:29, 10.09s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 45/250 [07:46<34:29, 10.09s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 46/250 [07:46<34:14, 10.07s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 46/250 [07:56<34:14, 10.07s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 47/250 [07:56<34:06, 10.08s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 47/250 [08:06<34:06, 10.08s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 48/250 [08:06<33:49, 10.05s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 48/250 [08:16<33:49, 10.05s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 49/250 [08:16<33:40, 10.05s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 49/250 [08:26<33:40, 10.05s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 50/250 [08:26<33:14,  9.97s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 50/250 [08:36<33:14,  9.97s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 51/250 [08:36<33:16, 10.03s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 51/250 [08:46<33:16, 10.03s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 52/250 [08:46<33:06, 10.03s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 52/250 [08:56<33:06, 10.03s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 53/250 [08:56<33:01, 10.06s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 53/250 [09:06<33:01, 10.06s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 54/250 [09:06<32:37,  9.99s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 54/250 [09:16<32:37,  9.99s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 55/250 [09:16<32:43, 10.07s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 55/250 [09:26<32:43, 10.07s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 56/250 [09:26<32:24, 10.02s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 56/250 [09:36<32:24, 10.02s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 57/250 [09:36<32:16, 10.03s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 57/250 [09:46<32:16, 10.03s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 58/250 [09:46<32:04, 10.02s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 58/250 [09:57<32:04, 10.02s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 59/250 [09:57<32:03, 10.07s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 59/250 [10:07<32:03, 10.07s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 60/250 [10:07<31:56, 10.09s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 60/250 [10:17<31:56, 10.09s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 61/250 [10:17<31:37, 10.04s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 61/250 [10:27<31:37, 10.04s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▏| 62/250 [10:27<31:27, 10.04s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▏| 62/250 [10:37<31:27, 10.04s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▎| 63/250 [10:37<31:10, 10.00s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▎| 63/250 [10:47<31:10, 10.00s/epoch(s), train_multiclassaccuracyEarly stopping at epoch 63 with validation loss 0.000 and training loss 1.010\n\nTraining:  25%|▎| 63/250 [10:47<32:00, 10.27s/epoch(s), train_multiclassaccuracy\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/26ktdkvhn11/run_model_training.py\", line 68, in <module>\n    \"train/Accuracy\": train_m[\"Accuracy\"],\nKeyError: 'Accuracy'\n",
  "history_begin_time" : 1678128199100,
  "history_end_time" : 1678128852414,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "ycru82",
  "indicator" : "Failed"
},{
  "history_id" : "arnbv0uscta",
  "history_input" : "from model_training_utils import add_hparams, EarlyStopping\nfrom get_device_config import *\nfrom loss_function import *\nfrom set_optmizer_and_scheduler import *\nfrom set_summary_writer import *\nfrom model_utils import *\nfrom torch_metrics_utils import *\nfrom tqdm.auto import tqdm\n\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 252 samples from /home/chetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 12 samples from /home/chetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\n======================================================================\nWriting Tensorboard logs to /home/chetana/tensorboard/2023-03-06_18-37\n======================================================================\n\nTraining:   0%|                                   | 0/250 [00:00<?, ?epoch(s)/s]\nTraining:   0%|                                   | 0/250 [00:12<?, ?epoch(s)/s]\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/arnbv0uscta/run_model_training.py\", line 23, in <module>\n    train_loss, val_loss, train_m, val_m = run_epoch(\n  File \"/home/chetana/gw-workspace/arnbv0uscta/model_utils.py\", line 87, in run_epoch\n    num_classes, train_metrics, writer, epoch, \"train\"\nNameError: name 'num_classes' is not defined\n",
  "history_begin_time" : 1678127843574,
  "history_end_time" : 1678127862811,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "ycru82",
  "indicator" : "Failed"
},{
  "history_id" : "ckg23dvxodk",
  "history_input" : "from model_training_utils import add_hparams, EarlyStopping\nfrom get_device_config import *\nfrom loss_function import *\nfrom set_optmizer_and_scheduler import *\nfrom set_summary_writer import *\nfrom model_utils import *\nfrom torch_metrics_utils import *\nfrom tqdm.auto import tqdm\n\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 252 samples from /home/chetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 12 samples from /home/chetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/ckg23dvxodk/run_model_training.py\", line 5, in <module>\n    from set_summary_writer import *\n  File \"/home/chetana/gw-workspace/ckg23dvxodk/set_summary_writer.py\", line 14\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\n    ^\nSyntaxError: unterminated string literal (detected at line 14)\n",
  "history_begin_time" : 1678127441382,
  "history_end_time" : 1678127449404,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "ycru82",
  "indicator" : "Failed"
},{
  "history_id" : "uyghs2mcflc",
  "history_input" : "from model_training_utils import add_hparams, EarlyStopping\nfrom get_device_config import *\nfrom loss_function import *\nfrom set_optmizer_and_scheduler import *\nfrom set_summary_writer import *\nfrom model_utils import *\nfrom torch_metrics_utils import *\nfrom tqdm.auto import tqdm\n\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 252 samples from /home/chetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 12 samples from /home/chetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/uyghs2mcflc/run_model_training.py\", line 4, in <module>\n    from set_optmizer_and_scheduler import *\n  File \"/home/chetana/gw-workspace/uyghs2mcflc/set_optmizer_and_scheduler.py\", line 7, in <module>\n    optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nNameError: name 'model' is not defined\n",
  "history_begin_time" : 1678125764664,
  "history_end_time" : 1678125770764,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "ycru82",
  "indicator" : "Failed"
},{
  "history_id" : "l9quwab3rwy",
  "history_input" : "from model_training_utils import add_hparams, EarlyStopping\nfrom get_device_config import *\nfrom loss_function import *\nfrom set_optmizer_and_scheduler import *\nfrom set_summary_writer import *\nfrom model_utils import *\nfrom torch_metrics_utils import *\nfrom tqdm.auto import tqdm\n\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/l9quwab3rwy/run_model_training.py\", line 1, in <module>\n    from model_training_utils import add_hparams, EarlyStopping\n  File \"/home/chetana/gw-workspace/l9quwab3rwy/model_training_utils.py\", line 5, in <module>\n    from torch.utils.tensorboard.summary import hparams\n  File \"/home/chetana/anaconda3/envs/ranjan/lib/python3.10/site-packages/torch/utils/tensorboard/__init__.py\", line 1, in <module>\n    import tensorboard\nModuleNotFoundError: No module named 'tensorboard'\n",
  "history_begin_time" : 1677775410240,
  "history_end_time" : 1677775414369,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "ycru82",
  "indicator" : "Failed"
},{
  "history_id" : "slwkxs0xuj9",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1678249820052,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "ycru82",
  "indicator" : "Stopped"
},{
  "history_id" : "4vxrws3jrki",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1678250276599,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "ycru82",
  "indicator" : "Stopped"
},]
