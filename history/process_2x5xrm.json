[{
  "history_id" : "r2jfrdbnf7w",
  "history_input" : "from training_and_plot_utils import *\nfrom model_components import *\nfrom device_config_and_data_loader import *\nfrom tqdm.auto import tqdm\n\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nprint(train_m)\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"MulticlassAccuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"MulticlassAccuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_final_full_data.pt\")\n\nprint(model_path)\n\n\nprint(\"train/Accuracy\", train_m[\"MulticlassAccuracy\"])\nprint(\"val/Accuracy\", val_m[\"MulticlassAccuracy\"])\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 24 samples from /home/chetana/ML_eddie/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-1999_lat14N-46N_lon166W-134W.npz.\nRead 12 samples from /home/chetana/ML_eddie/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\nRead 24 samples from /home/chetana/ML_eddie/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-1999_lat14N-46N_lon166W-134W.npz.\nRead 12 samples from /home/chetana/ML_eddie/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\nTotal number of pixels in training set: 0.39 megapixels across 24 SSH maps\nNumber of pixels that are not eddies: 0.28 megapixels (72.00%)\nNumber of pixels that are anticyclonic eddies: 0.06 megapixels (14.23%)\nNumber of pixels that are cyclonic eddies: 0.05 megapixels (13.77%)\n\n======================================================================\nWriting Tensorboard logs to /home/chetana/tensorboard/2023-04-25_17-34\n======================================================================\n\nTraining:   0%|                                   | 0/250 [00:00<?, ?epoch(s)/s]\nTraining:   0%| | 0/250 [00:11<?, ?epoch(s)/s, train_multiclassaccuracy=0.239357\nTraining:   0%| | 1/250 [00:11<49:03, 11.82s/epoch(s), train_multiclassaccuracy=\nTraining:   0%| | 1/250 [00:23<49:03, 11.82s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 2/250 [00:23<47:39, 11.53s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 2/250 [00:34<47:39, 11.53s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 3/250 [00:34<46:52, 11.39s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 3/250 [00:42<46:52, 11.39s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 4/250 [00:42<40:45,  9.94s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 4/250 [00:47<40:45,  9.94s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 5/250 [00:47<33:58,  8.32s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 5/250 [00:52<33:58,  8.32s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 6/250 [00:52<29:27,  7.24s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 6/250 [00:58<29:27,  7.24s/epoch(s), train_multiclassaccuracy=\n",
  "history_begin_time" : 1682444095865,
  "history_end_time" : 1682444094448,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "c2lqcn",
  "indicator" : "Running"
},{
  "history_id" : "g5l4de82gn3",
  "history_input" : "from training_and_plot_utils import *\nfrom model_components import *\nfrom device_config_and_data_loader import *\nfrom tqdm.auto import tqdm\n\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nprint(train_m)\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"MulticlassAccuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"MulticlassAccuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_final_full_data.pt\")\n\nprint(model_path)\n\n\nprint(\"train/Accuracy\", train_m[\"MulticlassAccuracy\"])\nprint(\"val/Accuracy\", val_m[\"MulticlassAccuracy\"])\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 24 samples from /home/chetana/ML_eddie/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-1999_lat14N-46N_lon166W-134W.npz.\nRead 12 samples from /home/chetana/ML_eddie/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\nRead 24 samples from /home/chetana/ML_eddie/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-1999_lat14N-46N_lon166W-134W.npz.\nRead 12 samples from /home/chetana/ML_eddie/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\nTotal number of pixels in training set: 0.39 megapixels across 24 SSH maps\nNumber of pixels that are not eddies: 0.28 megapixels (72.00%)\nNumber of pixels that are anticyclonic eddies: 0.06 megapixels (14.23%)\nNumber of pixels that are cyclonic eddies: 0.05 megapixels (13.77%)\n\n======================================================================\nWriting Tensorboard logs to /home/chetana/tensorboard/2023-04-25_17-29\n======================================================================\n\nTraining:   0%|                                   | 0/250 [00:00<?, ?epoch(s)/s]\nTraining:   0%| | 0/250 [00:05<?, ?epoch(s)/s, train_multiclassaccuracy=0.239357\nTraining:   0%| | 1/250 [00:05<21:19,  5.14s/epoch(s), train_multiclassaccuracy=\nTraining:   0%| | 1/250 [00:10<21:19,  5.14s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 2/250 [00:10<20:36,  4.99s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 2/250 [00:15<20:36,  4.99s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 3/250 [00:15<20:52,  5.07s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 3/250 [00:19<20:52,  5.07s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 4/250 [00:19<20:13,  4.93s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 4/250 [00:24<20:13,  4.93s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 5/250 [00:24<20:17,  4.97s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 5/250 [00:29<20:17,  4.97s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 6/250 [00:29<19:52,  4.89s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 6/250 [00:34<19:52,  4.89s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 7/250 [00:34<19:52,  4.91s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 7/250 [00:39<19:52,  4.91s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 8/250 [00:39<19:45,  4.90s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 8/250 [00:44<19:45,  4.90s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 9/250 [00:44<20:13,  5.04s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 9/250 [00:57<20:13,  5.04s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 10/250 [00:57<29:46,  7.44s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 10/250 [01:09<29:46,  7.44s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 11/250 [01:09<35:21,  8.88s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 11/250 [01:24<35:21,  8.88s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 12/250 [01:24<42:02, 10.60s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 12/250 [01:36<42:02, 10.60s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 13/250 [01:36<43:44, 11.07s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 13/250 [01:48<43:44, 11.07s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 14/250 [01:48<44:40, 11.36s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 14/250 [01:59<44:40, 11.36s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 15/250 [01:59<43:28, 11.10s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 15/250 [02:04<43:28, 11.10s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 16/250 [02:04<36:10,  9.28s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 16/250 [02:08<36:10,  9.28s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 17/250 [02:08<30:50,  7.94s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 17/250 [02:13<30:50,  7.94s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 18/250 [02:13<27:14,  7.05s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 18/250 [02:18<27:14,  7.05s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 19/250 [02:18<24:36,  6.39s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 19/250 [02:23<24:36,  6.39s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 20/250 [02:23<22:37,  5.90s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 20/250 [02:28<22:37,  5.90s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 21/250 [02:28<21:38,  5.67s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 21/250 [02:33<21:38,  5.67s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 22/250 [02:33<20:34,  5.42s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 22/250 [02:38<20:34,  5.42s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 23/250 [02:38<19:57,  5.28s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 23/250 [02:43<19:57,  5.28s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 24/250 [02:43<19:15,  5.11s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 24/250 [02:48<19:15,  5.11s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 25/250 [02:48<19:04,  5.09s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 25/250 [02:53<19:04,  5.09s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 26/250 [02:53<19:31,  5.23s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 26/250 [02:59<19:31,  5.23s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 27/250 [02:59<20:19,  5.47s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 27/250 [03:04<20:19,  5.47s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 28/250 [03:04<19:49,  5.36s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 28/250 [03:10<19:49,  5.36s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 29/250 [03:10<19:40,  5.34s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 29/250 [03:15<19:40,  5.34s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 30/250 [03:15<19:31,  5.32s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 30/250 [03:20<19:31,  5.32s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 31/250 [03:20<19:10,  5.25s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 31/250 [03:25<19:10,  5.25s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 32/250 [03:25<19:01,  5.23s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 32/250 [03:30<19:01,  5.23s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 33/250 [03:30<18:53,  5.22s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 33/250 [03:36<18:53,  5.22s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 34/250 [03:36<18:51,  5.24s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 34/250 [03:41<18:51,  5.24s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 35/250 [03:41<18:31,  5.17s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 35/250 [03:46<18:31,  5.17s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 36/250 [03:46<18:31,  5.19s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 36/250 [03:51<18:31,  5.19s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 37/250 [03:51<18:28,  5.20s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 37/250 [03:57<18:28,  5.20s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 38/250 [03:57<18:38,  5.27s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 38/250 [04:02<18:38,  5.27s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 39/250 [04:02<18:23,  5.23s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 39/250 [04:07<18:23,  5.23s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 40/250 [04:07<18:22,  5.25s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 40/250 [04:12<18:22,  5.25s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 41/250 [04:12<18:26,  5.29s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 41/250 [04:18<18:26,  5.29s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 42/250 [04:18<18:16,  5.27s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 42/250 [04:23<18:16,  5.27s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 43/250 [04:23<18:05,  5.24s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 43/250 [04:28<18:05,  5.24s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 44/250 [04:28<17:29,  5.10s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 44/250 [04:33<17:29,  5.10s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 45/250 [04:33<17:16,  5.06s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 45/250 [04:37<17:16,  5.06s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 46/250 [04:37<16:56,  4.98s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 46/250 [04:42<16:56,  4.98s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 47/250 [04:42<16:56,  5.01s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 47/250 [04:48<16:56,  5.01s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 48/250 [04:48<17:06,  5.08s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 48/250 [04:53<17:06,  5.08s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 49/250 [04:53<17:18,  5.17s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 49/250 [04:58<17:18,  5.17s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 50/250 [04:58<17:11,  5.16s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 50/250 [05:03<17:11,  5.16s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 51/250 [05:03<17:10,  5.18s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 51/250 [05:08<17:10,  5.18s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 52/250 [05:08<16:41,  5.06s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 52/250 [05:13<16:41,  5.06s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 53/250 [05:13<16:41,  5.08s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 53/250 [05:18<16:41,  5.08s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 54/250 [05:18<16:20,  5.00s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 54/250 [05:23<16:20,  5.00s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 55/250 [05:23<16:21,  5.03s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 55/250 [05:28<16:21,  5.03s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 56/250 [05:28<16:19,  5.05s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 56/250 [05:34<16:19,  5.05s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 57/250 [05:34<16:23,  5.10s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 57/250 [05:39<16:23,  5.10s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 58/250 [05:39<16:24,  5.13s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 58/250 [05:50<16:24,  5.13s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 59/250 [05:50<22:28,  7.06s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 59/250 [06:02<22:28,  7.06s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 60/250 [06:02<26:28,  8.36s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 60/250 [06:13<26:28,  8.36s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 61/250 [06:13<29:25,  9.34s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 61/250 [06:21<29:25,  9.34s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▏| 62/250 [06:21<28:06,  8.97s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▏| 62/250 [06:27<28:06,  8.97s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▎| 63/250 [06:27<25:03,  8.04s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▎| 63/250 [06:33<25:03,  8.04s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 64/250 [06:33<22:40,  7.31s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 64/250 [06:38<22:40,  7.31s/epoch(s), train_multiclassaccuracy\n",
  "history_begin_time" : 1682443755511,
  "history_end_time" : 1682443925655,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "c2lqcn",
  "indicator" : "Running"
},{
  "history_id" : "ahep5wd3bdw",
  "history_input" : "from training_and_plot_utils import *\nfrom model_components import *\nfrom device_config_and_data_loader import *\nfrom tqdm.auto import tqdm\n\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nprint(train_m)\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"MulticlassAccuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"MulticlassAccuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_final_full_data.pt\")\n\nprint(model_path)\n\n\nprint(\"train/Accuracy\", train_m[\"MulticlassAccuracy\"])\nprint(\"val/Accuracy\", val_m[\"MulticlassAccuracy\"])\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 24 samples from /home/chetana/ML_eddie/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-1999_lat14N-46N_lon166W-134W.npz.\nRead 12 samples from /home/chetana/ML_eddie/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\nRead 24 samples from /home/chetana/ML_eddie/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-1999_lat14N-46N_lon166W-134W.npz.\nRead 12 samples from /home/chetana/ML_eddie/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\nTotal number of pixels in training set: 0.39 megapixels across 24 SSH maps\nNumber of pixels that are not eddies: 0.28 megapixels (72.00%)\nNumber of pixels that are anticyclonic eddies: 0.06 megapixels (14.23%)\nNumber of pixels that are cyclonic eddies: 0.05 megapixels (13.77%)\n\n======================================================================\nWriting Tensorboard logs to /home/chetana/tensorboard/2023-04-25_17-11\n======================================================================\n\nTraining:   0%|                                   | 0/250 [00:00<?, ?epoch(s)/s]\nTraining:   0%| | 0/250 [00:05<?, ?epoch(s)/s, train_multiclassaccuracy=0.239357\nTraining:   0%| | 1/250 [00:05<22:04,  5.32s/epoch(s), train_multiclassaccuracy=\nTraining:   0%| | 1/250 [00:10<22:04,  5.32s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 2/250 [00:10<21:18,  5.16s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 2/250 [00:15<21:18,  5.16s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 3/250 [00:15<21:14,  5.16s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 3/250 [00:20<21:14,  5.16s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 4/250 [00:20<20:36,  5.03s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 4/250 [00:25<20:36,  5.03s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 5/250 [00:25<20:32,  5.03s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 5/250 [00:30<20:32,  5.03s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 6/250 [00:30<20:05,  4.94s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 6/250 [00:35<20:05,  4.94s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 7/250 [00:35<19:59,  4.94s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 7/250 [00:40<19:59,  4.94s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 8/250 [00:40<19:57,  4.95s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 8/250 [00:44<19:57,  4.95s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 9/250 [00:44<19:41,  4.90s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 9/250 [00:49<19:41,  4.90s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 10/250 [00:49<19:42,  4.93s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 10/250 [00:54<19:42,  4.93s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 11/250 [00:54<19:25,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 11/250 [00:59<19:25,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 12/250 [00:59<19:30,  4.92s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 12/250 [01:04<19:30,  4.92s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 13/250 [01:04<19:12,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 13/250 [01:09<19:12,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 14/250 [01:09<19:41,  5.01s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 14/250 [01:14<19:41,  5.01s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 15/250 [01:14<19:17,  4.92s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 15/250 [01:19<19:17,  4.92s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 16/250 [01:19<19:08,  4.91s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 16/250 [01:24<19:08,  4.91s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 17/250 [01:24<19:12,  4.94s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 17/250 [01:29<19:12,  4.94s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 18/250 [01:29<19:04,  4.94s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 18/250 [01:34<19:04,  4.94s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 19/250 [01:34<18:56,  4.92s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 19/250 [01:38<18:56,  4.92s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 20/250 [01:38<18:46,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 20/250 [01:43<18:46,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 21/250 [01:43<18:45,  4.92s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 21/250 [01:48<18:45,  4.92s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 22/250 [01:48<18:27,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 22/250 [01:53<18:27,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 23/250 [01:53<18:27,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 23/250 [01:58<18:27,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 24/250 [01:58<18:14,  4.84s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 24/250 [02:03<18:14,  4.84s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 25/250 [02:03<18:19,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 25/250 [02:08<18:19,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 26/250 [02:08<18:07,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 26/250 [02:13<18:07,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 27/250 [02:13<18:25,  4.96s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 27/250 [02:18<18:25,  4.96s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 28/250 [02:18<18:06,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 28/250 [02:23<18:06,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 29/250 [02:23<18:09,  4.93s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 29/250 [02:27<18:09,  4.93s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 30/250 [02:27<18:02,  4.92s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 30/250 [02:32<18:02,  4.92s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 31/250 [02:32<17:44,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 31/250 [02:37<17:44,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 32/250 [02:37<17:45,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 32/250 [02:42<17:45,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 33/250 [02:42<17:31,  4.84s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 33/250 [02:47<17:31,  4.84s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 34/250 [02:47<17:29,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 34/250 [02:51<17:29,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 35/250 [02:51<17:16,  4.82s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 35/250 [02:57<17:16,  4.82s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 36/250 [02:57<17:25,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 36/250 [03:01<17:25,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 37/250 [03:01<17:15,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 37/250 [03:06<17:15,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 38/250 [03:06<17:14,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 38/250 [03:11<17:14,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 39/250 [03:11<17:00,  4.84s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 39/250 [03:16<17:00,  4.84s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 40/250 [03:16<17:01,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 40/250 [03:21<17:01,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 41/250 [03:21<17:00,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 41/250 [03:26<17:00,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 42/250 [03:26<16:49,  4.85s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 42/250 [03:31<16:49,  4.85s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 43/250 [03:31<16:50,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 43/250 [03:35<16:50,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 44/250 [03:35<16:37,  4.84s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 44/250 [03:40<16:37,  4.84s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 45/250 [03:40<16:35,  4.85s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 45/250 [03:45<16:35,  4.85s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 46/250 [03:45<16:21,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 46/250 [03:50<16:21,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 47/250 [03:50<16:22,  4.84s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 47/250 [03:55<16:22,  4.84s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 48/250 [03:55<16:10,  4.80s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 48/250 [03:59<16:10,  4.80s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 49/250 [03:59<16:10,  4.83s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 49/250 [04:04<16:10,  4.83s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 50/250 [04:04<15:59,  4.80s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 50/250 [04:09<15:59,  4.80s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 51/250 [04:09<16:01,  4.83s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 51/250 [04:14<16:01,  4.83s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 52/250 [04:14<15:52,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 52/250 [04:19<15:52,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 53/250 [04:19<15:55,  4.85s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 53/250 [04:24<15:55,  4.85s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 54/250 [04:24<15:44,  4.82s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 54/250 [04:28<15:44,  4.82s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 55/250 [04:28<15:45,  4.85s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 55/250 [04:33<15:45,  4.85s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 56/250 [04:33<15:36,  4.83s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 56/250 [04:38<15:36,  4.83s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 57/250 [04:38<15:36,  4.85s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 57/250 [04:43<15:36,  4.85s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 58/250 [04:43<15:23,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 58/250 [04:48<15:23,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 59/250 [04:48<15:25,  4.85s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 59/250 [04:53<15:25,  4.85s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 60/250 [04:53<15:15,  4.82s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 60/250 [04:57<15:15,  4.82s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 61/250 [04:57<15:19,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 61/250 [05:02<15:19,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▏| 62/250 [05:02<15:08,  4.83s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▏| 62/250 [05:07<15:08,  4.83s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▎| 63/250 [05:07<15:08,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▎| 63/250 [05:12<15:08,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 64/250 [05:12<15:10,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 64/250 [05:17<15:10,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 65/250 [05:17<15:02,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 65/250 [05:22<15:02,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 66/250 [05:22<15:03,  4.91s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 66/250 [05:27<15:03,  4.91s/epoch(s), train_multiclassaccuracyEarly stopping at epoch 66 with validation loss 0.000 and training loss 1.006\n\nTraining:  26%|▎| 66/250 [05:27<15:12,  4.96s/epoch(s), train_multiclassaccuracy\n{'MulticlassAccuracy': tensor(0.6333)}\n/home/chetana/tensorboard/2023-04-25_17-11/model_ckpt_final_full_data.pt\ntrain/Accuracy tensor(0.6333)\nval/Accuracy tensor(0.7205)\n",
  "history_begin_time" : 1682442713234,
  "history_end_time" : 1682443046025,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "c2lqcn",
  "indicator" : "Done"
},{
  "history_id" : "m8jtfhe0dqy",
  "history_input" : "from training_and_plot_utils import *\nfrom model_components import *\nfrom device_config_and_data_loader import *\nfrom tqdm.auto import tqdm\n\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nprint(train_m)\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"MulticlassAccuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"MulticlassAccuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_final_full_data.pt\")\n\nprint(model_path)\n\n\nprint(\"train/Accuracy\", train_m[\"MulticlassAccuracy\"])\nprint(\"val/Accuracy\", val_m[\"MulticlassAccuracy\"])\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 24 samples from /home/chetana/ML_eddie/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-1999_lat14N-46N_lon166W-134W.npz.\nRead 12 samples from /home/chetana/ML_eddie/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\nRead 24 samples from /home/chetana/ML_eddie/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-1999_lat14N-46N_lon166W-134W.npz.\nRead 12 samples from /home/chetana/ML_eddie/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\nTotal number of pixels in training set: 0.39 megapixels across 24 SSH maps\nNumber of pixels that are not eddies: 0.28 megapixels (72.00%)\nNumber of pixels that are anticyclonic eddies: 0.06 megapixels (14.23%)\nNumber of pixels that are cyclonic eddies: 0.05 megapixels (13.77%)\n\n======================================================================\nWriting Tensorboard logs to /home/chetana/tensorboard/2023-04-25_17-00\n======================================================================\n\nTraining:   0%|                                   | 0/250 [00:00<?, ?epoch(s)/s]\nTraining:   0%| | 0/250 [00:05<?, ?epoch(s)/s, train_multiclassaccuracy=0.239357\nTraining:   0%| | 1/250 [00:05<22:26,  5.41s/epoch(s), train_multiclassaccuracy=\nTraining:   0%| | 1/250 [00:10<22:26,  5.41s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 2/250 [00:10<21:22,  5.17s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 2/250 [00:15<21:22,  5.17s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 3/250 [00:15<21:28,  5.22s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 3/250 [00:20<21:28,  5.22s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 4/250 [00:20<20:44,  5.06s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 4/250 [00:25<20:44,  5.06s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 5/250 [00:25<20:56,  5.13s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 5/250 [00:30<20:56,  5.13s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 6/250 [00:30<20:29,  5.04s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 6/250 [00:35<20:29,  5.04s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 7/250 [00:35<20:27,  5.05s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 7/250 [00:40<20:27,  5.05s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 8/250 [00:40<20:21,  5.05s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 8/250 [00:45<20:21,  5.05s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 9/250 [00:45<20:00,  4.98s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 9/250 [00:50<20:00,  4.98s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 10/250 [00:50<19:53,  4.97s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 10/250 [00:55<19:53,  4.97s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 11/250 [00:55<19:52,  4.99s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 11/250 [01:00<19:52,  4.99s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 12/250 [01:00<20:08,  5.08s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 12/250 [01:05<20:08,  5.08s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 13/250 [01:05<20:06,  5.09s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 13/250 [01:11<20:06,  5.09s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 14/250 [01:11<20:12,  5.14s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 14/250 [01:16<20:12,  5.14s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 15/250 [01:16<20:10,  5.15s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 15/250 [01:21<20:10,  5.15s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 16/250 [01:21<20:17,  5.20s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 16/250 [01:27<20:17,  5.20s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 17/250 [01:27<20:19,  5.24s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 17/250 [01:32<20:19,  5.24s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 18/250 [01:32<20:08,  5.21s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 18/250 [01:37<20:08,  5.21s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 19/250 [01:37<19:54,  5.17s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 19/250 [01:42<19:54,  5.17s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 20/250 [01:42<19:39,  5.13s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 20/250 [01:47<19:39,  5.13s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 21/250 [01:47<19:39,  5.15s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 21/250 [01:52<19:39,  5.15s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 22/250 [01:52<19:27,  5.12s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 22/250 [01:57<19:27,  5.12s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 23/250 [01:57<19:30,  5.16s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 23/250 [02:03<19:30,  5.16s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 24/250 [02:03<19:35,  5.20s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 24/250 [02:08<19:35,  5.20s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 25/250 [02:08<19:49,  5.29s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 25/250 [02:13<19:49,  5.29s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 26/250 [02:13<19:25,  5.21s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 26/250 [02:18<19:25,  5.21s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 27/250 [02:18<19:29,  5.25s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 27/250 [02:24<19:29,  5.25s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 28/250 [02:24<19:25,  5.25s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 28/250 [02:29<19:25,  5.25s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 29/250 [02:29<19:20,  5.25s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 29/250 [02:34<19:20,  5.25s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 30/250 [02:34<19:08,  5.22s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 30/250 [02:39<19:08,  5.22s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 31/250 [02:39<18:53,  5.18s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 31/250 [02:45<18:53,  5.18s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 32/250 [02:45<19:02,  5.24s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 32/250 [02:50<19:02,  5.24s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 33/250 [02:50<18:41,  5.17s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 33/250 [02:55<18:41,  5.17s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 34/250 [02:55<18:39,  5.18s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 34/250 [03:00<18:39,  5.18s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 35/250 [03:00<18:23,  5.13s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 35/250 [03:05<18:23,  5.13s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 36/250 [03:05<18:26,  5.17s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 36/250 [03:10<18:26,  5.17s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 37/250 [03:10<18:13,  5.13s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 37/250 [03:15<18:13,  5.13s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 38/250 [03:15<18:15,  5.17s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 38/250 [03:20<18:15,  5.17s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 39/250 [03:20<17:55,  5.10s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 39/250 [03:25<17:55,  5.10s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 40/250 [03:25<17:52,  5.11s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 40/250 [03:30<17:52,  5.11s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 41/250 [03:30<17:44,  5.09s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 41/250 [03:35<17:44,  5.09s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 42/250 [03:35<17:29,  5.05s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 42/250 [03:40<17:29,  5.05s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 43/250 [03:40<17:27,  5.06s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 43/250 [03:45<17:27,  5.06s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 44/250 [03:45<17:13,  5.02s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 44/250 [03:51<17:13,  5.02s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 45/250 [03:51<17:19,  5.07s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 45/250 [03:56<17:19,  5.07s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 46/250 [03:56<17:11,  5.06s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 46/250 [04:01<17:11,  5.06s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 47/250 [04:01<17:20,  5.13s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 47/250 [04:06<17:20,  5.13s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 48/250 [04:06<17:24,  5.17s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 48/250 [04:11<17:24,  5.17s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 49/250 [04:11<17:14,  5.15s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 49/250 [04:17<17:14,  5.15s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 50/250 [04:17<17:14,  5.17s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 50/250 [04:22<17:14,  5.17s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 51/250 [04:22<17:08,  5.17s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 51/250 [04:26<17:08,  5.17s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 52/250 [04:26<16:39,  5.05s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 52/250 [04:32<16:39,  5.05s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 53/250 [04:32<16:38,  5.07s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 53/250 [04:36<16:38,  5.07s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 54/250 [04:36<16:21,  5.01s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 54/250 [04:42<16:21,  5.01s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 55/250 [04:42<16:29,  5.07s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 55/250 [04:47<16:29,  5.07s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 56/250 [04:47<16:13,  5.02s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 56/250 [04:51<16:13,  5.02s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 57/250 [04:52<16:05,  5.00s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 57/250 [04:56<16:05,  5.00s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 58/250 [04:56<15:58,  4.99s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 58/250 [05:01<15:58,  4.99s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 59/250 [05:01<15:52,  4.99s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 59/250 [05:06<15:52,  4.99s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 60/250 [05:06<15:36,  4.93s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 60/250 [05:11<15:36,  4.93s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 61/250 [05:11<15:30,  4.93s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 61/250 [05:16<15:30,  4.93s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▏| 62/250 [05:16<15:15,  4.87s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▏| 62/250 [05:21<15:15,  4.87s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▎| 63/250 [05:21<15:12,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▎| 63/250 [05:26<15:12,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 64/250 [05:26<15:14,  4.92s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 64/250 [05:31<15:14,  4.92s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 65/250 [05:31<15:03,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 65/250 [05:36<15:03,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 66/250 [05:36<15:04,  4.92s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 66/250 [05:40<15:04,  4.92s/epoch(s), train_multiclassaccuracyEarly stopping at epoch 66 with validation loss 0.000 and training loss 1.006\n\nTraining:  26%|▎| 66/250 [05:40<15:50,  5.16s/epoch(s), train_multiclassaccuracy\n{'MulticlassAccuracy': tensor(0.6333)}\n/home/chetana/tensorboard/2023-04-25_17-00/model_ckpt_final_full_data.pt\ntrain/Accuracy tensor(0.6333)\nval/Accuracy tensor(0.7205)\n",
  "history_begin_time" : 1682441995894,
  "history_end_time" : 1682442342767,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "c2lqcn",
  "indicator" : "Done"
},{
  "history_id" : "ut4pn5ets5p",
  "history_input" : "from training_and_plot_utils import *\nfrom model_components import *\nfrom device_config_and_data_loader import *\nfrom tqdm.auto import tqdm\n\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nprint(train_m)\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"MulticlassAccuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"MulticlassAccuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_final_full_data.pt\")\n\nprint(model_path)\n\n\nprint(\"train/Accuracy\", train_m[\"MulticlassAccuracy\"])\nprint(\"val/Accuracy\", val_m[\"MulticlassAccuracy\"])\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 24 samples from /home/chetana/ML_eddie/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-1999_lat14N-46N_lon166W-134W.npz.\nRead 12 samples from /home/chetana/ML_eddie/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\nRead 24 samples from /home/chetana/ML_eddie/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-1999_lat14N-46N_lon166W-134W.npz.\nRead 12 samples from /home/chetana/ML_eddie/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\nTotal number of pixels in training set: 0.39 megapixels across 24 SSH maps\nNumber of pixels that are not eddies: 0.28 megapixels (72.00%)\nNumber of pixels that are anticyclonic eddies: 0.06 megapixels (14.23%)\nNumber of pixels that are cyclonic eddies: 0.05 megapixels (13.77%)\n\n======================================================================\nWriting Tensorboard logs to /home/chetana/tensorboard/2023-04-25_16-31\n======================================================================\n\nTraining:   0%|                                   | 0/250 [00:00<?, ?epoch(s)/s]\nTraining:   0%| | 0/250 [00:21<?, ?epoch(s)/s, train_multiclassaccuracy=0.239357\nTraining:   0%| | 1/250 [00:21<1:28:39, 21.36s/epoch(s), train_multiclassaccurac\nTraining:   0%| | 1/250 [00:36<1:28:39, 21.36s/epoch(s), train_multiclassaccurac\nTraining:   1%| | 2/250 [00:36<1:12:22, 17.51s/epoch(s), train_multiclassaccurac\nTraining:   1%| | 2/250 [00:41<1:12:22, 17.51s/epoch(s), train_multiclassaccurac\nTraining:   1%| | 3/250 [00:41<49:10, 11.95s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 3/250 [00:54<49:10, 11.95s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 4/250 [00:54<51:04, 12.46s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 4/250 [01:07<51:04, 12.46s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 5/250 [01:07<51:01, 12.49s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 5/250 [01:12<51:01, 12.49s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 6/250 [01:12<41:10, 10.12s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 6/250 [01:18<41:10, 10.12s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 7/250 [01:18<35:02,  8.65s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 7/250 [01:24<35:02,  8.65s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 8/250 [01:24<31:02,  7.70s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 8/250 [01:29<31:02,  7.70s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 9/250 [01:29<28:13,  7.03s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 9/250 [01:35<28:13,  7.03s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 10/250 [01:35<26:31,  6.63s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 10/250 [01:40<26:31,  6.63s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 11/250 [01:40<25:09,  6.32s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 11/250 [01:46<25:09,  6.32s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 12/250 [01:46<24:24,  6.15s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 12/250 [01:52<24:24,  6.15s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 13/250 [01:52<23:26,  5.94s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 13/250 [01:57<23:26,  5.94s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 14/250 [01:57<23:06,  5.88s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 14/250 [02:03<23:06,  5.88s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 15/250 [02:03<22:30,  5.75s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 15/250 [02:09<22:30,  5.75s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 16/250 [02:09<22:36,  5.80s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 16/250 [02:15<22:36,  5.80s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 17/250 [02:15<22:38,  5.83s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 17/250 [02:21<22:38,  5.83s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 18/250 [02:21<22:47,  5.89s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 18/250 [02:27<22:47,  5.89s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 19/250 [02:27<22:37,  5.88s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 19/250 [02:33<22:37,  5.88s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 20/250 [02:33<22:48,  5.95s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 20/250 [02:44<22:48,  5.95s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 21/250 [02:44<28:41,  7.52s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 21/250 [02:57<28:41,  7.52s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 22/250 [02:57<35:07,  9.24s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 22/250 [03:03<35:07,  9.24s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 23/250 [03:03<30:38,  8.10s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 23/250 [03:08<30:38,  8.10s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 24/250 [03:08<27:05,  7.19s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 24/250 [03:13<27:05,  7.19s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 25/250 [03:13<24:40,  6.58s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 25/250 [03:20<24:40,  6.58s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 26/250 [03:20<24:46,  6.63s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 26/250 [03:25<24:46,  6.63s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 27/250 [03:25<23:09,  6.23s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 27/250 [03:30<23:09,  6.23s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 28/250 [03:30<21:40,  5.86s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 28/250 [03:35<21:40,  5.86s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 29/250 [03:35<20:53,  5.67s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 29/250 [03:40<20:53,  5.67s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 30/250 [03:40<20:12,  5.51s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 30/250 [03:50<20:12,  5.51s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 31/250 [03:50<24:56,  6.83s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 31/250 [03:56<24:56,  6.83s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 32/250 [03:56<23:25,  6.45s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 32/250 [04:01<23:25,  6.45s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 33/250 [04:01<22:00,  6.09s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 33/250 [04:06<22:00,  6.09s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 34/250 [04:06<21:08,  5.87s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 34/250 [04:11<21:08,  5.87s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 35/250 [04:11<19:59,  5.58s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 35/250 [04:16<19:59,  5.58s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 36/250 [04:16<19:13,  5.39s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 36/250 [04:21<19:13,  5.39s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 37/250 [04:21<18:40,  5.26s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 37/250 [04:26<18:40,  5.26s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 38/250 [04:26<18:25,  5.21s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 38/250 [04:31<18:25,  5.21s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 39/250 [04:31<18:04,  5.14s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 39/250 [04:36<18:04,  5.14s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 40/250 [04:36<17:44,  5.07s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 40/250 [04:41<17:44,  5.07s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 41/250 [04:41<17:38,  5.07s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 41/250 [04:46<17:38,  5.07s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 42/250 [04:46<17:25,  5.03s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 42/250 [04:51<17:25,  5.03s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 43/250 [04:51<17:27,  5.06s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 43/250 [04:56<17:27,  5.06s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 44/250 [04:56<17:04,  4.97s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 44/250 [05:01<17:04,  4.97s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 45/250 [05:01<17:07,  5.01s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 45/250 [05:06<17:07,  5.01s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 46/250 [05:06<17:03,  5.01s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 46/250 [05:11<17:03,  5.01s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 47/250 [05:11<17:09,  5.07s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 47/250 [05:16<17:09,  5.07s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 48/250 [05:16<16:45,  4.98s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 48/250 [05:21<16:45,  4.98s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 49/250 [05:21<16:52,  5.04s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 49/250 [05:26<16:52,  5.04s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 50/250 [05:26<16:43,  5.02s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 50/250 [05:31<16:43,  5.02s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 51/250 [05:31<16:45,  5.05s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 51/250 [05:36<16:45,  5.05s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 52/250 [05:36<16:23,  4.97s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 52/250 [05:44<16:23,  4.97s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 53/250 [05:44<19:23,  5.91s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 53/250 [05:49<19:23,  5.91s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 54/250 [05:49<18:27,  5.65s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 54/250 [05:55<18:27,  5.65s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 55/250 [05:55<18:01,  5.55s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 55/250 [06:00<18:01,  5.55s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 56/250 [06:00<17:31,  5.42s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 56/250 [06:05<17:31,  5.42s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 57/250 [06:05<17:20,  5.39s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 57/250 [06:10<17:20,  5.39s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 58/250 [06:10<16:41,  5.22s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 58/250 [06:15<16:41,  5.22s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 59/250 [06:15<16:37,  5.22s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 59/250 [06:20<16:37,  5.22s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 60/250 [06:20<16:25,  5.18s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 60/250 [06:25<16:25,  5.18s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 61/250 [06:25<16:22,  5.20s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 61/250 [06:31<16:22,  5.20s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▏| 62/250 [06:31<16:11,  5.17s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▏| 62/250 [06:36<16:11,  5.17s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▎| 63/250 [06:36<16:08,  5.18s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▎| 63/250 [06:41<16:08,  5.18s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 64/250 [06:41<16:11,  5.22s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 64/250 [06:46<16:11,  5.22s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 65/250 [06:46<16:00,  5.19s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 65/250 [06:51<16:00,  5.19s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 66/250 [06:51<15:57,  5.21s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 66/250 [06:56<15:57,  5.21s/epoch(s), train_multiclassaccuracyEarly stopping at epoch 66 with validation loss 0.000 and training loss 1.006\n\nTraining:  26%|▎| 66/250 [06:56<19:22,  6.32s/epoch(s), train_multiclassaccuracy\n{'MulticlassAccuracy': tensor(0.6333)}\n/home/chetana/tensorboard/2023-04-25_16-31/model_ckpt_final_full_data.pt\ntrain/Accuracy tensor(0.6333)\nval/Accuracy tensor(0.7205)\n",
  "history_begin_time" : 1682440252776,
  "history_end_time" : 1682440679204,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "c2lqcn",
  "indicator" : "Done"
},{
  "history_id" : "j3cvqsjt97j",
  "history_input" : "from training_and_plot_utils import *\nfrom model_components import *\nfrom device_config_and_data_loader import *\nfrom tqdm.auto import tqdm\n\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nprint(train_m)\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"MulticlassAccuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"MulticlassAccuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_final_full_data.pt\")\n\nprint(model_path)\n\n\nprint(\"train/Accuracy\", train_m[\"MulticlassAccuracy\"])\nprint(\"val/Accuracy\", val_m[\"MulticlassAccuracy\"])\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 24 samples from /home/chetana/ML_eddie/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-1999_lat14N-46N_lon166W-134W.npz.\nRead 12 samples from /home/chetana/ML_eddie/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\nRead 24 samples from /home/chetana/ML_eddie/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-1999_lat14N-46N_lon166W-134W.npz.\nRead 12 samples from /home/chetana/ML_eddie/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\nTotal number of pixels in training set: 0.39 megapixels across 24 SSH maps\nNumber of pixels that are not eddies: 0.28 megapixels (72.00%)\nNumber of pixels that are anticyclonic eddies: 0.06 megapixels (14.23%)\nNumber of pixels that are cyclonic eddies: 0.05 megapixels (13.77%)\n\n======================================================================\nWriting Tensorboard logs to /home/chetana/tensorboard/2023-04-25_16-29\n======================================================================\n\nTraining:   0%|                                   | 0/250 [00:00<?, ?epoch(s)/s]\nTraining:   0%| | 0/250 [00:05<?, ?epoch(s)/s, train_multiclassaccuracy=0.239357\nTraining:   0%| | 1/250 [00:05<24:03,  5.80s/epoch(s), train_multiclassaccuracy=\nTraining:   0%| | 1/250 [00:14<24:03,  5.80s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 2/250 [00:14<30:19,  7.34s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 2/250 [00:27<30:19,  7.34s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 3/250 [00:27<41:41, 10.13s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 3/250 [00:41<41:41, 10.13s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 4/250 [00:41<47:21, 11.55s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 4/250 [00:59<47:21, 11.55s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 5/250 [00:59<56:13, 13.77s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 5/250 [01:18<56:13, 13.77s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 6/250 [01:18<1:04:06, 15.76s/epoch(s), train_multiclassaccurac\nTraining:   2%| | 6/250 [01:33<1:04:06, 15.76s/epoch(s), train_multiclassaccurac\nTraining:   3%| | 7/250 [01:33<1:02:51, 15.52s/epoch(s), train_multiclassaccurac\nTraining:   3%| | 7/250 [01:44<1:02:51, 15.52s/epoch(s), train_multiclassaccurac\nTraining:   3%| | 8/250 [01:44<56:25, 13.99s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 8/250 [01:56<56:25, 13.99s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 9/250 [01:56<54:10, 13.49s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 9/250 [02:09<54:10, 13.49s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 10/250 [02:09<52:41, 13.17s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 10/250 [02:15<52:41, 13.17s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 11/250 [02:15<44:20, 11.13s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 11/250 [02:21<44:20, 11.13s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 12/250 [02:21<37:02,  9.34s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 12/250 [02:26<37:02,  9.34s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 13/250 [02:26<31:51,  8.06s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 13/250 [02:31<31:51,  8.06s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 14/250 [02:31<28:39,  7.29s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 14/250 [02:37<28:39,  7.29s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 15/250 [02:37<26:22,  6.73s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 15/250 [02:42<26:22,  6.73s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 16/250 [02:42<24:41,  6.33s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 16/250 [02:47<24:41,  6.33s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 17/250 [02:47<23:11,  5.97s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 17/250 [02:53<23:11,  5.97s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 18/250 [02:53<23:09,  5.99s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 18/250 [02:58<23:09,  5.99s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 19/250 [02:58<22:12,  5.77s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 19/250 [03:04<22:12,  5.77s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 20/250 [03:04<21:28,  5.60s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 20/250 [03:09<21:28,  5.60s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 21/250 [03:09<20:54,  5.48s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 21/250 [03:14<20:54,  5.48s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 22/250 [03:14<20:49,  5.48s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 22/250 [03:21<20:49,  5.48s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 23/250 [03:21<21:33,  5.70s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 23/250 [03:28<21:33,  5.70s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 24/250 [03:28<23:07,  6.14s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 24/250 [03:34<23:07,  6.14s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 25/250 [03:35<23:47,  6.35s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 25/250 [03:48<23:47,  6.35s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 26/250 [03:48<31:58,  8.56s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 26/250 [04:03<31:58,  8.56s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 27/250 [04:03<38:53, 10.46s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 27/250 [04:10<38:53, 10.46s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 28/250 [04:10<34:45,  9.39s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 28/250 [04:17<34:45,  9.39s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 29/250 [04:17<31:46,  8.63s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 29/250 [04:25<31:46,  8.63s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 30/250 [04:25<31:30,  8.59s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 30/250 [04:32<31:30,  8.59s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 31/250 [04:32<28:44,  7.87s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 31/250 [04:38<28:44,  7.87s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 32/250 [04:38<27:00,  7.43s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 32/250 [04:45<27:00,  7.43s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 33/250 [04:45<25:55,  7.17s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 33/250 [04:56<25:55,  7.17s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 34/250 [04:56<30:37,  8.51s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 34/250 [05:02<30:37,  8.51s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 35/250 [05:02<27:54,  7.79s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 35/250 [05:09<27:54,  7.79s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 36/250 [05:09<26:07,  7.32s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 36/250 [05:15<26:07,  7.32s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 37/250 [05:15<24:56,  7.03s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 37/250 [05:22<24:56,  7.03s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 38/250 [05:22<24:47,  7.02s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 38/250 [05:28<24:47,  7.02s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 39/250 [05:28<24:05,  6.85s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 39/250 [05:35<24:05,  6.85s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 40/250 [05:35<23:37,  6.75s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 40/250 [05:41<23:37,  6.75s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 41/250 [05:41<23:20,  6.70s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 41/250 [05:48<23:20,  6.70s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 42/250 [05:48<22:45,  6.57s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 42/250 [05:54<22:45,  6.57s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 43/250 [05:54<22:45,  6.60s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 43/250 [06:01<22:45,  6.60s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 44/250 [06:01<22:49,  6.65s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 44/250 [06:08<22:49,  6.65s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 45/250 [06:08<22:43,  6.65s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 45/250 [06:14<22:43,  6.65s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 46/250 [06:14<22:20,  6.57s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 46/250 [06:21<22:20,  6.57s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 47/250 [06:21<22:33,  6.67s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 47/250 [06:27<22:33,  6.67s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 48/250 [06:27<22:10,  6.59s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 48/250 [06:34<22:10,  6.59s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 49/250 [06:34<22:17,  6.65s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 49/250 [06:41<22:17,  6.65s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 50/250 [06:41<22:01,  6.61s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 50/250 [06:50<22:01,  6.61s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 51/250 [06:50<24:25,  7.37s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 51/250 [06:56<24:25,  7.37s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 52/250 [06:56<23:00,  6.97s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 52/250 [07:02<23:00,  6.97s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 53/250 [07:02<22:21,  6.81s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 53/250 [07:09<22:21,  6.81s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 54/250 [07:09<21:54,  6.70s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 54/250 [07:15<21:54,  6.70s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 55/250 [07:15<21:41,  6.68s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 55/250 [07:22<21:41,  6.68s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 56/250 [07:22<21:04,  6.52s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 56/250 [07:28<21:04,  6.52s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 57/250 [07:28<20:44,  6.45s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 57/250 [07:34<20:44,  6.45s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 58/250 [07:34<20:22,  6.37s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 58/250 [07:41<20:22,  6.37s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 59/250 [07:41<20:34,  6.47s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 59/250 [07:47<20:34,  6.47s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 60/250 [07:47<20:28,  6.47s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 60/250 [07:54<20:28,  6.47s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 61/250 [07:54<20:25,  6.48s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 61/250 [08:00<20:25,  6.48s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▏| 62/250 [08:00<19:56,  6.37s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▏| 62/250 [08:05<19:56,  6.37s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▎| 63/250 [08:05<18:48,  6.03s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▎| 63/250 [08:10<18:48,  6.03s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 64/250 [08:10<17:42,  5.71s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 64/250 [08:15<17:42,  5.71s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 65/250 [08:15<16:41,  5.41s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 65/250 [08:20<16:41,  5.41s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 66/250 [08:20<16:06,  5.26s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 66/250 [08:24<16:06,  5.26s/epoch(s), train_multiclassaccuracyEarly stopping at epoch 66 with validation loss 0.000 and training loss 1.006\n\nTraining:  26%|▎| 66/250 [08:24<23:27,  7.65s/epoch(s), train_multiclassaccuracy\n{'MulticlassAccuracy': tensor(0.6333)}\n/home/chetana/tensorboard/2023-04-25_16-29/model_ckpt_final_full_data.pt\ntrain/Accuracy tensor(0.6333)\nval/Accuracy tensor(0.7205)\n",
  "history_begin_time" : 1682440191996,
  "history_end_time" : 1682440702868,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "c2lqcn",
  "indicator" : "Done"
},{
  "history_id" : "z9c27zf72f5",
  "history_input" : "from training_and_plot_utils import *\nfrom model_components import *\nfrom device_config_and_data_loader import *\nfrom tqdm.auto import tqdm\n\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nprint(train_m)\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"MulticlassAccuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"MulticlassAccuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_final_full_data.pt\")\n\nprint(model_path)\n\n\nprint(\"train/Accuracy\", train_m[\"MulticlassAccuracy\"])\nprint(\"val/Accuracy\", val_m[\"MulticlassAccuracy\"])\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 24 samples from /home/chetana/ML_eddie/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-1999_lat14N-46N_lon166W-134W.npz.\nRead 12 samples from /home/chetana/ML_eddie/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\nRead 24 samples from /home/chetana/ML_eddie/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-1999_lat14N-46N_lon166W-134W.npz.\nRead 12 samples from /home/chetana/ML_eddie/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\nTotal number of pixels in training set: 0.39 megapixels across 24 SSH maps\nNumber of pixels that are not eddies: 0.28 megapixels (72.00%)\nNumber of pixels that are anticyclonic eddies: 0.06 megapixels (14.23%)\nNumber of pixels that are cyclonic eddies: 0.05 megapixels (13.77%)\n\n======================================================================\nWriting Tensorboard logs to /home/chetana/tensorboard/2023-04-25_16-26\n======================================================================\n\nTraining:   0%|                                   | 0/250 [00:00<?, ?epoch(s)/s]\nTraining:   0%| | 0/250 [00:05<?, ?epoch(s)/s, train_multiclassaccuracy=0.239357\nTraining:   0%| | 1/250 [00:05<21:11,  5.10s/epoch(s), train_multiclassaccuracy=\nTraining:   0%| | 1/250 [00:09<21:11,  5.10s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 2/250 [00:09<20:11,  4.89s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 2/250 [00:14<20:11,  4.89s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 3/250 [00:14<20:16,  4.93s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 3/250 [00:19<20:16,  4.93s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 4/250 [00:19<20:03,  4.89s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 4/250 [00:25<20:03,  4.89s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 5/250 [00:25<20:41,  5.07s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 5/250 [00:29<20:41,  5.07s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 6/250 [00:29<20:09,  4.96s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 6/250 [00:34<20:09,  4.96s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 7/250 [00:34<19:58,  4.93s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 7/250 [00:39<19:58,  4.93s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 8/250 [00:39<19:58,  4.95s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 8/250 [00:44<19:58,  4.95s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 9/250 [00:44<19:33,  4.87s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 9/250 [00:49<19:33,  4.87s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 10/250 [00:49<19:32,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 10/250 [00:53<19:32,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 11/250 [00:53<19:13,  4.83s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 11/250 [00:58<19:13,  4.83s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 12/250 [00:58<19:15,  4.85s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 12/250 [01:03<19:15,  4.85s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 13/250 [01:03<19:00,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 13/250 [01:08<19:00,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 14/250 [01:08<19:00,  4.83s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 14/250 [01:13<19:00,  4.83s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 15/250 [01:13<18:45,  4.79s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 15/250 [01:17<18:45,  4.79s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 16/250 [01:17<18:44,  4.80s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 16/250 [01:22<18:44,  4.80s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 17/250 [01:22<18:45,  4.83s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 17/250 [01:28<18:45,  4.83s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 18/250 [01:28<19:02,  4.92s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 18/250 [01:33<19:02,  4.92s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 19/250 [01:33<19:16,  5.01s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 19/250 [01:38<19:16,  5.01s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 20/250 [01:38<19:08,  4.99s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 20/250 [01:43<19:08,  4.99s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 21/250 [01:43<19:15,  5.05s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 21/250 [01:48<19:15,  5.05s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 22/250 [01:48<19:01,  5.00s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 22/250 [01:53<19:01,  5.00s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 23/250 [01:53<18:59,  5.02s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 23/250 [01:58<18:59,  5.02s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 24/250 [01:58<18:35,  4.94s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 24/250 [02:03<18:35,  4.94s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 25/250 [02:03<18:33,  4.95s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 25/250 [02:07<18:33,  4.95s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 26/250 [02:07<18:13,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 26/250 [02:12<18:13,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 27/250 [02:12<18:10,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 27/250 [02:17<18:10,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 28/250 [02:17<17:54,  4.84s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 28/250 [02:22<17:54,  4.84s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 29/250 [02:22<17:55,  4.87s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 29/250 [02:27<17:55,  4.87s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 30/250 [02:27<17:54,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 30/250 [02:31<17:54,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 31/250 [02:31<17:38,  4.83s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 31/250 [02:36<17:38,  4.83s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 32/250 [02:36<17:38,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 32/250 [02:41<17:38,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 33/250 [02:41<17:26,  4.82s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 33/250 [02:46<17:26,  4.82s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 34/250 [02:46<17:28,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 34/250 [02:51<17:28,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 35/250 [02:51<17:15,  4.82s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 35/250 [02:56<17:15,  4.82s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 36/250 [02:56<17:19,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 36/250 [03:01<17:19,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 37/250 [03:01<17:17,  4.87s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 37/250 [03:06<17:17,  4.87s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 38/250 [03:06<17:43,  5.02s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 38/250 [03:11<17:43,  5.02s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 39/250 [03:11<17:39,  5.02s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 39/250 [03:17<17:39,  5.02s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 40/250 [03:17<18:08,  5.18s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 40/250 [03:23<18:08,  5.18s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 41/250 [03:23<19:11,  5.51s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 41/250 [03:33<19:11,  5.51s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 42/250 [03:33<24:05,  6.95s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 42/250 [03:45<24:05,  6.95s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 43/250 [03:45<29:32,  8.56s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 43/250 [04:00<29:32,  8.56s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 44/250 [04:00<35:12, 10.25s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 44/250 [04:20<35:12, 10.25s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 45/250 [04:20<45:49, 13.41s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 45/250 [04:39<45:49, 13.41s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 46/250 [04:39<51:13, 15.07s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 46/250 [04:54<51:13, 15.07s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 47/250 [04:54<50:40, 14.98s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 47/250 [05:00<50:40, 14.98s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 48/250 [05:00<40:42, 12.09s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 48/250 [05:12<40:42, 12.09s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 49/250 [05:12<40:28, 12.08s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 49/250 [05:23<40:28, 12.08s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 50/250 [05:23<39:59, 12.00s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 50/250 [05:29<39:59, 12.00s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 51/250 [05:29<33:21, 10.06s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 51/250 [05:34<33:21, 10.06s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 52/250 [05:34<28:28,  8.63s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 52/250 [05:40<28:28,  8.63s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 53/250 [05:40<25:21,  7.72s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 53/250 [05:45<25:21,  7.72s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 54/250 [05:45<22:56,  7.02s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 54/250 [05:51<22:56,  7.02s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 55/250 [05:51<21:20,  6.57s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 55/250 [05:56<21:20,  6.57s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 56/250 [05:56<20:05,  6.21s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 56/250 [06:02<20:05,  6.21s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 57/250 [06:02<19:22,  6.02s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 57/250 [06:07<19:22,  6.02s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 58/250 [06:07<18:42,  5.85s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 58/250 [06:13<18:42,  5.85s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 59/250 [06:13<18:20,  5.76s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 59/250 [06:18<18:20,  5.76s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 60/250 [06:18<17:50,  5.63s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 60/250 [06:24<17:50,  5.63s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 61/250 [06:24<17:44,  5.63s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 61/250 [06:30<17:44,  5.63s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▏| 62/250 [06:30<18:00,  5.75s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▏| 62/250 [06:36<18:00,  5.75s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▎| 63/250 [06:36<18:20,  5.88s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▎| 63/250 [06:42<18:20,  5.88s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 64/250 [06:42<18:15,  5.89s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 64/250 [06:47<18:15,  5.89s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 65/250 [06:47<17:49,  5.78s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 65/250 [07:01<17:49,  5.78s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 66/250 [07:01<24:57,  8.14s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 66/250 [07:14<24:57,  8.14s/epoch(s), train_multiclassaccuracyEarly stopping at epoch 66 with validation loss 0.000 and training loss 1.006\n\nTraining:  26%|▎| 66/250 [07:14<20:12,  6.59s/epoch(s), train_multiclassaccuracy\n{'MulticlassAccuracy': tensor(0.6333)}\n/home/chetana/tensorboard/2023-04-25_16-26/model_ckpt_final_full_data.pt\ntrain/Accuracy tensor(0.6333)\nval/Accuracy tensor(0.7205)\n",
  "history_begin_time" : 1682439999281,
  "history_end_time" : 1682440440388,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "c2lqcn",
  "indicator" : "Done"
},{
  "history_id" : "rnbomyv44ct",
  "history_input" : "from training_and_plot_utils import *\nfrom model_components import *\nfrom device_config_and_data_loader import *\nfrom tqdm.auto import tqdm\n\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nprint(train_m)\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"MulticlassAccuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"MulticlassAccuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_final_full_data.pt\")\n\nprint(model_path)\n\n\nprint(\"train/Accuracy\", train_m[\"MulticlassAccuracy\"])\nprint(\"val/Accuracy\", val_m[\"MulticlassAccuracy\"])\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 24 samples from /home/chetana/ML_eddie/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-1999_lat14N-46N_lon166W-134W.npz.\nRead 12 samples from /home/chetana/ML_eddie/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\nRead 24 samples from /home/chetana/ML_eddie/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-1999_lat14N-46N_lon166W-134W.npz.\nRead 12 samples from /home/chetana/ML_eddie/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\nTotal number of pixels in training set: 0.39 megapixels across 24 SSH maps\nNumber of pixels that are not eddies: 0.28 megapixels (72.00%)\nNumber of pixels that are anticyclonic eddies: 0.06 megapixels (14.23%)\nNumber of pixels that are cyclonic eddies: 0.05 megapixels (13.77%)\n\n======================================================================\nWriting Tensorboard logs to /home/chetana/tensorboard/2023-04-25_16-20\n======================================================================\n\nTraining:   0%|                                   | 0/250 [00:00<?, ?epoch(s)/s]\nTraining:   0%| | 0/250 [00:05<?, ?epoch(s)/s, train_multiclassaccuracy=0.239357\nTraining:   0%| | 1/250 [00:05<23:29,  5.66s/epoch(s), train_multiclassaccuracy=\nTraining:   0%| | 1/250 [00:10<23:29,  5.66s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 2/250 [00:10<22:01,  5.33s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 2/250 [00:15<22:01,  5.33s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 3/250 [00:15<21:36,  5.25s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 3/250 [00:20<21:36,  5.25s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 4/250 [00:20<20:58,  5.11s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 4/250 [00:25<20:58,  5.11s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 5/250 [00:25<20:50,  5.10s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 5/250 [00:30<20:50,  5.10s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 6/250 [00:30<20:28,  5.04s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 6/250 [00:35<20:28,  5.04s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 7/250 [00:35<20:35,  5.09s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 7/250 [00:41<20:35,  5.09s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 8/250 [00:41<20:36,  5.11s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 8/250 [00:46<20:36,  5.11s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 9/250 [00:46<20:20,  5.06s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 9/250 [00:51<20:20,  5.06s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 10/250 [00:51<20:25,  5.11s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 10/250 [00:56<20:25,  5.11s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 11/250 [00:56<20:08,  5.06s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 11/250 [01:01<20:08,  5.06s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 12/250 [01:01<19:55,  5.02s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 12/250 [01:05<19:55,  5.02s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 13/250 [01:05<19:33,  4.95s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 13/250 [01:10<19:33,  4.95s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 14/250 [01:10<19:28,  4.95s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 14/250 [01:15<19:28,  4.95s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 15/250 [01:15<19:12,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 15/250 [01:20<19:12,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 16/250 [01:20<19:10,  4.92s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 16/250 [01:25<19:10,  4.92s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 17/250 [01:25<18:57,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 17/250 [01:30<18:57,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 18/250 [01:30<18:55,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 18/250 [01:35<18:55,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 19/250 [01:35<18:55,  4.92s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 19/250 [01:40<18:55,  4.92s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 20/250 [01:40<18:38,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 20/250 [01:45<18:38,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 21/250 [01:45<18:41,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 21/250 [01:49<18:41,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 22/250 [01:49<18:29,  4.87s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 22/250 [01:54<18:29,  4.87s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 23/250 [01:54<18:30,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 23/250 [01:59<18:30,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 24/250 [01:59<18:16,  4.85s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 24/250 [02:04<18:16,  4.85s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 25/250 [02:04<18:17,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 25/250 [02:09<18:17,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 26/250 [02:09<18:16,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 26/250 [02:14<18:16,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 27/250 [02:14<18:13,  4.91s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 27/250 [02:19<18:13,  4.91s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 28/250 [02:19<18:00,  4.87s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 28/250 [02:24<18:00,  4.87s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 29/250 [02:24<17:56,  4.87s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 29/250 [02:28<17:56,  4.87s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 30/250 [02:29<17:55,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 30/250 [02:33<17:55,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 31/250 [02:33<17:46,  4.87s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 31/250 [02:39<17:46,  4.87s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 32/250 [02:39<18:06,  4.99s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 32/250 [02:43<18:06,  4.99s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 33/250 [02:43<17:45,  4.91s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 33/250 [02:48<17:45,  4.91s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 34/250 [02:48<17:39,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 34/250 [02:53<17:39,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 35/250 [02:53<17:28,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 35/250 [02:58<17:28,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 36/250 [02:58<17:26,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 36/250 [03:03<17:26,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 37/250 [03:03<17:11,  4.84s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 37/250 [03:08<17:11,  4.84s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 38/250 [03:08<17:10,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 38/250 [03:12<17:10,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 39/250 [03:12<17:03,  4.85s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 39/250 [03:17<17:03,  4.85s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 40/250 [03:17<17:08,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 40/250 [03:23<17:08,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 41/250 [03:23<17:28,  5.02s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 41/250 [03:28<17:28,  5.02s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 42/250 [03:28<17:18,  4.99s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 42/250 [03:33<17:18,  4.99s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 43/250 [03:33<17:22,  5.04s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 43/250 [03:38<17:22,  5.04s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 44/250 [03:38<17:00,  4.95s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 44/250 [03:42<17:00,  4.95s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 45/250 [03:42<16:54,  4.95s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 45/250 [03:47<16:54,  4.95s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 46/250 [03:47<16:39,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 46/250 [03:52<16:39,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 47/250 [03:52<16:35,  4.91s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 47/250 [03:57<16:35,  4.91s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 48/250 [03:57<16:21,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 48/250 [04:02<16:21,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 49/250 [04:02<16:20,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 49/250 [04:07<16:20,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 50/250 [04:07<16:07,  4.84s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 50/250 [04:11<16:07,  4.84s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 51/250 [04:12<16:07,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 51/250 [04:16<16:07,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 52/250 [04:16<15:55,  4.83s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 52/250 [04:21<15:55,  4.83s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 53/250 [04:21<15:55,  4.85s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 53/250 [04:26<15:55,  4.85s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 54/250 [04:26<15:43,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 54/250 [04:31<15:43,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 55/250 [04:31<15:44,  4.84s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 55/250 [04:36<15:44,  4.84s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 56/250 [04:36<15:33,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 56/250 [04:40<15:33,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 57/250 [04:40<15:34,  4.84s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 57/250 [04:45<15:34,  4.84s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 58/250 [04:45<15:22,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 58/250 [04:50<15:22,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 59/250 [04:50<15:24,  4.84s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 59/250 [04:55<15:24,  4.84s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 60/250 [04:55<15:13,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 60/250 [05:00<15:13,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 61/250 [05:00<15:13,  4.83s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 61/250 [05:04<15:13,  4.83s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▏| 62/250 [05:04<15:02,  4.80s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▏| 62/250 [05:09<15:02,  4.80s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▎| 63/250 [05:09<15:02,  4.83s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▎| 63/250 [05:14<15:02,  4.83s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 64/250 [05:14<15:01,  4.84s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 64/250 [05:19<15:01,  4.84s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 65/250 [05:19<14:52,  4.82s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 65/250 [05:24<14:52,  4.82s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 66/250 [05:24<14:58,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 66/250 [05:29<14:58,  4.88s/epoch(s), train_multiclassaccuracyEarly stopping at epoch 66 with validation loss 0.000 and training loss 1.006\n\nTraining:  26%|▎| 66/250 [05:29<15:17,  4.99s/epoch(s), train_multiclassaccuracy\n{'MulticlassAccuracy': tensor(0.6333)}\n/home/chetana/tensorboard/2023-04-25_16-20/model_ckpt_final_full_data.pt\ntrain/Accuracy tensor(0.6333)\nval/Accuracy tensor(0.7205)\n",
  "history_begin_time" : 1682439651059,
  "history_end_time" : 1682439985918,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "c2lqcn",
  "indicator" : "Done"
},{
  "history_id" : "y3berjjyc9d",
  "history_input" : null,
  "history_output" : "Exhausted available authentication methods",
  "history_begin_time" : 1682439494495,
  "history_end_time" : 1682439496403,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "c2lqcn",
  "indicator" : "Failed"
},{
  "history_id" : "4uew866c1ok",
  "history_input" : null,
  "history_output" : "Exhausted available authentication methods",
  "history_begin_time" : 1682439478110,
  "history_end_time" : 1682439480213,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "c2lqcn",
  "indicator" : "Failed"
},{
  "history_id" : "05of3jbx8tr",
  "history_input" : "from training_and_plot_utils import *\nfrom model_components import *\nfrom device_config_and_data_loader import *\nfrom tqdm.auto import tqdm\n\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nprint(train_m)\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"MulticlassAccuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"MulticlassAccuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_final_full_data.pt\")\n\nprint(model_path)\n\n\nprint(\"train/Accuracy\", train_m[\"MulticlassAccuracy\"])\nprint(\"val/Accuracy\", val_m[\"MulticlassAccuracy\"])\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 24 samples from /home/chetana/ML_eddie/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-1999_lat14N-46N_lon166W-134W.npz.\nRead 12 samples from /home/chetana/ML_eddie/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\nRead 24 samples from /home/chetana/ML_eddie/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-1999_lat14N-46N_lon166W-134W.npz.\nRead 12 samples from /home/chetana/ML_eddie/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\nTotal number of pixels in training set: 0.39 megapixels across 24 SSH maps\nNumber of pixels that are not eddies: 0.28 megapixels (72.00%)\nNumber of pixels that are anticyclonic eddies: 0.06 megapixels (14.23%)\nNumber of pixels that are cyclonic eddies: 0.05 megapixels (13.77%)\n\n======================================================================\nWriting Tensorboard logs to /home/chetana/tensorboard/2023-04-25_15-40\n======================================================================\n\nTraining:   0%|                                   | 0/250 [00:00<?, ?epoch(s)/s]\nTraining:   0%| | 0/250 [00:05<?, ?epoch(s)/s, train_multiclassaccuracy=0.239357\nTraining:   0%| | 1/250 [00:05<20:52,  5.03s/epoch(s), train_multiclassaccuracy=\nTraining:   0%| | 1/250 [00:09<20:52,  5.03s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 2/250 [00:09<20:15,  4.90s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 2/250 [00:14<20:15,  4.90s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 3/250 [00:14<20:32,  4.99s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 3/250 [00:19<20:32,  4.99s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 4/250 [00:19<19:59,  4.88s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 4/250 [00:24<19:59,  4.88s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 5/250 [00:24<19:55,  4.88s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 5/250 [00:29<19:55,  4.88s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 6/250 [00:29<19:35,  4.82s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 6/250 [00:34<19:35,  4.82s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 7/250 [00:34<19:32,  4.83s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 7/250 [00:38<19:32,  4.83s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 8/250 [00:38<19:31,  4.84s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 8/250 [00:43<19:31,  4.84s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 9/250 [00:43<19:21,  4.82s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 9/250 [00:48<19:21,  4.82s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 10/250 [00:48<19:39,  4.92s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 10/250 [00:53<19:39,  4.92s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 11/250 [00:53<19:38,  4.93s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 11/250 [00:58<19:38,  4.93s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 12/250 [00:58<19:51,  5.01s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 12/250 [01:03<19:51,  5.01s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 13/250 [01:03<19:42,  4.99s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 13/250 [01:08<19:42,  4.99s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 14/250 [01:08<19:38,  4.99s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 14/250 [01:13<19:38,  4.99s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 15/250 [01:13<19:12,  4.91s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 15/250 [01:18<19:12,  4.91s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 16/250 [01:18<19:08,  4.91s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 16/250 [01:23<19:08,  4.91s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 17/250 [01:23<18:50,  4.85s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 17/250 [01:28<18:50,  4.85s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 18/250 [01:28<18:48,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 18/250 [01:33<18:48,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 19/250 [01:33<18:42,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 19/250 [01:37<18:42,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 20/250 [01:37<18:30,  4.83s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 20/250 [01:42<18:30,  4.83s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 21/250 [01:42<18:28,  4.84s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 21/250 [01:47<18:28,  4.84s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 22/250 [01:47<18:14,  4.80s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 22/250 [01:52<18:14,  4.80s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 23/250 [01:52<18:24,  4.87s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 23/250 [01:57<18:24,  4.87s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 24/250 [01:57<18:10,  4.83s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 24/250 [02:02<18:10,  4.83s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 25/250 [02:02<18:14,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 25/250 [02:06<18:14,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 26/250 [02:06<17:57,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 26/250 [02:11<17:57,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 27/250 [02:11<17:53,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 27/250 [02:16<17:53,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 28/250 [02:16<17:40,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 28/250 [02:21<17:40,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 29/250 [02:21<17:46,  4.82s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 29/250 [02:26<17:46,  4.82s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 30/250 [02:26<17:45,  4.84s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 30/250 [02:30<17:45,  4.84s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 31/250 [02:30<17:30,  4.80s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 31/250 [02:35<17:30,  4.80s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 32/250 [02:35<17:33,  4.83s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 32/250 [02:40<17:33,  4.83s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 33/250 [02:40<17:27,  4.83s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 33/250 [02:45<17:27,  4.83s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 34/250 [02:45<17:27,  4.85s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 34/250 [02:50<17:27,  4.85s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 35/250 [02:50<17:13,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 35/250 [02:55<17:13,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 36/250 [02:55<17:17,  4.85s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 36/250 [02:59<17:17,  4.85s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 37/250 [02:59<17:01,  4.79s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 37/250 [03:04<17:01,  4.79s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 38/250 [03:04<17:01,  4.82s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 38/250 [03:09<17:01,  4.82s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 39/250 [03:09<16:49,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 39/250 [03:14<16:49,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 40/250 [03:14<16:52,  4.82s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 40/250 [03:19<16:52,  4.82s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 41/250 [03:19<16:50,  4.83s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 41/250 [03:23<16:50,  4.83s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 42/250 [03:23<16:37,  4.80s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 42/250 [03:28<16:37,  4.80s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 43/250 [03:28<16:39,  4.83s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 43/250 [03:33<16:39,  4.83s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 44/250 [03:33<16:25,  4.79s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 44/250 [03:38<16:25,  4.79s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 45/250 [03:38<16:26,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 45/250 [03:42<16:26,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 46/250 [03:42<16:15,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 46/250 [03:47<16:15,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 47/250 [03:47<16:22,  4.84s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 47/250 [03:52<16:22,  4.84s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 48/250 [03:52<16:12,  4.82s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 48/250 [03:57<16:12,  4.82s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 49/250 [03:57<16:13,  4.84s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 49/250 [04:02<16:13,  4.84s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 50/250 [04:02<15:59,  4.80s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 50/250 [04:07<15:59,  4.80s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 51/250 [04:07<15:58,  4.82s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 51/250 [04:11<15:58,  4.82s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 52/250 [04:11<15:44,  4.77s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 52/250 [04:16<15:44,  4.77s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 53/250 [04:16<15:46,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 53/250 [04:21<15:46,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 54/250 [04:21<15:36,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 54/250 [04:26<15:36,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 55/250 [04:26<15:39,  4.82s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 55/250 [04:30<15:39,  4.82s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 56/250 [04:30<15:26,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 56/250 [04:35<15:26,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 57/250 [04:35<15:29,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 57/250 [04:40<15:29,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 58/250 [04:40<15:17,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 58/250 [04:45<15:17,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 59/250 [04:45<15:17,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 59/250 [04:50<15:17,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 60/250 [04:50<15:07,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 60/250 [04:55<15:07,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 61/250 [04:55<15:07,  4.80s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 61/250 [04:59<15:07,  4.80s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▏| 62/250 [04:59<14:58,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▏| 62/250 [05:04<14:58,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▎| 63/250 [05:04<15:00,  4.82s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▎| 63/250 [05:09<15:00,  4.82s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 64/250 [05:09<14:56,  4.82s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 64/250 [05:14<14:56,  4.82s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 65/250 [05:14<14:45,  4.79s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 65/250 [05:19<14:45,  4.79s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 66/250 [05:19<14:47,  4.83s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 66/250 [05:23<14:47,  4.83s/epoch(s), train_multiclassaccuracyEarly stopping at epoch 66 with validation loss 0.000 and training loss 1.006\n\nTraining:  26%|▎| 66/250 [05:23<15:02,  4.91s/epoch(s), train_multiclassaccuracy\n{'MulticlassAccuracy': tensor(0.6333)}\n/home/chetana/tensorboard/2023-04-25_15-40/model_ckpt_final_full_data.pt\ntrain/Accuracy tensor(0.6333)\nval/Accuracy tensor(0.7205)\n",
  "history_begin_time" : 1682437198213,
  "history_end_time" : 1682437527654,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "c2lqcn",
  "indicator" : "Done"
},{
  "history_id" : "maausms01kb",
  "history_input" : "from training_and_plot_utils import *\nfrom model_components import *\nfrom device_config_and_data_loader import *\nfrom tqdm.auto import tqdm\n\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nprint(train_m)\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"MulticlassAccuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"MulticlassAccuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_final_full_data.pt\")\n\nprint(model_path)\n\n\nprint(\"train/Accuracy\", train_m[\"MulticlassAccuracy\"])\nprint(\"val/Accuracy\", val_m[\"MulticlassAccuracy\"])\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 24 samples from /home/chetana/ML_eddie/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-1999_lat14N-46N_lon166W-134W.npz.\nRead 12 samples from /home/chetana/ML_eddie/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\n======================================================================\nWriting Tensorboard logs to /home/chetana/tensorboard/2023-04-25_15-03\n======================================================================\n\nTraining:   0%|                                   | 0/250 [00:00<?, ?epoch(s)/s]\nTraining:   0%| | 0/250 [00:06<?, ?epoch(s)/s, train_multiclassaccuracy=0.239357\nTraining:   0%| | 1/250 [00:06<26:26,  6.37s/epoch(s), train_multiclassaccuracy=\nTraining:   0%| | 1/250 [00:11<26:26,  6.37s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 2/250 [00:11<22:46,  5.51s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 2/250 [00:16<22:46,  5.51s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 3/250 [00:16<21:46,  5.29s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 3/250 [00:21<21:46,  5.29s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 4/250 [00:21<20:50,  5.08s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 4/250 [00:25<20:50,  5.08s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 5/250 [00:25<20:31,  5.02s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 5/250 [00:30<20:31,  5.02s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 6/250 [00:30<20:01,  4.93s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 6/250 [00:35<20:01,  4.93s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 7/250 [00:35<20:01,  4.94s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 7/250 [00:40<20:01,  4.94s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 8/250 [00:40<19:52,  4.93s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 8/250 [00:45<19:52,  4.93s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 9/250 [00:45<19:31,  4.86s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 9/250 [00:50<19:31,  4.86s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 10/250 [00:50<19:28,  4.87s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 10/250 [00:54<19:28,  4.87s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 11/250 [00:54<19:11,  4.82s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 11/250 [00:59<19:11,  4.82s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 12/250 [00:59<19:23,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 12/250 [01:04<19:23,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 13/250 [01:04<19:07,  4.84s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 13/250 [01:09<19:07,  4.84s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 14/250 [01:09<19:06,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 14/250 [01:14<19:06,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 15/250 [01:14<18:53,  4.82s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 15/250 [01:19<18:53,  4.82s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 16/250 [01:19<18:56,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 16/250 [01:23<18:56,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 17/250 [01:23<18:41,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 17/250 [01:28<18:41,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 18/250 [01:28<18:44,  4.84s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 18/250 [01:33<18:44,  4.84s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 19/250 [01:33<18:43,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 19/250 [01:38<18:43,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 20/250 [01:38<18:28,  4.82s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 20/250 [01:43<18:28,  4.82s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 21/250 [01:43<18:32,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 21/250 [01:48<18:32,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 22/250 [01:48<18:23,  4.84s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 22/250 [01:53<18:23,  4.84s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 23/250 [01:53<18:25,  4.87s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 23/250 [01:57<18:25,  4.87s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 24/250 [01:57<18:14,  4.84s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 24/250 [02:02<18:14,  4.84s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 25/250 [02:02<18:18,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 25/250 [02:07<18:18,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 26/250 [02:07<18:04,  4.84s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 26/250 [02:12<18:04,  4.84s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 27/250 [02:12<18:04,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 27/250 [02:17<18:04,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 28/250 [02:17<17:52,  4.83s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 28/250 [02:22<17:52,  4.83s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 29/250 [02:22<18:00,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 29/250 [02:27<18:00,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 30/250 [02:27<17:56,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 30/250 [02:32<17:56,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 31/250 [02:32<17:40,  4.84s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 31/250 [02:36<17:40,  4.84s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 32/250 [02:36<17:42,  4.87s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 32/250 [02:41<17:42,  4.87s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 33/250 [02:41<17:26,  4.82s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 33/250 [02:46<17:26,  4.82s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 34/250 [02:46<17:24,  4.84s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 34/250 [02:51<17:24,  4.84s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 35/250 [02:51<17:11,  4.80s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 35/250 [02:56<17:11,  4.80s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 36/250 [02:56<17:19,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 36/250 [03:01<17:19,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 37/250 [03:01<17:09,  4.83s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 37/250 [03:05<17:09,  4.83s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 38/250 [03:05<17:09,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 38/250 [03:10<17:09,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 39/250 [03:10<16:54,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 39/250 [03:15<16:54,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 40/250 [03:15<16:53,  4.83s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 40/250 [03:20<16:53,  4.83s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 41/250 [03:20<16:49,  4.83s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 41/250 [03:25<16:49,  4.83s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 42/250 [03:25<16:40,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 42/250 [03:30<16:40,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 43/250 [03:30<16:45,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 43/250 [03:34<16:45,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 44/250 [03:34<16:34,  4.83s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 44/250 [03:39<16:34,  4.83s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 45/250 [03:39<16:35,  4.85s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 45/250 [03:44<16:35,  4.85s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 46/250 [03:44<16:23,  4.82s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 46/250 [03:49<16:23,  4.82s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 47/250 [03:49<16:28,  4.87s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 47/250 [03:54<16:28,  4.87s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 48/250 [03:54<16:15,  4.83s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 48/250 [03:59<16:15,  4.83s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 49/250 [03:59<16:16,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 49/250 [04:03<16:16,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 50/250 [04:03<16:04,  4.82s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 50/250 [04:09<16:04,  4.82s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 51/250 [04:09<16:21,  4.93s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 51/250 [04:14<16:21,  4.93s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 52/250 [04:14<16:20,  4.95s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 52/250 [04:19<16:20,  4.95s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 53/250 [04:19<16:31,  5.03s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 53/250 [04:24<16:31,  5.03s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 54/250 [04:24<16:22,  5.01s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 54/250 [04:29<16:22,  5.01s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 55/250 [04:29<16:23,  5.04s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 55/250 [04:34<16:23,  5.04s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 56/250 [04:34<16:12,  5.01s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 56/250 [04:39<16:12,  5.01s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 57/250 [04:39<16:15,  5.05s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 57/250 [04:44<16:15,  5.05s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 58/250 [04:44<16:02,  5.01s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 58/250 [04:49<16:02,  5.01s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 59/250 [04:49<16:03,  5.04s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 59/250 [04:54<16:03,  5.04s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 60/250 [04:54<15:53,  5.02s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 60/250 [04:59<15:53,  5.02s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 61/250 [04:59<15:59,  5.08s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 61/250 [05:04<15:59,  5.08s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▏| 62/250 [05:04<15:45,  5.03s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▏| 62/250 [05:09<15:45,  5.03s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▎| 63/250 [05:09<15:45,  5.06s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▎| 63/250 [05:14<15:45,  5.06s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 64/250 [05:14<15:40,  5.06s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 64/250 [05:19<15:40,  5.06s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 65/250 [05:19<15:26,  5.01s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 65/250 [05:24<15:26,  5.01s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 66/250 [05:24<15:17,  4.99s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 66/250 [05:29<15:17,  4.99s/epoch(s), train_multiclassaccuracyEarly stopping at epoch 66 with validation loss 0.000 and training loss 1.006\n\nTraining:  26%|▎| 66/250 [05:29<15:18,  4.99s/epoch(s), train_multiclassaccuracy\n{'MulticlassAccuracy': tensor(0.6333)}\n/home/chetana/tensorboard/2023-04-25_15-03/model_ckpt_final_full_data.pt\ntrain/Accuracy tensor(0.6333)\nval/Accuracy tensor(0.7205)\n",
  "history_begin_time" : 1682435015620,
  "history_end_time" : 1682435352683,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "c2lqcn",
  "indicator" : "Done"
},{
  "history_id" : "qw407x3ikex",
  "history_input" : "from training_and_plot_utils import *\nfrom model_components import *\nfrom device_config_and_data_loader import *\nfrom tqdm.auto import tqdm\n\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nprint(train_m)\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"MulticlassAccuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"MulticlassAccuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_final_full_data.pt\")\n\nprint(model_path)\n\n\nprint(\"train/Accuracy\", train_m[\"MulticlassAccuracy\"])\nprint(\"val/Accuracy\", val_m[\"MulticlassAccuracy\"])\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 24 samples from /home/chetana/ML_eddie/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-1999_lat14N-46N_lon166W-134W.npz.\nRead 12 samples from /home/chetana/ML_eddie/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\n======================================================================\nWriting Tensorboard logs to /home/chetana/tensorboard/2023-04-18_19-42\n======================================================================\n\nTraining:   0%|                                   | 0/250 [00:00<?, ?epoch(s)/s]\nTraining:   0%| | 0/250 [00:05<?, ?epoch(s)/s, train_multiclassaccuracy=0.239357\nTraining:   0%| | 1/250 [00:05<23:06,  5.57s/epoch(s), train_multiclassaccuracy=\nTraining:   0%| | 1/250 [00:10<23:06,  5.57s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 2/250 [00:10<21:48,  5.28s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 2/250 [00:15<21:48,  5.28s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 3/250 [00:15<21:41,  5.27s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 3/250 [00:20<21:41,  5.27s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 4/250 [00:20<21:16,  5.19s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 4/250 [00:26<21:16,  5.19s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 5/250 [00:26<21:16,  5.21s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 5/250 [00:31<21:16,  5.21s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 6/250 [00:31<20:57,  5.15s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 6/250 [00:36<20:57,  5.15s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 7/250 [00:36<21:01,  5.19s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 7/250 [00:41<21:01,  5.19s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 8/250 [00:41<20:58,  5.20s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 8/250 [00:46<20:58,  5.20s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 9/250 [00:46<20:28,  5.10s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 9/250 [00:51<20:28,  5.10s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 10/250 [00:51<20:28,  5.12s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 10/250 [00:56<20:28,  5.12s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 11/250 [00:56<20:01,  5.03s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 11/250 [01:01<20:01,  5.03s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 12/250 [01:01<19:57,  5.03s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 12/250 [01:06<19:57,  5.03s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 13/250 [01:06<19:38,  4.97s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 13/250 [01:11<19:38,  4.97s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 14/250 [01:11<19:31,  4.97s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 14/250 [01:16<19:31,  4.97s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 15/250 [01:16<19:17,  4.93s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 15/250 [01:21<19:17,  4.93s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 16/250 [01:21<19:19,  4.95s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 16/250 [01:26<19:19,  4.95s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 17/250 [01:26<19:06,  4.92s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 17/250 [01:31<19:06,  4.92s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 18/250 [01:31<19:02,  4.93s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 18/250 [01:36<19:02,  4.93s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 19/250 [01:36<19:01,  4.94s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 19/250 [01:40<19:01,  4.94s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 20/250 [01:40<18:52,  4.92s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 20/250 [01:45<18:52,  4.92s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 21/250 [01:45<18:52,  4.95s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 21/250 [01:50<18:52,  4.95s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 22/250 [01:50<18:39,  4.91s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 22/250 [01:55<18:39,  4.91s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 23/250 [01:55<18:51,  4.98s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 23/250 [02:00<18:51,  4.98s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 24/250 [02:00<18:32,  4.92s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 24/250 [02:05<18:32,  4.92s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 25/250 [02:05<18:29,  4.93s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 25/250 [02:10<18:29,  4.93s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 26/250 [02:10<18:17,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 26/250 [02:15<18:17,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 27/250 [02:15<18:13,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 27/250 [02:20<18:13,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 28/250 [02:20<17:58,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 28/250 [02:25<17:58,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 29/250 [02:25<18:06,  4.92s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 29/250 [02:30<18:06,  4.92s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 30/250 [02:30<18:08,  4.95s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 30/250 [02:35<18:08,  4.95s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 31/250 [02:35<17:57,  4.92s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 31/250 [02:40<17:57,  4.92s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 32/250 [02:40<17:57,  4.94s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 32/250 [02:44<17:57,  4.94s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 33/250 [02:44<17:41,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 33/250 [02:49<17:41,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 34/250 [02:49<17:43,  4.92s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 34/250 [02:54<17:43,  4.92s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 35/250 [02:54<17:33,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 35/250 [02:59<17:33,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 36/250 [02:59<17:39,  4.95s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 36/250 [03:04<17:39,  4.95s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 37/250 [03:04<17:22,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 37/250 [03:09<17:22,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 38/250 [03:09<17:26,  4.94s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 38/250 [03:14<17:26,  4.94s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 39/250 [03:14<17:16,  4.91s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 39/250 [03:19<17:16,  4.91s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 40/250 [03:19<17:16,  4.93s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 40/250 [03:24<17:16,  4.93s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 41/250 [03:24<17:21,  4.98s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 41/250 [03:29<17:21,  4.98s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 42/250 [03:29<17:09,  4.95s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 42/250 [03:34<17:09,  4.95s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 43/250 [03:34<17:14,  5.00s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 43/250 [03:39<17:14,  5.00s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 44/250 [03:39<17:01,  4.96s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 44/250 [03:44<17:01,  4.96s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 45/250 [03:44<17:01,  4.98s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 45/250 [03:49<17:01,  4.98s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 46/250 [03:49<16:43,  4.92s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 46/250 [03:54<16:43,  4.92s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 47/250 [03:54<16:53,  4.99s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 47/250 [03:59<16:53,  4.99s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 48/250 [03:59<16:51,  5.01s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 48/250 [04:04<16:51,  5.01s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 49/250 [04:04<16:57,  5.06s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 49/250 [04:09<16:57,  5.06s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 50/250 [04:09<16:49,  5.05s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 50/250 [04:14<16:49,  5.05s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 51/250 [04:14<16:52,  5.09s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 51/250 [04:19<16:52,  5.09s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 52/250 [04:19<16:45,  5.08s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 52/250 [04:24<16:45,  5.08s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 53/250 [04:24<16:35,  5.05s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 53/250 [04:29<16:35,  5.05s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 54/250 [04:29<16:22,  5.01s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 54/250 [04:34<16:22,  5.01s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 55/250 [04:34<16:19,  5.02s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 55/250 [04:39<16:19,  5.02s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 56/250 [04:39<16:05,  4.98s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 56/250 [04:44<16:05,  4.98s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 57/250 [04:44<15:58,  4.97s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 57/250 [04:49<15:58,  4.97s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 58/250 [04:49<15:40,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 58/250 [04:54<15:40,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 59/250 [04:54<15:39,  4.92s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 59/250 [04:59<15:39,  4.92s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 60/250 [04:59<15:31,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 60/250 [05:04<15:31,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 61/250 [05:04<15:40,  4.98s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 61/250 [05:09<15:40,  4.98s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▏| 62/250 [05:09<15:23,  4.91s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▏| 62/250 [05:14<15:23,  4.91s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▎| 63/250 [05:14<15:25,  4.95s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▎| 63/250 [05:19<15:25,  4.95s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 64/250 [05:19<15:30,  5.01s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 64/250 [05:24<15:30,  5.01s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 65/250 [05:24<15:23,  4.99s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 65/250 [05:29<15:23,  4.99s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 66/250 [05:29<15:26,  5.03s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 66/250 [05:34<15:26,  5.03s/epoch(s), train_multiclassaccuracyEarly stopping at epoch 66 with validation loss 0.000 and training loss 1.006\n\nTraining:  26%|▎| 66/250 [05:34<15:31,  5.06s/epoch(s), train_multiclassaccuracy\n{'MulticlassAccuracy': tensor(0.6333)}\n/home/chetana/tensorboard/2023-04-18_19-42/model_ckpt_final_full_data.pt\ntrain/Accuracy tensor(0.6333)\nval/Accuracy tensor(0.7205)\n",
  "history_begin_time" : 1681846915381,
  "history_end_time" : 1681847254868,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "c2lqcn",
  "indicator" : "Done"
},{
  "history_id" : "lswmqt4q1sy",
  "history_input" : "from training_and_plot_utils import *\nfrom model_components import *\nfrom device_config_and_data_loader import *\nfrom tqdm.auto import tqdm\n\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nprint(train_m)\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"MulticlassAccuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"MulticlassAccuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_final_full_data.pt\")\n\nprint(model_path)\n\n\nprint(\"train/Accuracy\", train_m[\"MulticlassAccuracy\"])\nprint(\"val/Accuracy\", val_m[\"MulticlassAccuracy\"])\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 24 samples from /home/chetana/ML_eddie/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-1999_lat14N-46N_lon166W-134W.npz.\nRead 12 samples from /home/chetana/ML_eddie/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\n======================================================================\nWriting Tensorboard logs to /home/chetana/tensorboard/2023-04-18_19-35\n======================================================================\n\nTraining:   0%|                                   | 0/250 [00:00<?, ?epoch(s)/s]\nTraining:   0%| | 0/250 [00:05<?, ?epoch(s)/s, train_multiclassaccuracy=0.239357\nTraining:   0%| | 1/250 [00:05<22:30,  5.42s/epoch(s), train_multiclassaccuracy=\nTraining:   0%| | 1/250 [00:10<22:30,  5.42s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 2/250 [00:10<21:21,  5.17s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 2/250 [00:15<21:21,  5.17s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 3/250 [00:15<21:18,  5.18s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 3/250 [00:20<21:18,  5.18s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 4/250 [00:20<21:11,  5.17s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 4/250 [00:26<21:11,  5.17s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 5/250 [00:26<21:16,  5.21s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 5/250 [00:31<21:16,  5.21s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 6/250 [00:31<21:00,  5.16s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 6/250 [00:36<21:00,  5.16s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 7/250 [00:36<20:54,  5.16s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 7/250 [00:41<20:54,  5.16s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 8/250 [00:41<20:52,  5.17s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 8/250 [00:46<20:52,  5.17s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 9/250 [00:46<20:41,  5.15s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 9/250 [00:51<20:41,  5.15s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 10/250 [00:51<20:35,  5.15s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 10/250 [00:56<20:35,  5.15s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 11/250 [00:56<20:27,  5.14s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 11/250 [01:02<20:27,  5.14s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 12/250 [01:02<20:39,  5.21s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 12/250 [01:07<20:39,  5.21s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 13/250 [01:07<20:19,  5.15s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 13/250 [01:12<20:19,  5.15s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 14/250 [01:12<20:18,  5.16s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 14/250 [01:17<20:18,  5.16s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 15/250 [01:17<20:02,  5.11s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 15/250 [01:22<20:02,  5.11s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 16/250 [01:22<19:57,  5.12s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 16/250 [01:27<19:57,  5.12s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 17/250 [01:27<19:39,  5.06s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 17/250 [01:32<19:39,  5.06s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 18/250 [01:32<19:35,  5.07s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 18/250 [01:37<19:35,  5.07s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 19/250 [01:37<19:21,  5.03s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 19/250 [01:42<19:21,  5.03s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 20/250 [01:42<18:59,  4.95s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 20/250 [01:47<18:59,  4.95s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 21/250 [01:47<18:52,  4.95s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 21/250 [01:52<18:52,  4.95s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 22/250 [01:52<18:43,  4.93s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 22/250 [01:57<18:43,  4.93s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 23/250 [01:57<18:54,  5.00s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 23/250 [02:02<18:54,  5.00s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 24/250 [02:02<18:51,  5.01s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 24/250 [02:07<18:51,  5.01s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 25/250 [02:07<18:55,  5.05s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 25/250 [02:12<18:55,  5.05s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 26/250 [02:12<18:42,  5.01s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 26/250 [02:17<18:42,  5.01s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 27/250 [02:17<18:43,  5.04s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 27/250 [02:22<18:43,  5.04s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 28/250 [02:22<18:28,  5.00s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 28/250 [02:27<18:28,  5.00s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 29/250 [02:27<18:30,  5.02s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 29/250 [02:32<18:30,  5.02s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 30/250 [02:32<18:21,  5.01s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 30/250 [02:37<18:21,  5.01s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 31/250 [02:37<17:58,  4.93s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 31/250 [02:42<17:58,  4.93s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 32/250 [02:42<17:57,  4.94s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 32/250 [02:46<17:57,  4.94s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 33/250 [02:46<17:41,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 33/250 [02:51<17:41,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 34/250 [02:51<17:40,  4.91s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 34/250 [02:56<17:40,  4.91s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 35/250 [02:56<17:33,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 35/250 [03:01<17:33,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 36/250 [03:01<17:35,  4.93s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 36/250 [03:06<17:35,  4.93s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 37/250 [03:06<17:21,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 37/250 [03:11<17:21,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 38/250 [03:11<17:19,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 38/250 [03:16<17:19,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 39/250 [03:16<17:02,  4.85s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 39/250 [03:21<17:02,  4.85s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 40/250 [03:21<17:00,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 40/250 [03:25<17:00,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 41/250 [03:25<16:57,  4.87s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 41/250 [03:30<16:57,  4.87s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 42/250 [03:30<16:48,  4.85s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 42/250 [03:35<16:48,  4.85s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 43/250 [03:35<16:50,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 43/250 [03:40<16:50,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 44/250 [03:40<16:40,  4.85s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 44/250 [03:45<16:40,  4.85s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 45/250 [03:45<16:40,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 45/250 [03:50<16:40,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 46/250 [03:50<16:25,  4.83s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 46/250 [03:55<16:25,  4.83s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 47/250 [03:55<16:36,  4.91s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 47/250 [04:00<16:36,  4.91s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 48/250 [04:00<16:29,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 48/250 [04:05<16:29,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 49/250 [04:05<16:42,  4.99s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 49/250 [04:10<16:42,  4.99s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 50/250 [04:10<16:32,  4.96s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 50/250 [04:15<16:32,  4.96s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 51/250 [04:15<16:35,  5.00s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 51/250 [04:20<16:35,  5.00s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 52/250 [04:20<16:22,  4.96s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 52/250 [04:25<16:22,  4.96s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 53/250 [04:25<16:19,  4.97s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 53/250 [04:30<16:19,  4.97s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 54/250 [04:30<16:09,  4.94s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 54/250 [04:35<16:09,  4.94s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 55/250 [04:35<16:14,  5.00s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 55/250 [04:39<16:14,  5.00s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 56/250 [04:39<15:57,  4.94s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 56/250 [04:44<15:57,  4.94s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 57/250 [04:44<15:57,  4.96s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 57/250 [04:49<15:57,  4.96s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 58/250 [04:49<15:46,  4.93s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 58/250 [04:54<15:46,  4.93s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 59/250 [04:54<15:42,  4.94s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 59/250 [04:59<15:42,  4.94s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 60/250 [04:59<15:29,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 60/250 [05:04<15:29,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 61/250 [05:04<15:33,  4.94s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 61/250 [05:09<15:33,  4.94s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▏| 62/250 [05:09<15:19,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▏| 62/250 [05:14<15:19,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▎| 63/250 [05:14<15:25,  4.95s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▎| 63/250 [05:19<15:25,  4.95s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 64/250 [05:19<15:20,  4.95s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 64/250 [05:24<15:20,  4.95s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 65/250 [05:24<15:06,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 65/250 [05:29<15:06,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 66/250 [05:29<15:08,  4.94s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 66/250 [05:34<15:08,  4.94s/epoch(s), train_multiclassaccuracyEarly stopping at epoch 66 with validation loss 0.000 and training loss 1.006\n\nTraining:  26%|▎| 66/250 [05:34<15:31,  5.06s/epoch(s), train_multiclassaccuracy\n{'MulticlassAccuracy': tensor(0.6333)}\n/home/chetana/tensorboard/2023-04-18_19-35/model_ckpt_final_full_data.pt\ntrain/Accuracy tensor(0.6333)\nval/Accuracy tensor(0.7205)\n",
  "history_begin_time" : 1681846526291,
  "history_end_time" : 1681846866444,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "c2lqcn",
  "indicator" : "Done"
},{
  "history_id" : "ez8v1xg6knb",
  "history_input" : "from training_and_plot_utils import *\nfrom model_components import *\nfrom get_device_config import *\nfrom tqdm.auto import tqdm\n\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nprint(train_m)\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"MulticlassAccuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"MulticlassAccuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_final_full_data.pt\")\n\nprint(model_path)\n\n\nprint(\"train/Accuracy\", train_m[\"MulticlassAccuracy\"])\nprint(\"val/Accuracy\", val_m[\"MulticlassAccuracy\"])\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 24 samples from /home/chetana/ML_eddie/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-1999_lat14N-46N_lon166W-134W.npz.\nRead 12 samples from /home/chetana/ML_eddie/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\n======================================================================\nWriting Tensorboard logs to /home/chetana/tensorboard/2023-04-18_19-19\n======================================================================\n\nTraining:   0%|                                   | 0/250 [00:00<?, ?epoch(s)/s]\nTraining:   0%| | 0/250 [00:05<?, ?epoch(s)/s, train_multiclassaccuracy=0.239357\nTraining:   0%| | 1/250 [00:05<21:43,  5.23s/epoch(s), train_multiclassaccuracy=\nTraining:   0%| | 1/250 [00:10<21:43,  5.23s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 2/250 [00:10<21:16,  5.15s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 2/250 [00:15<21:16,  5.15s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 3/250 [00:15<21:28,  5.22s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 3/250 [00:20<21:28,  5.22s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 4/250 [00:20<20:58,  5.12s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 4/250 [00:25<20:58,  5.12s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 5/250 [00:25<20:55,  5.12s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 5/250 [00:30<20:55,  5.12s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 6/250 [00:30<20:35,  5.06s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 6/250 [00:35<20:35,  5.06s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 7/250 [00:35<20:25,  5.04s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 7/250 [00:40<20:25,  5.04s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 8/250 [00:40<20:17,  5.03s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 8/250 [00:45<20:17,  5.03s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 9/250 [00:45<19:51,  4.95s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 9/250 [00:50<19:51,  4.95s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 10/250 [00:50<19:46,  4.95s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 10/250 [00:55<19:46,  4.95s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 11/250 [00:55<19:29,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 11/250 [01:00<19:29,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 12/250 [01:00<19:28,  4.91s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 12/250 [01:04<19:28,  4.91s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 13/250 [01:04<19:11,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 13/250 [01:09<19:11,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 14/250 [01:09<19:15,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 14/250 [01:14<19:15,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 15/250 [01:14<18:59,  4.85s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 15/250 [01:19<18:59,  4.85s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 16/250 [01:19<18:59,  4.87s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 16/250 [01:24<18:59,  4.87s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 17/250 [01:24<18:44,  4.83s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 17/250 [01:29<18:44,  4.83s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 18/250 [01:29<18:46,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 18/250 [01:34<18:46,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 19/250 [01:34<18:46,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 19/250 [01:38<18:46,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 20/250 [01:38<18:31,  4.83s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 20/250 [01:43<18:31,  4.83s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 21/250 [01:43<18:39,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 21/250 [01:48<18:39,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 22/250 [01:48<18:26,  4.85s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 22/250 [01:53<18:26,  4.85s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 23/250 [01:53<18:31,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 23/250 [01:58<18:31,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 24/250 [01:58<18:20,  4.87s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 24/250 [02:03<18:20,  4.87s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 25/250 [02:03<18:19,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 25/250 [02:08<18:19,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 26/250 [02:08<18:15,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 26/250 [02:13<18:15,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 27/250 [02:13<18:24,  4.95s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 27/250 [02:18<18:24,  4.95s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 28/250 [02:18<18:07,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 28/250 [02:22<18:07,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 29/250 [02:22<18:03,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 29/250 [02:27<18:03,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 30/250 [02:27<18:00,  4.91s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 30/250 [02:32<18:00,  4.91s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 31/250 [02:32<17:48,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 31/250 [02:37<17:48,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 32/250 [02:37<17:52,  4.92s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 32/250 [02:42<17:52,  4.92s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 33/250 [02:42<17:40,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 33/250 [02:47<17:40,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 34/250 [02:47<17:43,  4.93s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 34/250 [02:52<17:43,  4.93s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 35/250 [02:52<17:31,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 35/250 [02:57<17:31,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 36/250 [02:57<17:27,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 36/250 [03:02<17:27,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 37/250 [03:02<17:12,  4.85s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 37/250 [03:06<17:12,  4.85s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 38/250 [03:06<17:14,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 38/250 [03:11<17:14,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 39/250 [03:11<17:02,  4.85s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 39/250 [03:16<17:02,  4.85s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 40/250 [03:16<17:02,  4.87s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 40/250 [03:21<17:02,  4.87s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 41/250 [03:21<17:06,  4.91s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 41/250 [03:26<17:06,  4.91s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 42/250 [03:26<17:13,  4.97s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 42/250 [03:31<17:13,  4.97s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 43/250 [03:31<17:16,  5.01s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 43/250 [03:36<17:16,  5.01s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 44/250 [03:36<17:11,  5.01s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 44/250 [03:41<17:11,  5.01s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 45/250 [03:41<17:12,  5.04s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 45/250 [03:46<17:12,  5.04s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 46/250 [03:46<17:01,  5.01s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 46/250 [03:51<17:01,  5.01s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 47/250 [03:51<16:57,  5.01s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 47/250 [03:56<16:57,  5.01s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 48/250 [03:56<16:36,  4.94s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 48/250 [04:01<16:36,  4.94s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 49/250 [04:01<16:40,  4.98s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 49/250 [04:06<16:40,  4.98s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 50/250 [04:06<16:26,  4.93s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 50/250 [04:11<16:26,  4.93s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 51/250 [04:11<16:25,  4.95s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 51/250 [04:16<16:25,  4.95s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 52/250 [04:16<16:10,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 52/250 [04:21<16:10,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 53/250 [04:21<16:09,  4.92s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 53/250 [04:26<16:09,  4.92s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 54/250 [04:26<16:07,  4.93s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 54/250 [04:31<16:07,  4.93s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 55/250 [04:31<16:04,  4.94s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 55/250 [04:36<16:04,  4.94s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 56/250 [04:36<15:58,  4.94s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 56/250 [04:41<15:58,  4.94s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 57/250 [04:41<15:53,  4.94s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 57/250 [04:45<15:53,  4.94s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 58/250 [04:45<15:38,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 58/250 [04:50<15:38,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 59/250 [04:50<15:35,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 59/250 [04:55<15:35,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 60/250 [04:55<15:34,  4.92s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 60/250 [05:00<15:34,  4.92s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 61/250 [05:00<15:31,  4.93s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 61/250 [05:05<15:31,  4.93s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▏| 62/250 [05:05<15:17,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▏| 62/250 [05:10<15:17,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▎| 63/250 [05:10<15:14,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▎| 63/250 [05:15<15:14,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 64/250 [05:15<15:11,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 64/250 [05:20<15:11,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 65/250 [05:20<14:58,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 65/250 [05:25<14:58,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 66/250 [05:25<15:01,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 66/250 [05:29<15:01,  4.90s/epoch(s), train_multiclassaccuracyEarly stopping at epoch 66 with validation loss 0.000 and training loss 1.006\n\nTraining:  26%|▎| 66/250 [05:29<15:19,  5.00s/epoch(s), train_multiclassaccuracy\n{'MulticlassAccuracy': tensor(0.6333)}\n/home/chetana/tensorboard/2023-04-18_19-19/model_ckpt_final_full_data.pt\ntrain/Accuracy tensor(0.6333)\nval/Accuracy tensor(0.7205)\n",
  "history_begin_time" : 1681845578728,
  "history_end_time" : 1681845914431,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "c2lqcn",
  "indicator" : "Done"
},{
  "history_id" : "w8bh8zgrsat",
  "history_input" : "from training_and_plot_utils import *\nfrom model_components import *\nfrom get_device_config import *\nfrom tqdm.auto import tqdm\n\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nprint(train_m)\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"MulticlassAccuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"MulticlassAccuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_final_full_data.pt\")\n\nprint(model_path)\n\n\nprint(\"train/Accuracy\", train_m[\"MulticlassAccuracy\"])\nprint(\"val/Accuracy\", val_m[\"MulticlassAccuracy\"])\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 24 samples from /home/chetana/ML_eddie/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-1999_lat14N-46N_lon166W-134W.npz.\nRead 12 samples from /home/chetana/ML_eddie/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\n======================================================================\nWriting Tensorboard logs to /home/chetana/tensorboard/2023-04-18_18-26\n======================================================================\n\nTraining:   0%|                                   | 0/250 [00:00<?, ?epoch(s)/s]\nTraining:   0%| | 0/250 [00:06<?, ?epoch(s)/s, train_multiclassaccuracy=0.239357\nTraining:   0%| | 1/250 [00:06<28:33,  6.88s/epoch(s), train_multiclassaccuracy=\nTraining:   0%| | 1/250 [00:11<28:33,  6.88s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 2/250 [00:11<23:49,  5.76s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 2/250 [00:16<23:49,  5.76s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 3/250 [00:16<22:28,  5.46s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 3/250 [00:21<22:28,  5.46s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 4/250 [00:21<21:39,  5.28s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 4/250 [00:27<21:39,  5.28s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 5/250 [00:27<21:43,  5.32s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 5/250 [00:32<21:43,  5.32s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 6/250 [00:32<21:01,  5.17s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 6/250 [00:37<21:01,  5.17s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 7/250 [00:37<20:55,  5.17s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 7/250 [00:42<20:55,  5.17s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 8/250 [00:42<20:37,  5.11s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 8/250 [00:47<20:37,  5.11s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 9/250 [00:47<20:18,  5.06s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 9/250 [00:52<20:18,  5.06s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 10/250 [00:52<20:17,  5.07s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 10/250 [00:57<20:17,  5.07s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 11/250 [00:57<20:02,  5.03s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 11/250 [01:02<20:02,  5.03s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 12/250 [01:02<20:02,  5.05s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 12/250 [01:07<20:02,  5.05s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 13/250 [01:07<19:42,  4.99s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 13/250 [01:12<19:42,  4.99s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 14/250 [01:12<19:46,  5.03s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 14/250 [01:17<19:46,  5.03s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 15/250 [01:17<19:28,  4.97s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 15/250 [01:22<19:28,  4.97s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 16/250 [01:22<19:48,  5.08s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 16/250 [01:27<19:48,  5.08s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 17/250 [01:27<19:30,  5.02s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 17/250 [01:32<19:30,  5.02s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 18/250 [01:32<19:37,  5.08s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 18/250 [01:37<19:37,  5.08s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 19/250 [01:37<19:27,  5.05s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 19/250 [01:42<19:27,  5.05s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 20/250 [01:42<19:02,  4.97s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 20/250 [01:47<19:02,  4.97s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 21/250 [01:47<18:57,  4.97s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 21/250 [01:52<18:57,  4.97s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 22/250 [01:52<18:42,  4.92s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 22/250 [01:57<18:42,  4.92s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 23/250 [01:57<18:43,  4.95s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 23/250 [02:02<18:43,  4.95s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 24/250 [02:02<18:26,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 24/250 [02:07<18:26,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 25/250 [02:07<18:35,  4.96s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 25/250 [02:11<18:35,  4.96s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 26/250 [02:11<18:19,  4.91s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 26/250 [02:16<18:19,  4.91s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 27/250 [02:16<18:16,  4.91s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 27/250 [02:21<18:16,  4.91s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 28/250 [02:21<18:01,  4.87s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 28/250 [02:26<18:01,  4.87s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 29/250 [02:26<17:58,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 29/250 [02:31<17:58,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 30/250 [02:31<17:55,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 30/250 [02:36<17:55,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 31/250 [02:36<17:44,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 31/250 [02:41<17:44,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 32/250 [02:41<17:50,  4.91s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 32/250 [02:46<17:50,  4.91s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 33/250 [02:46<17:34,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 33/250 [02:50<17:34,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 34/250 [02:50<17:37,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 34/250 [02:55<17:37,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 35/250 [02:55<17:28,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 35/250 [03:00<17:28,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 36/250 [03:00<17:31,  4.91s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 36/250 [03:05<17:31,  4.91s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 37/250 [03:05<17:18,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 37/250 [03:10<17:18,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 38/250 [03:10<17:17,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 38/250 [03:15<17:17,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 39/250 [03:15<17:03,  4.85s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 39/250 [03:20<17:03,  4.85s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 40/250 [03:20<17:02,  4.87s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 40/250 [03:25<17:02,  4.87s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 41/250 [03:25<16:59,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 41/250 [03:29<16:59,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 42/250 [03:29<16:46,  4.84s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 42/250 [03:34<16:46,  4.84s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 43/250 [03:34<16:49,  4.87s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 43/250 [03:39<16:49,  4.87s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 44/250 [03:39<16:39,  4.85s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 44/250 [03:46<16:39,  4.85s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 45/250 [03:46<18:31,  5.42s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 45/250 [03:56<18:31,  5.42s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 46/250 [03:56<23:25,  6.89s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 46/250 [04:06<23:25,  6.89s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 47/250 [04:06<25:48,  7.63s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 47/250 [04:17<25:48,  7.63s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 48/250 [04:17<29:20,  8.71s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 48/250 [04:25<29:20,  8.71s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 49/250 [04:25<29:07,  8.69s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 49/250 [04:35<29:07,  8.69s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 50/250 [04:35<30:05,  9.03s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 50/250 [04:44<30:05,  9.03s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 51/250 [04:44<30:04,  9.07s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 51/250 [04:52<30:04,  9.07s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 52/250 [04:52<28:20,  8.59s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 52/250 [04:57<28:20,  8.59s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 53/250 [04:57<24:35,  7.49s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 53/250 [05:02<24:35,  7.49s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 54/250 [05:02<21:46,  6.67s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 54/250 [05:07<21:46,  6.67s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 55/250 [05:07<20:04,  6.18s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 55/250 [05:11<20:04,  6.18s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 56/250 [05:11<18:35,  5.75s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 56/250 [05:16<18:35,  5.75s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 57/250 [05:16<17:42,  5.50s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 57/250 [05:21<17:42,  5.50s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 58/250 [05:21<16:52,  5.27s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 58/250 [05:26<16:52,  5.27s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 59/250 [05:26<16:26,  5.16s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 59/250 [05:31<16:26,  5.16s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 60/250 [05:31<15:57,  5.04s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 60/250 [05:36<15:57,  5.04s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 61/250 [05:36<15:46,  5.01s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 61/250 [05:40<15:46,  5.01s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▏| 62/250 [05:40<15:27,  4.93s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▏| 62/250 [05:45<15:27,  4.93s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▎| 63/250 [05:45<15:23,  4.94s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▎| 63/250 [05:50<15:23,  4.94s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 64/250 [05:50<15:16,  4.93s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 64/250 [05:55<15:16,  4.93s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 65/250 [05:55<15:04,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 65/250 [06:00<15:04,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 66/250 [06:00<15:09,  4.94s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 66/250 [06:05<15:09,  4.94s/epoch(s), train_multiclassaccuracyEarly stopping at epoch 66 with validation loss 0.000 and training loss 1.006\n\nTraining:  26%|▎| 66/250 [06:05<16:58,  5.54s/epoch(s), train_multiclassaccuracy\n{'MulticlassAccuracy': tensor(0.6333)}\n/home/chetana/tensorboard/2023-04-18_18-26/model_ckpt_final_full_data.pt\ntrain/Accuracy tensor(0.6333)\nval/Accuracy tensor(0.7205)\n",
  "history_begin_time" : 1681842378314,
  "history_end_time" : 1681842749655,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "c2lqcn",
  "indicator" : "Done"
},{
  "history_id" : "wibzhsdhv9z",
  "history_input" : "from training_and_plot_utils import *\nfrom model_components import *\nfrom get_device_config import *\nfrom tqdm.auto import tqdm\n\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nprint(train_m)\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"MulticlassAccuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"MulticlassAccuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_final_full_data.pt\")\n\nprint(model_path)\n\n\nprint(\"train/Accuracy\", train_m[\"MulticlassAccuracy\"])\nprint(\"val/Accuracy\", val_m[\"MulticlassAccuracy\"])\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 24 samples from /home/chetana/ML_eddie/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-1999_lat14N-46N_lon166W-134W.npz.\nRead 12 samples from /home/chetana/ML_eddie/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/wibzhsdhv9z/run_model_training.py\", line 2, in <module>\n    from model_components import *\n  File \"/home/chetana/gw-workspace/wibzhsdhv9z/model_components.py\", line 97, in <module>\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\nAttributeError: type object 'datetime.datetime' has no attribute 'datetime'\n",
  "history_begin_time" : 1681841830583,
  "history_end_time" : 1681841836236,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "c2lqcn",
  "indicator" : "Failed"
},{
  "history_id" : "pa9t0439zic",
  "history_input" : "from training_and_plot_utils import *\nfrom model_components import *\nfrom get_device_config import *\nfrom tqdm.auto import tqdm\n\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nprint(train_m)\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"MulticlassAccuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"MulticlassAccuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_final_full_data.pt\")\n\nprint(model_path)\n\n\nprint(\"train/Accuracy\": train_m[\"MulticlassAccuracy\"])\nprint(\"val/Accuracy\": val_m[\"MulticlassAccuracy\"])\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "  File \"/home/chetana/gw-workspace/pa9t0439zic/run_model_training.py\", line 78\n    print(\"train/Accuracy\": train_m[\"MulticlassAccuracy\"])\n                          ^\nSyntaxError: invalid syntax\n",
  "history_begin_time" : 1681841574852,
  "history_end_time" : 1681841576354,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "c2lqcn",
  "indicator" : "Done"
},{
  "history_id" : "m05zfgvsz1p",
  "history_input" : "from training_and_plot_utils import *\nfrom model_components import *\nfrom get_device_config import *\nfrom tqdm.auto import tqdm\n\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nprint(train_m)\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"MulticlassAccuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"MulticlassAccuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_final_full_data.pt\")\n\nprint(model_path)\n\n\nprint(\"train/Accuracy\": train_m[\"MulticlassAccuracy\"])\nprint(\"val/Accuracy\": val_m[\"MulticlassAccuracy\"])\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "  File \"/home/chetana/gw-workspace/m05zfgvsz1p/run_model_training.py\", line 78\n    print(\"train/Accuracy\": train_m[\"MulticlassAccuracy\"])\n                          ^\n",
  "history_begin_time" : 1681841183930,
  "history_end_time" : 1681841186148,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "c2lqcn",
  "indicator" : "Failed"
},{
  "history_id" : "uto2dhd5r5l",
  "history_input" : "from model_training_utils import add_hparams, EarlyStopping\nfrom get_device_config import *\nfrom loss_function import *\nfrom set_optmizer_and_scheduler import *\nfrom set_summary_writer import *\nfrom model_utils import *\nfrom torch_metrics_utils import *\nfrom tqdm.auto import tqdm\n\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nprint(train_m)\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"MulticlassAccuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"MulticlassAccuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, \"model_ckpt_final.pt\")\nprint(model_path)\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 21 samples from /home/chetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 12 samples from /home/chetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\n======================================================================\nWriting Tensorboard logs to /home/chetana/tensorboard/2023-04-05_04-05\n======================================================================\n\nTraining:   0%|                                   | 0/250 [00:00<?, ?epoch(s)/s]\nTraining:   0%| | 0/250 [00:05<?, ?epoch(s)/s, train_multiclassaccuracy=0.237458\nTraining:   0%| | 1/250 [00:05<22:39,  5.46s/epoch(s), train_multiclassaccuracy=\nTraining:   0%| | 1/250 [00:10<22:39,  5.46s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 2/250 [00:10<20:54,  5.06s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 2/250 [00:15<20:54,  5.06s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 3/250 [00:15<20:39,  5.02s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 3/250 [00:19<20:39,  5.02s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 4/250 [00:19<20:07,  4.91s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 4/250 [00:24<20:07,  4.91s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 5/250 [00:24<20:05,  4.92s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 5/250 [00:29<20:05,  4.92s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 6/250 [00:29<19:44,  4.85s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 6/250 [00:34<19:44,  4.85s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 7/250 [00:34<19:43,  4.87s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 7/250 [00:39<19:43,  4.87s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 8/250 [00:39<19:35,  4.86s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 8/250 [00:44<19:35,  4.86s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 9/250 [00:44<19:18,  4.81s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 9/250 [00:48<19:18,  4.81s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 10/250 [00:48<19:20,  4.84s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 10/250 [00:53<19:20,  4.84s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 11/250 [00:53<19:09,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 11/250 [00:58<19:09,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 12/250 [00:58<19:08,  4.83s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 12/250 [01:03<19:08,  4.83s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 13/250 [01:03<18:54,  4.79s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 13/250 [01:08<18:54,  4.79s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 14/250 [01:08<18:54,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 14/250 [01:12<18:54,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 15/250 [01:12<18:39,  4.77s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 15/250 [01:17<18:39,  4.77s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 16/250 [01:17<18:39,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 16/250 [01:22<18:39,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 17/250 [01:22<18:23,  4.74s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 17/250 [01:27<18:23,  4.74s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 18/250 [01:27<18:31,  4.79s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 18/250 [01:31<18:31,  4.79s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 19/250 [01:31<18:19,  4.76s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 19/250 [01:36<18:19,  4.76s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 20/250 [01:36<18:19,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 20/250 [01:41<18:19,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 21/250 [01:41<18:15,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 21/250 [01:46<18:15,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 22/250 [01:46<18:02,  4.75s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 22/250 [01:50<18:02,  4.75s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 23/250 [01:50<18:01,  4.76s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 23/250 [01:55<18:01,  4.76s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 24/250 [01:55<17:47,  4.72s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 24/250 [02:00<17:47,  4.72s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 25/250 [02:00<17:48,  4.75s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 25/250 [02:05<17:48,  4.75s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 26/250 [02:05<17:37,  4.72s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 26/250 [02:09<17:37,  4.72s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 27/250 [02:09<17:42,  4.77s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 27/250 [02:14<17:42,  4.77s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 28/250 [02:14<17:31,  4.73s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 28/250 [02:19<17:31,  4.73s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 29/250 [02:19<17:31,  4.76s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 29/250 [02:24<17:31,  4.76s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 30/250 [02:24<17:21,  4.73s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 30/250 [02:28<17:21,  4.73s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 31/250 [02:28<17:25,  4.77s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 31/250 [02:33<17:25,  4.77s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 32/250 [02:33<17:25,  4.79s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 32/250 [02:38<17:25,  4.79s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 33/250 [02:38<17:11,  4.75s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 33/250 [02:43<17:11,  4.75s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 34/250 [02:43<17:12,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 34/250 [02:47<17:12,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 35/250 [02:47<16:59,  4.74s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 35/250 [02:52<16:59,  4.74s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 36/250 [02:52<17:02,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 36/250 [02:57<17:02,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 37/250 [02:57<16:52,  4.75s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 37/250 [03:02<16:52,  4.75s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 38/250 [03:02<16:54,  4.79s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 38/250 [03:07<16:54,  4.79s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 39/250 [03:07<16:48,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 39/250 [03:11<16:48,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 40/250 [03:11<16:47,  4.80s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 40/250 [03:16<16:47,  4.80s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 41/250 [03:16<16:37,  4.77s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 41/250 [03:21<16:37,  4.77s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 42/250 [03:21<16:34,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 42/250 [03:26<16:34,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 43/250 [03:26<16:19,  4.73s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 43/250 [03:30<16:19,  4.73s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 44/250 [03:30<16:17,  4.75s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 44/250 [03:35<16:17,  4.75s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 45/250 [03:35<16:05,  4.71s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 45/250 [03:40<16:05,  4.71s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 46/250 [03:40<16:04,  4.73s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 46/250 [03:44<16:04,  4.73s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 47/250 [03:44<15:54,  4.70s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 47/250 [03:49<15:54,  4.70s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 48/250 [03:49<15:57,  4.74s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 48/250 [03:54<15:57,  4.74s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 49/250 [03:54<16:05,  4.80s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 49/250 [03:59<16:05,  4.80s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 50/250 [03:59<15:55,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 50/250 [04:04<15:55,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 51/250 [04:04<16:00,  4.82s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 51/250 [04:08<16:00,  4.82s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 52/250 [04:08<15:46,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 52/250 [04:13<15:46,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 53/250 [04:13<15:43,  4.79s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 53/250 [04:18<15:43,  4.79s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 54/250 [04:18<15:30,  4.75s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 54/250 [04:23<15:30,  4.75s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 55/250 [04:23<15:31,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 55/250 [04:28<15:31,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 56/250 [04:28<15:26,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 56/250 [04:33<15:26,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 57/250 [04:33<15:33,  4.84s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 57/250 [04:37<15:33,  4.84s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 58/250 [04:37<15:21,  4.80s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 58/250 [04:42<15:21,  4.80s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 59/250 [04:42<15:18,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 59/250 [04:47<15:18,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 60/250 [04:47<15:09,  4.79s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 60/250 [04:52<15:09,  4.79s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 61/250 [04:52<15:08,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 61/250 [04:56<15:08,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▏| 62/250 [04:56<14:59,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▏| 62/250 [05:01<14:59,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▎| 63/250 [05:01<14:59,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▎| 63/250 [05:06<14:59,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 64/250 [05:06<14:47,  4.77s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 64/250 [05:11<14:47,  4.77s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 65/250 [05:11<14:48,  4.80s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 65/250 [05:16<14:48,  4.80s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 66/250 [05:16<14:38,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 66/250 [05:20<14:38,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  27%|▎| 67/250 [05:20<14:40,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:  27%|▎| 67/250 [05:25<14:40,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:  27%|▎| 68/250 [05:25<14:37,  4.82s/epoch(s), train_multiclassaccuracy\nTraining:  27%|▎| 68/250 [05:30<14:37,  4.82s/epoch(s), train_multiclassaccuracyEarly stopping at epoch 68 with validation loss 0.000 and training loss 1.006\n\nTraining:  27%|▎| 68/250 [05:30<14:44,  4.86s/epoch(s), train_multiclassaccuracy\n{'MulticlassAccuracy': tensor(0.6485)}\n/home/chetana/tensorboard/2023-04-05_04-05/model_ckpt_final.pt\n",
  "history_begin_time" : 1680667507343,
  "history_end_time" : 1680667843468,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "c2lqcn",
  "indicator" : "Done"
},{
  "history_id" : "e4q0qc4iegb",
  "history_input" : "from model_training_utils import add_hparams, EarlyStopping\nfrom get_device_config import *\nfrom loss_function import *\nfrom set_optmizer_and_scheduler import *\nfrom set_summary_writer import *\nfrom model_utils import *\nfrom torch_metrics_utils import *\nfrom tqdm.auto import tqdm\n\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nprint(train_m)\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"MulticlassAccuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"MulticlassAccuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, \"model_ckpt_final.pt\")\nprint(model_path)\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 21 samples from /home/chetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 1 samples from /home/chetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\n======================================================================\nWriting Tensorboard logs to /home/chetana/tensorboard/2023-04-05_03-55\n======================================================================\n\nTraining:   0%|                                   | 0/250 [00:00<?, ?epoch(s)/s]\nTraining:   0%|                                   | 0/250 [00:01<?, ?epoch(s)/s]\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/e4q0qc4iegb/run_model_training.py\", line 23, in <module>\n    train_loss, val_loss, train_m, val_m = run_epoch(\n  File \"/home/chetana/gw-workspace/e4q0qc4iegb/model_utils.py\", line 72, in run_epoch\n    indices_ = np.random.choice(\n  File \"mtrand.pyx\", line 965, in numpy.random.mtrand.RandomState.choice\nValueError: Cannot take a larger sample than population when 'replace=False'\n",
  "history_begin_time" : 1680666905847,
  "history_end_time" : 1680666912952,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "c2lqcn",
  "indicator" : "Failed"
},{
  "history_id" : "4l345jmkk9c",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1680032244939,
  "history_end_time" : 1680032244939,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "c2lqcn",
  "indicator" : "Skipped"
},{
  "history_id" : "613htwg0qek",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1680029557968,
  "history_end_time" : 1680029661150,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "c2lqcn",
  "indicator" : "Stopped"
},{
  "history_id" : "ezwirlljpcv",
  "history_input" : "from model_training_utils import add_hparams, EarlyStopping\nfrom get_device_config import *\nfrom loss_function import *\nfrom set_optmizer_and_scheduler import *\nfrom set_summary_writer import *\nfrom model_utils import *\nfrom torch_metrics_utils import *\nfrom tqdm.auto import tqdm\n\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nprint(train_m)\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"MulticlassAccuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"MulticlassAccuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\nprint(model_path)\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 21 samples from /home/chetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 12 samples from /home/chetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\n======================================================================\nWriting Tensorboard logs to /home/chetana/tensorboard/2023-03-15_03-26\n======================================================================\n\nTraining:   0%|                                   | 0/250 [00:00<?, ?epoch(s)/s]\nTraining:   0%| | 0/250 [00:06<?, ?epoch(s)/s, train_multiclassaccuracy=0.23686,\nTraining:   0%| | 1/250 [00:06<28:00,  6.75s/epoch(s), train_multiclassaccuracy=\nTraining:   0%| | 1/250 [00:12<28:00,  6.75s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 2/250 [00:12<24:18,  5.88s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 2/250 [00:17<24:18,  5.88s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 3/250 [00:17<23:42,  5.76s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 3/250 [00:23<23:42,  5.76s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 4/250 [00:23<23:07,  5.64s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 4/250 [00:28<23:07,  5.64s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 5/250 [00:28<22:58,  5.63s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 5/250 [00:33<22:58,  5.63s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 6/250 [00:33<22:22,  5.50s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 6/250 [00:39<22:22,  5.50s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 7/250 [00:39<22:15,  5.50s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 7/250 [00:44<22:15,  5.50s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 8/250 [00:44<22:05,  5.48s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 8/250 [00:50<22:05,  5.48s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 9/250 [00:50<21:56,  5.46s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 9/250 [00:55<21:56,  5.46s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 10/250 [00:55<21:44,  5.43s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 10/250 [01:00<21:44,  5.43s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 11/250 [01:00<21:25,  5.38s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 11/250 [01:06<21:25,  5.38s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 12/250 [01:06<21:30,  5.42s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 12/250 [01:11<21:30,  5.42s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 13/250 [01:11<21:10,  5.36s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 13/250 [01:17<21:10,  5.36s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 14/250 [01:17<21:09,  5.38s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 14/250 [01:22<21:09,  5.38s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 15/250 [01:22<20:54,  5.34s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 15/250 [01:27<20:54,  5.34s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 16/250 [01:27<20:59,  5.38s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 16/250 [01:33<20:59,  5.38s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 17/250 [01:33<20:41,  5.33s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 17/250 [01:38<20:41,  5.33s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 18/250 [01:38<20:46,  5.37s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 18/250 [01:43<20:46,  5.37s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 19/250 [01:43<20:30,  5.33s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 19/250 [01:49<20:30,  5.33s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 20/250 [01:49<20:34,  5.37s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 20/250 [01:54<20:34,  5.37s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 21/250 [01:54<20:38,  5.41s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 21/250 [01:59<20:38,  5.41s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 22/250 [01:59<20:25,  5.37s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 22/250 [02:05<20:25,  5.37s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 23/250 [02:05<20:28,  5.41s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 23/250 [02:10<20:28,  5.41s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 24/250 [02:10<20:06,  5.34s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 24/250 [02:16<20:06,  5.34s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 25/250 [02:16<20:03,  5.35s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 25/250 [02:21<20:03,  5.35s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 26/250 [02:21<19:47,  5.30s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 26/250 [02:26<19:47,  5.30s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 27/250 [02:26<19:55,  5.36s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 27/250 [02:31<19:55,  5.36s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 28/250 [02:31<19:42,  5.33s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 28/250 [02:37<19:42,  5.33s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 29/250 [02:37<19:43,  5.35s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 29/250 [02:42<19:43,  5.35s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 30/250 [02:42<19:33,  5.33s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 30/250 [02:48<19:33,  5.33s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 31/250 [02:48<19:36,  5.37s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 31/250 [02:53<19:36,  5.37s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 32/250 [02:53<19:29,  5.36s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 32/250 [02:58<19:29,  5.36s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 33/250 [02:58<19:12,  5.31s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 33/250 [03:04<19:12,  5.31s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 34/250 [03:04<19:11,  5.33s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 34/250 [03:09<19:11,  5.33s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 35/250 [03:09<19:00,  5.30s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 35/250 [03:14<19:00,  5.30s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 36/250 [03:14<19:02,  5.34s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 36/250 [03:19<19:02,  5.34s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 37/250 [03:19<18:49,  5.30s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 37/250 [03:25<18:49,  5.30s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 38/250 [03:25<18:55,  5.36s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 38/250 [03:30<18:55,  5.36s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 39/250 [03:30<18:42,  5.32s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 39/250 [03:36<18:42,  5.32s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 40/250 [03:36<18:44,  5.35s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 40/250 [03:41<18:44,  5.35s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 41/250 [03:41<18:31,  5.32s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 41/250 [03:47<18:31,  5.32s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 42/250 [03:47<19:31,  5.63s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 42/250 [04:00<19:31,  5.63s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 43/250 [04:00<27:14,  7.90s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 43/250 [04:11<27:14,  7.90s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 44/250 [04:11<30:25,  8.86s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 44/250 [04:21<30:25,  8.86s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 45/250 [04:21<31:27,  9.21s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 45/250 [04:35<31:27,  9.21s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 46/250 [04:35<35:27, 10.43s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 46/250 [04:47<35:27, 10.43s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 47/250 [04:47<37:11, 10.99s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 47/250 [05:03<37:11, 10.99s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 48/250 [05:04<42:30, 12.63s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 48/250 [05:17<42:30, 12.63s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 49/250 [05:17<42:45, 12.77s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 49/250 [05:29<42:45, 12.77s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 50/250 [05:29<42:23, 12.72s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 50/250 [05:42<42:23, 12.72s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 51/250 [05:42<42:07, 12.70s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 51/250 [05:52<42:07, 12.70s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 52/250 [05:52<39:43, 12.04s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 52/250 [05:58<39:43, 12.04s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 53/250 [05:58<33:06, 10.08s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 53/250 [06:03<33:06, 10.08s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 54/250 [06:03<28:28,  8.72s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 54/250 [06:09<28:28,  8.72s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 55/250 [06:09<25:22,  7.81s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 55/250 [06:14<25:22,  7.81s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 56/250 [06:14<22:54,  7.09s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 56/250 [06:20<22:54,  7.09s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 57/250 [06:20<21:22,  6.64s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 57/250 [06:26<21:22,  6.64s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 58/250 [06:26<20:07,  6.29s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 58/250 [06:31<20:07,  6.29s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 59/250 [06:31<19:24,  6.10s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 59/250 [06:37<19:24,  6.10s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 60/250 [06:37<18:37,  5.88s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 60/250 [06:42<18:37,  5.88s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 61/250 [06:42<18:26,  5.86s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 61/250 [06:48<18:26,  5.86s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▏| 62/250 [06:48<18:01,  5.75s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▏| 62/250 [06:54<18:01,  5.75s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▎| 63/250 [06:54<17:54,  5.74s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▎| 63/250 [06:59<17:54,  5.74s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 64/250 [06:59<17:34,  5.67s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 64/250 [07:05<17:34,  5.67s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 65/250 [07:05<17:27,  5.66s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 65/250 [07:10<17:27,  5.66s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 66/250 [07:10<17:06,  5.58s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 66/250 [07:16<17:06,  5.58s/epoch(s), train_multiclassaccuracy\nTraining:  27%|▎| 67/250 [07:16<16:59,  5.57s/epoch(s), train_multiclassaccuracy\nTraining:  27%|▎| 67/250 [07:21<16:59,  5.57s/epoch(s), train_multiclassaccuracyEarly stopping at epoch 67 with validation loss 0.000 and training loss 1.010\n\nTraining:  27%|▎| 67/250 [07:21<20:06,  6.59s/epoch(s), train_multiclassaccuracy\n{'MulticlassAccuracy': tensor(0.6411)}\n/home/chetana/tensorboard/2023-03-15_03-26/model_ckpt_68.pt\n",
  "history_begin_time" : 1678850774929,
  "history_end_time" : 1678851222622,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "ycru82",
  "indicator" : "Done"
},{
  "history_id" : "eirz1k089jx",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678837413885,
  "history_end_time" : 1678837413885,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "ycru82",
  "indicator" : "Skipped"
},{
  "history_id" : "c0argvkmbzh",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678837361474,
  "history_end_time" : 1678837361474,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "ycru82",
  "indicator" : "Skipped"
},{
  "history_id" : "0xf6rpyr59t",
  "history_input" : null,
  "history_output" : "Exhausted available authentication methods",
  "history_begin_time" : 1678250140126,
  "history_end_time" : 1678250142781,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "ycru82",
  "indicator" : "Failed"
},{
  "history_id" : "i9LoOyVLxGxi",
  "history_input" : "from model_training_utils import add_hparams, EarlyStopping\nfrom get_device_config import *\nfrom loss_function import *\nfrom set_optmizer_and_scheduler import *\nfrom set_summary_writer import *\nfrom model_utils import *\nfrom torch_metrics_utils import *\nfrom tqdm.auto import tqdm\n\n\"\"\"\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nprint(train_m)\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"MulticlassAccuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"MulticlassAccuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\"\"\"\n\n# save model to tensorboard folder\nN = 2\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\nprint(model_path)\n#torch.save(model.state_dict(), model_path)",
  "history_output" : "Read 252 samples from /home/chetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 12 samples from /home/chetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\n======================================================================\nWriting Tensorboard logs to /home/chetana/tensorboard/2023-03-07_18-13\n======================================================================\n/home/chetana/tensorboard/2023-03-07_18-13/model_ckpt_3.pt\n",
  "history_begin_time" : 1678212810307,
  "history_end_time" : 1678212815277,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "QJCRHZFLRKl3",
  "history_input" : "from model_training_utils import add_hparams, EarlyStopping\nfrom get_device_config import *\nfrom loss_function import *\nfrom set_optmizer_and_scheduler import *\nfrom set_summary_writer import *\nfrom model_utils import *\nfrom torch_metrics_utils import *\nfrom tqdm.auto import tqdm\n\n\"\"\"\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nprint(train_m)\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"MulticlassAccuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"MulticlassAccuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\"\"\"\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\nprint(model_path)\n#torch.save(model.state_dict(), model_path)",
  "history_output" : "Read 252 samples from /home/chetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 12 samples from /home/chetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\n======================================================================\nWriting Tensorboard logs to /home/chetana/tensorboard/2023-03-07_18-12\n======================================================================\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/QJCRHZFLRKl3/run_model_training.py\", line 77, in <module>\n    model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\nNameError: name 'N' is not defined\n",
  "history_begin_time" : 1678212770065,
  "history_end_time" : 1678212775033,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "ctcLRcIBvznC",
  "history_input" : "from model_training_utils import add_hparams, EarlyStopping\nfrom get_device_config import *\nfrom loss_function import *\nfrom set_optmizer_and_scheduler import *\nfrom set_summary_writer import *\nfrom model_utils import *\nfrom torch_metrics_utils import *\nfrom tqdm.auto import tqdm\n\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nprint(train_m)\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"MulticlassAccuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"MulticlassAccuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 252 samples from /home/chetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 12 samples from /home/chetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\n======================================================================\nWriting Tensorboard logs to /home/chetana/tensorboard/2023-03-07_17-45\n======================================================================\n\nTraining:   0%|                                   | 0/250 [00:00<?, ?epoch(s)/s]\nTraining:   0%| | 0/250 [00:12<?, ?epoch(s)/s, train_multiclassaccuracy=0.244053\nTraining:   0%| | 1/250 [00:12<51:41, 12.46s/epoch(s), train_multiclassaccuracy=\nTraining:   0%| | 1/250 [00:22<51:41, 12.46s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 2/250 [00:22<46:35, 11.27s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 2/250 [00:33<46:35, 11.27s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 3/250 [00:33<45:00, 10.94s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 3/250 [00:43<45:00, 10.94s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 4/250 [00:43<43:49, 10.69s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 4/250 [00:54<43:49, 10.69s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 5/250 [00:54<43:06, 10.56s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 5/250 [01:04<43:06, 10.56s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 6/250 [01:04<42:40, 10.50s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 6/250 [01:14<42:40, 10.50s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 7/250 [01:14<42:11, 10.42s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 7/250 [01:24<42:11, 10.42s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 8/250 [01:24<41:40, 10.33s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 8/250 [01:35<41:40, 10.33s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 9/250 [01:35<41:34, 10.35s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 9/250 [01:45<41:34, 10.35s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 10/250 [01:45<40:52, 10.22s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 10/250 [01:55<40:52, 10.22s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 11/250 [01:55<40:31, 10.17s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 11/250 [02:05<40:31, 10.17s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 12/250 [02:05<40:15, 10.15s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 12/250 [02:15<40:15, 10.15s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 13/250 [02:15<40:16, 10.20s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 13/250 [02:26<40:16, 10.20s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 14/250 [02:26<40:21, 10.26s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 14/250 [02:35<40:21, 10.26s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 15/250 [02:35<39:45, 10.15s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 15/250 [02:46<39:45, 10.15s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 16/250 [02:46<39:38, 10.16s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 16/250 [02:56<39:38, 10.16s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 17/250 [02:56<39:19, 10.13s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 17/250 [03:06<39:19, 10.13s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 18/250 [03:06<39:18, 10.17s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 18/250 [03:16<39:18, 10.17s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 19/250 [03:16<39:05, 10.15s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 19/250 [03:26<39:05, 10.15s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 20/250 [03:26<39:05, 10.20s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 20/250 [03:36<39:05, 10.20s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 21/250 [03:36<38:50, 10.18s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 21/250 [03:47<38:50, 10.18s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 22/250 [03:47<38:40, 10.18s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 22/250 [03:57<38:40, 10.18s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 23/250 [03:57<38:32, 10.19s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 23/250 [04:07<38:32, 10.19s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 24/250 [04:07<38:07, 10.12s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 24/250 [04:17<38:07, 10.12s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 25/250 [04:17<38:01, 10.14s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 25/250 [04:27<38:01, 10.14s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 26/250 [04:27<37:38, 10.08s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 26/250 [04:37<37:38, 10.08s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 27/250 [04:37<37:42, 10.14s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 27/250 [04:47<37:42, 10.14s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 28/250 [04:47<37:16, 10.08s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 28/250 [04:58<37:16, 10.08s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 29/250 [04:58<37:30, 10.18s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 29/250 [05:07<37:30, 10.18s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 30/250 [05:07<36:56, 10.07s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 30/250 [05:18<36:56, 10.07s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 31/250 [05:18<36:53, 10.11s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 31/250 [05:28<36:53, 10.11s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 32/250 [05:28<36:51, 10.14s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 32/250 [05:38<36:51, 10.14s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 33/250 [05:38<36:40, 10.14s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 33/250 [05:48<36:40, 10.14s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 34/250 [05:48<36:39, 10.18s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 34/250 [05:59<36:39, 10.18s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 35/250 [05:59<36:35, 10.21s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 35/250 [06:09<36:35, 10.21s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 36/250 [06:09<36:29, 10.23s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 36/250 [06:19<36:29, 10.23s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 37/250 [06:19<36:06, 10.17s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 37/250 [06:29<36:06, 10.17s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 38/250 [06:29<35:53, 10.16s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 38/250 [06:39<35:53, 10.16s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 39/250 [06:39<35:37, 10.13s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 39/250 [06:49<35:37, 10.13s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 40/250 [06:49<35:35, 10.17s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 40/250 [06:59<35:35, 10.17s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 41/250 [06:59<35:12, 10.11s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 41/250 [07:10<35:12, 10.11s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 42/250 [07:10<35:12, 10.16s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 42/250 [07:20<35:12, 10.16s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 43/250 [07:20<35:00, 10.15s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 43/250 [07:30<35:00, 10.15s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 44/250 [07:30<35:00, 10.20s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 44/250 [07:40<35:00, 10.20s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 45/250 [07:40<34:55, 10.22s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 45/250 [07:50<34:55, 10.22s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 46/250 [07:50<34:37, 10.19s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 46/250 [08:01<34:37, 10.19s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 47/250 [08:01<34:29, 10.20s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 47/250 [08:11<34:29, 10.20s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 48/250 [08:11<34:11, 10.16s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 48/250 [08:21<34:11, 10.16s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 49/250 [08:21<34:04, 10.17s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 49/250 [08:31<34:04, 10.17s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 50/250 [08:31<33:41, 10.11s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 50/250 [08:41<33:41, 10.11s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 51/250 [08:41<33:33, 10.12s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 51/250 [08:51<33:33, 10.12s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 52/250 [08:51<33:17, 10.09s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 52/250 [09:01<33:17, 10.09s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 53/250 [09:01<33:11, 10.11s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 53/250 [09:11<33:11, 10.11s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 54/250 [09:11<33:00, 10.10s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 54/250 [09:21<33:00, 10.10s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 55/250 [09:21<32:54, 10.13s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 55/250 [09:31<32:54, 10.13s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 56/250 [09:31<32:39, 10.10s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 56/250 [09:42<32:39, 10.10s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 57/250 [09:42<32:32, 10.12s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 57/250 [09:52<32:32, 10.12s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 58/250 [09:52<32:17, 10.09s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 58/250 [10:02<32:17, 10.09s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 59/250 [10:02<32:12, 10.12s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 59/250 [10:12<32:12, 10.12s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 60/250 [10:12<32:03, 10.13s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 60/250 [10:22<32:03, 10.13s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 61/250 [10:22<31:45, 10.08s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 61/250 [10:32<31:45, 10.08s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▏| 62/250 [10:32<31:42, 10.12s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▏| 62/250 [10:42<31:42, 10.12s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▎| 63/250 [10:42<31:27, 10.10s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▎| 63/250 [10:52<31:27, 10.10s/epoch(s), train_multiclassaccuracyEarly stopping at epoch 63 with validation loss 0.000 and training loss 1.010\n\nTraining:  25%|▎| 63/250 [10:52<32:17, 10.36s/epoch(s), train_multiclassaccuracy\n{'MulticlassAccuracy': tensor(0.6377)}\n",
  "history_begin_time" : 1678211089162,
  "history_end_time" : 1678211770583,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "TP46kpenHUD0",
  "history_input" : "from model_training_utils import add_hparams, EarlyStopping\nfrom get_device_config import *\nfrom loss_function import *\nfrom set_optmizer_and_scheduler import *\nfrom set_summary_writer import *\nfrom model_utils import *\nfrom torch_metrics_utils import *\nfrom tqdm.auto import tqdm\n\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nprint(train_m)\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 252 samples from /home/chetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 12 samples from /home/chetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\n======================================================================\nWriting Tensorboard logs to /home/chetana/tensorboard/2023-03-06_19-15\n======================================================================\n\nTraining:   0%|                                   | 0/250 [00:00<?, ?epoch(s)/s]\nTraining:   0%| | 0/250 [00:11<?, ?epoch(s)/s, train_multiclassaccuracy=0.244053\nTraining:   0%| | 1/250 [00:11<46:09, 11.12s/epoch(s), train_multiclassaccuracy=\nTraining:   0%| | 1/250 [00:21<46:09, 11.12s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 2/250 [00:21<43:48, 10.60s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 2/250 [00:31<43:48, 10.60s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 3/250 [00:31<43:42, 10.62s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 3/250 [00:42<43:42, 10.62s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 4/250 [00:42<43:09, 10.53s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 4/250 [00:52<43:09, 10.53s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 5/250 [00:52<42:41, 10.45s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 5/250 [01:02<42:41, 10.45s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 6/250 [01:02<41:54, 10.31s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 6/250 [01:12<41:54, 10.31s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 7/250 [01:12<41:27, 10.24s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 7/250 [01:22<41:27, 10.24s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 8/250 [01:22<41:00, 10.17s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 8/250 [01:33<41:00, 10.17s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 9/250 [01:33<40:50, 10.17s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 9/250 [01:43<40:50, 10.17s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 10/250 [01:43<40:32, 10.13s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 10/250 [01:52<40:32, 10.13s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 11/250 [01:52<40:05, 10.06s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 11/250 [02:03<40:05, 10.06s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 12/250 [02:03<40:03, 10.10s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 12/250 [02:13<40:03, 10.10s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 13/250 [02:13<39:36, 10.03s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 13/250 [02:23<39:36, 10.03s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 14/250 [02:23<39:36, 10.07s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 14/250 [02:33<39:36, 10.07s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 15/250 [02:33<39:14, 10.02s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 15/250 [02:43<39:14, 10.02s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 16/250 [02:43<39:10, 10.05s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 16/250 [02:53<39:10, 10.05s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 17/250 [02:53<38:49, 10.00s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 17/250 [03:03<38:49, 10.00s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 18/250 [03:03<38:39, 10.00s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 18/250 [03:13<38:39, 10.00s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 19/250 [03:13<38:26,  9.99s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 19/250 [03:23<38:26,  9.99s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 20/250 [03:23<38:23, 10.02s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 20/250 [03:32<38:23, 10.02s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 21/250 [03:32<38:00,  9.96s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 21/250 [03:42<38:00,  9.96s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 22/250 [03:42<37:48,  9.95s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 22/250 [03:52<37:48,  9.95s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 23/250 [03:52<37:41,  9.96s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 23/250 [04:02<37:41,  9.96s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 24/250 [04:02<37:33,  9.97s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 24/250 [04:12<37:33,  9.97s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 25/250 [04:12<37:32, 10.01s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 25/250 [04:22<37:32, 10.01s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 26/250 [04:22<37:16,  9.98s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 26/250 [04:33<37:16,  9.98s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 27/250 [04:33<37:21, 10.05s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 27/250 [04:43<37:21, 10.05s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 28/250 [04:43<37:07, 10.03s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 28/250 [04:53<37:07, 10.03s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 29/250 [04:53<37:09, 10.09s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 29/250 [05:03<37:09, 10.09s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 30/250 [05:03<36:43, 10.02s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 30/250 [05:13<36:43, 10.02s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 31/250 [05:13<36:35, 10.03s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 31/250 [05:23<36:35, 10.03s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 32/250 [05:23<36:23, 10.02s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 32/250 [05:33<36:23, 10.02s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 33/250 [05:33<36:03,  9.97s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 33/250 [05:43<36:03,  9.97s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 34/250 [05:43<35:53,  9.97s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 34/250 [05:52<35:53,  9.97s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 35/250 [05:52<35:33,  9.92s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 35/250 [06:02<35:33,  9.92s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 36/250 [06:02<35:34,  9.97s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 36/250 [06:12<35:34,  9.97s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 37/250 [06:12<35:28,  9.99s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 37/250 [06:23<35:28,  9.99s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 38/250 [06:23<35:40, 10.09s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 38/250 [06:33<35:40, 10.09s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 39/250 [06:33<35:20, 10.05s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 39/250 [06:43<35:20, 10.05s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 40/250 [06:43<35:23, 10.11s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 40/250 [06:53<35:23, 10.11s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 41/250 [06:53<35:04, 10.07s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 41/250 [07:03<35:04, 10.07s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 42/250 [07:03<34:58, 10.09s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 42/250 [07:13<34:58, 10.09s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 43/250 [07:13<35:00, 10.15s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 43/250 [07:24<35:00, 10.15s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 44/250 [07:24<35:27, 10.33s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 44/250 [07:35<35:27, 10.33s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 45/250 [07:35<35:42, 10.45s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 45/250 [07:45<35:42, 10.45s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 46/250 [07:45<35:06, 10.33s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 46/250 [07:55<35:06, 10.33s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 47/250 [07:55<34:43, 10.26s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 47/250 [08:05<34:43, 10.26s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 48/250 [08:05<34:32, 10.26s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 48/250 [08:16<34:32, 10.26s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 49/250 [08:16<34:46, 10.38s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 49/250 [08:26<34:46, 10.38s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 50/250 [08:26<34:19, 10.30s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 50/250 [08:37<34:19, 10.30s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 51/250 [08:37<34:33, 10.42s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 51/250 [08:47<34:33, 10.42s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 52/250 [08:47<34:20, 10.41s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 52/250 [08:57<34:20, 10.41s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 53/250 [08:57<34:08, 10.40s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 53/250 [09:07<34:08, 10.40s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 54/250 [09:07<33:25, 10.23s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 54/250 [09:18<33:25, 10.23s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 55/250 [09:18<33:26, 10.29s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 55/250 [09:28<33:26, 10.29s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 56/250 [09:28<32:56, 10.19s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 56/250 [09:38<32:56, 10.19s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 57/250 [09:38<32:47, 10.20s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 57/250 [09:48<32:47, 10.20s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 58/250 [09:48<32:57, 10.30s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 58/250 [09:59<32:57, 10.30s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 59/250 [09:59<33:20, 10.47s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 59/250 [10:10<33:20, 10.47s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 60/250 [10:10<33:24, 10.55s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 60/250 [10:20<33:24, 10.55s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 61/250 [10:20<33:03, 10.49s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 61/250 [10:31<33:03, 10.49s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▏| 62/250 [10:31<32:27, 10.36s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▏| 62/250 [10:41<32:27, 10.36s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▎| 63/250 [10:41<32:11, 10.33s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▎| 63/250 [10:51<32:11, 10.33s/epoch(s), train_multiclassaccuracyEarly stopping at epoch 63 with validation loss 0.000 and training loss 1.010\n\nTraining:  25%|▎| 63/250 [10:51<32:13, 10.34s/epoch(s), train_multiclassaccuracy\n{'MulticlassAccuracy': tensor(0.6377)}\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/TP46kpenHUD0/run_model_training.py\", line 69, in <module>\n    \"train/Accuracy\": train_m[\"Accuracy\"],\nKeyError: 'Accuracy'\n",
  "history_begin_time" : 1678130122322,
  "history_end_time" : 1678130778806,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "26ktdkvhn11",
  "history_input" : "from model_training_utils import add_hparams, EarlyStopping\nfrom get_device_config import *\nfrom loss_function import *\nfrom set_optmizer_and_scheduler import *\nfrom set_summary_writer import *\nfrom model_utils import *\nfrom torch_metrics_utils import *\nfrom tqdm.auto import tqdm\n\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 252 samples from /home/chetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 12 samples from /home/chetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\n======================================================================\nWriting Tensorboard logs to /home/chetana/tensorboard/2023-03-06_18-43\n======================================================================\n\nTraining:   0%|                                   | 0/250 [00:00<?, ?epoch(s)/s]\nTraining:   0%| | 0/250 [00:11<?, ?epoch(s)/s, train_multiclassaccuracy=0.244053\nTraining:   0%| | 1/250 [00:11<46:06, 11.11s/epoch(s), train_multiclassaccuracy=\nTraining:   0%| | 1/250 [00:21<46:06, 11.11s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 2/250 [00:21<44:18, 10.72s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 2/250 [00:32<44:18, 10.72s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 3/250 [00:32<43:54, 10.67s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 3/250 [00:42<43:54, 10.67s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 4/250 [00:42<43:13, 10.54s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 4/250 [00:52<43:13, 10.54s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 5/250 [00:52<42:49, 10.49s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 5/250 [01:03<42:49, 10.49s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 6/250 [01:03<42:13, 10.38s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 6/250 [01:13<42:13, 10.38s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 7/250 [01:13<41:55, 10.35s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 7/250 [01:23<41:55, 10.35s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 8/250 [01:23<41:28, 10.28s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 8/250 [01:33<41:28, 10.28s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 9/250 [01:33<41:31, 10.34s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 9/250 [01:44<41:31, 10.34s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 10/250 [01:44<41:49, 10.46s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 10/250 [01:54<41:49, 10.46s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 11/250 [01:54<41:10, 10.34s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 11/250 [02:05<41:10, 10.34s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 12/250 [02:05<41:03, 10.35s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 12/250 [02:15<41:03, 10.35s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 13/250 [02:15<40:38, 10.29s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 13/250 [02:25<40:38, 10.29s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 14/250 [02:25<40:23, 10.27s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 14/250 [02:35<40:23, 10.27s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 15/250 [02:35<39:50, 10.17s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 15/250 [02:45<39:50, 10.17s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 16/250 [02:45<39:41, 10.18s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 16/250 [02:55<39:41, 10.18s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 17/250 [02:55<39:21, 10.13s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 17/250 [03:05<39:21, 10.13s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 18/250 [03:05<39:01, 10.09s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 18/250 [03:15<39:01, 10.09s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 19/250 [03:15<38:29, 10.00s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 19/250 [03:25<38:29, 10.00s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 20/250 [03:25<38:21, 10.01s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 20/250 [03:35<38:21, 10.01s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 21/250 [03:35<38:12, 10.01s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 21/250 [03:45<38:12, 10.01s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 22/250 [03:45<38:03, 10.01s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 22/250 [03:55<38:03, 10.01s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 23/250 [03:55<37:54, 10.02s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 23/250 [04:05<37:54, 10.02s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 24/250 [04:05<37:34,  9.98s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 24/250 [04:15<37:34,  9.98s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 25/250 [04:15<37:23,  9.97s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 25/250 [04:25<37:23,  9.97s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 26/250 [04:25<37:18,  9.99s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 26/250 [04:35<37:18,  9.99s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 27/250 [04:35<37:20, 10.05s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 27/250 [04:45<37:20, 10.05s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 28/250 [04:45<36:59, 10.00s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 28/250 [04:55<36:59, 10.00s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 29/250 [04:55<36:48,  9.99s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 29/250 [05:05<36:48,  9.99s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 30/250 [05:05<36:37,  9.99s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 30/250 [05:15<36:37,  9.99s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 31/250 [05:15<36:31, 10.01s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 31/250 [05:25<36:31, 10.01s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 32/250 [05:25<36:19, 10.00s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 32/250 [05:35<36:19, 10.00s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 33/250 [05:35<36:05,  9.98s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 33/250 [05:45<36:05,  9.98s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 34/250 [05:45<36:06, 10.03s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 34/250 [05:55<36:06, 10.03s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 35/250 [05:55<35:54, 10.02s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 35/250 [06:05<35:54, 10.02s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 36/250 [06:05<35:59, 10.09s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 36/250 [06:15<35:59, 10.09s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 37/250 [06:15<35:38, 10.04s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 37/250 [06:25<35:38, 10.04s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 38/250 [06:25<35:41, 10.10s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 38/250 [06:35<35:41, 10.10s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 39/250 [06:35<35:24, 10.07s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 39/250 [06:46<35:24, 10.07s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 40/250 [06:46<35:24, 10.12s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 40/250 [06:56<35:24, 10.12s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 41/250 [06:56<35:06, 10.08s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 41/250 [07:06<35:06, 10.08s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 42/250 [07:06<34:59, 10.09s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 42/250 [07:16<34:59, 10.09s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 43/250 [07:16<34:41, 10.06s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 43/250 [07:26<34:41, 10.06s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 44/250 [07:26<34:39, 10.09s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 44/250 [07:36<34:39, 10.09s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 45/250 [07:36<34:29, 10.09s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 45/250 [07:46<34:29, 10.09s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 46/250 [07:46<34:14, 10.07s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 46/250 [07:56<34:14, 10.07s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 47/250 [07:56<34:06, 10.08s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 47/250 [08:06<34:06, 10.08s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 48/250 [08:06<33:49, 10.05s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 48/250 [08:16<33:49, 10.05s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 49/250 [08:16<33:40, 10.05s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 49/250 [08:26<33:40, 10.05s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 50/250 [08:26<33:14,  9.97s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 50/250 [08:36<33:14,  9.97s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 51/250 [08:36<33:16, 10.03s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 51/250 [08:46<33:16, 10.03s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 52/250 [08:46<33:06, 10.03s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 52/250 [08:56<33:06, 10.03s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 53/250 [08:56<33:01, 10.06s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 53/250 [09:06<33:01, 10.06s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 54/250 [09:06<32:37,  9.99s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 54/250 [09:16<32:37,  9.99s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 55/250 [09:16<32:43, 10.07s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 55/250 [09:26<32:43, 10.07s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 56/250 [09:26<32:24, 10.02s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 56/250 [09:36<32:24, 10.02s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 57/250 [09:36<32:16, 10.03s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 57/250 [09:46<32:16, 10.03s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 58/250 [09:46<32:04, 10.02s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 58/250 [09:57<32:04, 10.02s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 59/250 [09:57<32:03, 10.07s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 59/250 [10:07<32:03, 10.07s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 60/250 [10:07<31:56, 10.09s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 60/250 [10:17<31:56, 10.09s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 61/250 [10:17<31:37, 10.04s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 61/250 [10:27<31:37, 10.04s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▏| 62/250 [10:27<31:27, 10.04s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▏| 62/250 [10:37<31:27, 10.04s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▎| 63/250 [10:37<31:10, 10.00s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▎| 63/250 [10:47<31:10, 10.00s/epoch(s), train_multiclassaccuracyEarly stopping at epoch 63 with validation loss 0.000 and training loss 1.010\n\nTraining:  25%|▎| 63/250 [10:47<32:00, 10.27s/epoch(s), train_multiclassaccuracy\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/26ktdkvhn11/run_model_training.py\", line 68, in <module>\n    \"train/Accuracy\": train_m[\"Accuracy\"],\nKeyError: 'Accuracy'\n",
  "history_begin_time" : 1678128199100,
  "history_end_time" : 1678128852414,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "ycru82",
  "indicator" : "Failed"
},{
  "history_id" : "arnbv0uscta",
  "history_input" : "from model_training_utils import add_hparams, EarlyStopping\nfrom get_device_config import *\nfrom loss_function import *\nfrom set_optmizer_and_scheduler import *\nfrom set_summary_writer import *\nfrom model_utils import *\nfrom torch_metrics_utils import *\nfrom tqdm.auto import tqdm\n\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 252 samples from /home/chetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 12 samples from /home/chetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\n======================================================================\nWriting Tensorboard logs to /home/chetana/tensorboard/2023-03-06_18-37\n======================================================================\n\nTraining:   0%|                                   | 0/250 [00:00<?, ?epoch(s)/s]\nTraining:   0%|                                   | 0/250 [00:12<?, ?epoch(s)/s]\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/arnbv0uscta/run_model_training.py\", line 23, in <module>\n    train_loss, val_loss, train_m, val_m = run_epoch(\n  File \"/home/chetana/gw-workspace/arnbv0uscta/model_utils.py\", line 87, in run_epoch\n    num_classes, train_metrics, writer, epoch, \"train\"\nNameError: name 'num_classes' is not defined\n",
  "history_begin_time" : 1678127843574,
  "history_end_time" : 1678127862811,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "ycru82",
  "indicator" : "Failed"
},{
  "history_id" : "ckg23dvxodk",
  "history_input" : "from model_training_utils import add_hparams, EarlyStopping\nfrom get_device_config import *\nfrom loss_function import *\nfrom set_optmizer_and_scheduler import *\nfrom set_summary_writer import *\nfrom model_utils import *\nfrom torch_metrics_utils import *\nfrom tqdm.auto import tqdm\n\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 252 samples from /home/chetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 12 samples from /home/chetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/ckg23dvxodk/run_model_training.py\", line 5, in <module>\n    from set_summary_writer import *\n  File \"/home/chetana/gw-workspace/ckg23dvxodk/set_summary_writer.py\", line 14\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\n    ^\nSyntaxError: unterminated string literal (detected at line 14)\n",
  "history_begin_time" : 1678127441382,
  "history_end_time" : 1678127449404,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "ycru82",
  "indicator" : "Failed"
},{
  "history_id" : "uyghs2mcflc",
  "history_input" : "from model_training_utils import add_hparams, EarlyStopping\nfrom get_device_config import *\nfrom loss_function import *\nfrom set_optmizer_and_scheduler import *\nfrom set_summary_writer import *\nfrom model_utils import *\nfrom torch_metrics_utils import *\nfrom tqdm.auto import tqdm\n\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 252 samples from /home/chetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 12 samples from /home/chetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/uyghs2mcflc/run_model_training.py\", line 4, in <module>\n    from set_optmizer_and_scheduler import *\n  File \"/home/chetana/gw-workspace/uyghs2mcflc/set_optmizer_and_scheduler.py\", line 7, in <module>\n    optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nNameError: name 'model' is not defined\n",
  "history_begin_time" : 1678125764664,
  "history_end_time" : 1678125770764,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "ycru82",
  "indicator" : "Failed"
},{
  "history_id" : "l9quwab3rwy",
  "history_input" : "from model_training_utils import add_hparams, EarlyStopping\nfrom get_device_config import *\nfrom loss_function import *\nfrom set_optmizer_and_scheduler import *\nfrom set_summary_writer import *\nfrom model_utils import *\nfrom torch_metrics_utils import *\nfrom tqdm.auto import tqdm\n\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/l9quwab3rwy/run_model_training.py\", line 1, in <module>\n    from model_training_utils import add_hparams, EarlyStopping\n  File \"/home/chetana/gw-workspace/l9quwab3rwy/model_training_utils.py\", line 5, in <module>\n    from torch.utils.tensorboard.summary import hparams\n  File \"/home/chetana/anaconda3/envs/ranjan/lib/python3.10/site-packages/torch/utils/tensorboard/__init__.py\", line 1, in <module>\n    import tensorboard\nModuleNotFoundError: No module named 'tensorboard'\n",
  "history_begin_time" : 1677775410240,
  "history_end_time" : 1677775414369,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "ycru82",
  "indicator" : "Failed"
},{
  "history_id" : "13aod1ubjpp",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1678838010076,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "ycru82",
  "indicator" : "Stopped"
},{
  "history_id" : "pi6b594ajsi",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1678850059452,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "ycru82",
  "indicator" : "Stopped"
},{
  "history_id" : "4vxrws3jrki",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1678250276599,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "ycru82",
  "indicator" : "Stopped"
},{
  "history_id" : "slwkxs0xuj9",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1678249820052,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "ycru82",
  "indicator" : "Stopped"
},{
  "history_id" : "jk03l3eo87d",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1680666547111,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "c2lqcn",
  "indicator" : "Stopped"
},{
  "history_id" : "ubxhahmqdma",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1680666655739,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "c2lqcn",
  "indicator" : "Stopped"
},{
  "history_id" : "vva3525u0ju",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1680666772311,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "c2lqcn",
  "indicator" : "Stopped"
},{
  "history_id" : "7eb488ce1i6",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1680666801839,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "c2lqcn",
  "indicator" : "Stopped"
},{
  "history_id" : "t58b6akur9b",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1682440474433,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "c2lqcn",
  "indicator" : "Stopped"
},{
  "history_id" : "90m9b3mu21y",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1682442550969,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "c2lqcn",
  "indicator" : "Stopped"
},{
  "history_id" : "5pns3orwa3e",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1682443979653,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "c2lqcn",
  "indicator" : "Stopped"
},]
