[{
  "history_id" : "qw407x3ikex",
  "history_input" : "from training_and_plot_utils import *\nfrom model_components import *\nfrom device_config_and_data_loader import *\nfrom tqdm.auto import tqdm\n\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nprint(train_m)\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"MulticlassAccuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"MulticlassAccuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_final_full_data.pt\")\n\nprint(model_path)\n\n\nprint(\"train/Accuracy\", train_m[\"MulticlassAccuracy\"])\nprint(\"val/Accuracy\", val_m[\"MulticlassAccuracy\"])\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 24 samples from /home/chetana/ML_eddie/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-1999_lat14N-46N_lon166W-134W.npz.\nRead 12 samples from /home/chetana/ML_eddie/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\n======================================================================\nWriting Tensorboard logs to /home/chetana/tensorboard/2023-04-18_19-42\n======================================================================\n\nTraining:   0%|                                   | 0/250 [00:00<?, ?epoch(s)/s]\nTraining:   0%| | 0/250 [00:05<?, ?epoch(s)/s, train_multiclassaccuracy=0.239357\nTraining:   0%| | 1/250 [00:05<23:06,  5.57s/epoch(s), train_multiclassaccuracy=\nTraining:   0%| | 1/250 [00:10<23:06,  5.57s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 2/250 [00:10<21:48,  5.28s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 2/250 [00:15<21:48,  5.28s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 3/250 [00:15<21:41,  5.27s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 3/250 [00:20<21:41,  5.27s/epoch(s), train_multiclassaccuracy=\n",
  "history_begin_time" : 1681846916457,
  "history_end_time" : 1681846915381,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "c2lqcn",
  "indicator" : "Running"
},{
  "history_id" : "lswmqt4q1sy",
  "history_input" : "from training_and_plot_utils import *\nfrom model_components import *\nfrom device_config_and_data_loader import *\nfrom tqdm.auto import tqdm\n\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nprint(train_m)\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"MulticlassAccuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"MulticlassAccuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_final_full_data.pt\")\n\nprint(model_path)\n\n\nprint(\"train/Accuracy\", train_m[\"MulticlassAccuracy\"])\nprint(\"val/Accuracy\", val_m[\"MulticlassAccuracy\"])\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 24 samples from /home/chetana/ML_eddie/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-1999_lat14N-46N_lon166W-134W.npz.\nRead 12 samples from /home/chetana/ML_eddie/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\n======================================================================\nWriting Tensorboard logs to /home/chetana/tensorboard/2023-04-18_19-35\n======================================================================\n\nTraining:   0%|                                   | 0/250 [00:00<?, ?epoch(s)/s]\nTraining:   0%| | 0/250 [00:05<?, ?epoch(s)/s, train_multiclassaccuracy=0.239357\nTraining:   0%| | 1/250 [00:05<22:30,  5.42s/epoch(s), train_multiclassaccuracy=\nTraining:   0%| | 1/250 [00:10<22:30,  5.42s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 2/250 [00:10<21:21,  5.17s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 2/250 [00:15<21:21,  5.17s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 3/250 [00:15<21:18,  5.18s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 3/250 [00:20<21:18,  5.18s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 4/250 [00:20<21:11,  5.17s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 4/250 [00:26<21:11,  5.17s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 5/250 [00:26<21:16,  5.21s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 5/250 [00:31<21:16,  5.21s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 6/250 [00:31<21:00,  5.16s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 6/250 [00:36<21:00,  5.16s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 7/250 [00:36<20:54,  5.16s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 7/250 [00:41<20:54,  5.16s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 8/250 [00:41<20:52,  5.17s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 8/250 [00:46<20:52,  5.17s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 9/250 [00:46<20:41,  5.15s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 9/250 [00:51<20:41,  5.15s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 10/250 [00:51<20:35,  5.15s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 10/250 [00:56<20:35,  5.15s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 11/250 [00:56<20:27,  5.14s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 11/250 [01:02<20:27,  5.14s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 12/250 [01:02<20:39,  5.21s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 12/250 [01:07<20:39,  5.21s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 13/250 [01:07<20:19,  5.15s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 13/250 [01:12<20:19,  5.15s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 14/250 [01:12<20:18,  5.16s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 14/250 [01:17<20:18,  5.16s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 15/250 [01:17<20:02,  5.11s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 15/250 [01:22<20:02,  5.11s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 16/250 [01:22<19:57,  5.12s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 16/250 [01:27<19:57,  5.12s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 17/250 [01:27<19:39,  5.06s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 17/250 [01:32<19:39,  5.06s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 18/250 [01:32<19:35,  5.07s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 18/250 [01:37<19:35,  5.07s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 19/250 [01:37<19:21,  5.03s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 19/250 [01:42<19:21,  5.03s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 20/250 [01:42<18:59,  4.95s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 20/250 [01:47<18:59,  4.95s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 21/250 [01:47<18:52,  4.95s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 21/250 [01:52<18:52,  4.95s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 22/250 [01:52<18:43,  4.93s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 22/250 [01:57<18:43,  4.93s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 23/250 [01:57<18:54,  5.00s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 23/250 [02:02<18:54,  5.00s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 24/250 [02:02<18:51,  5.01s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 24/250 [02:07<18:51,  5.01s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 25/250 [02:07<18:55,  5.05s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 25/250 [02:12<18:55,  5.05s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 26/250 [02:12<18:42,  5.01s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 26/250 [02:17<18:42,  5.01s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 27/250 [02:17<18:43,  5.04s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 27/250 [02:22<18:43,  5.04s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 28/250 [02:22<18:28,  5.00s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 28/250 [02:27<18:28,  5.00s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 29/250 [02:27<18:30,  5.02s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 29/250 [02:32<18:30,  5.02s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 30/250 [02:32<18:21,  5.01s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 30/250 [02:37<18:21,  5.01s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 31/250 [02:37<17:58,  4.93s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 31/250 [02:42<17:58,  4.93s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 32/250 [02:42<17:57,  4.94s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 32/250 [02:46<17:57,  4.94s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 33/250 [02:46<17:41,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 33/250 [02:51<17:41,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 34/250 [02:51<17:40,  4.91s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 34/250 [02:56<17:40,  4.91s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 35/250 [02:56<17:33,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 35/250 [03:01<17:33,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 36/250 [03:01<17:35,  4.93s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 36/250 [03:06<17:35,  4.93s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 37/250 [03:06<17:21,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 37/250 [03:11<17:21,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 38/250 [03:11<17:19,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 38/250 [03:16<17:19,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 39/250 [03:16<17:02,  4.85s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 39/250 [03:21<17:02,  4.85s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 40/250 [03:21<17:00,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 40/250 [03:25<17:00,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 41/250 [03:25<16:57,  4.87s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 41/250 [03:30<16:57,  4.87s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 42/250 [03:30<16:48,  4.85s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 42/250 [03:35<16:48,  4.85s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 43/250 [03:35<16:50,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 43/250 [03:40<16:50,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 44/250 [03:40<16:40,  4.85s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 44/250 [03:45<16:40,  4.85s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 45/250 [03:45<16:40,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 45/250 [03:50<16:40,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 46/250 [03:50<16:25,  4.83s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 46/250 [03:55<16:25,  4.83s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 47/250 [03:55<16:36,  4.91s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 47/250 [04:00<16:36,  4.91s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 48/250 [04:00<16:29,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 48/250 [04:05<16:29,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 49/250 [04:05<16:42,  4.99s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 49/250 [04:10<16:42,  4.99s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 50/250 [04:10<16:32,  4.96s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 50/250 [04:15<16:32,  4.96s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 51/250 [04:15<16:35,  5.00s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 51/250 [04:20<16:35,  5.00s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 52/250 [04:20<16:22,  4.96s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 52/250 [04:25<16:22,  4.96s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 53/250 [04:25<16:19,  4.97s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 53/250 [04:30<16:19,  4.97s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 54/250 [04:30<16:09,  4.94s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 54/250 [04:35<16:09,  4.94s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 55/250 [04:35<16:14,  5.00s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 55/250 [04:39<16:14,  5.00s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 56/250 [04:39<15:57,  4.94s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 56/250 [04:44<15:57,  4.94s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 57/250 [04:44<15:57,  4.96s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 57/250 [04:49<15:57,  4.96s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 58/250 [04:49<15:46,  4.93s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 58/250 [04:54<15:46,  4.93s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 59/250 [04:54<15:42,  4.94s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 59/250 [04:59<15:42,  4.94s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 60/250 [04:59<15:29,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 60/250 [05:04<15:29,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 61/250 [05:04<15:33,  4.94s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 61/250 [05:09<15:33,  4.94s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▏| 62/250 [05:09<15:19,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▏| 62/250 [05:14<15:19,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▎| 63/250 [05:14<15:25,  4.95s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▎| 63/250 [05:19<15:25,  4.95s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 64/250 [05:19<15:20,  4.95s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 64/250 [05:24<15:20,  4.95s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 65/250 [05:24<15:06,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 65/250 [05:29<15:06,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 66/250 [05:29<15:08,  4.94s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 66/250 [05:34<15:08,  4.94s/epoch(s), train_multiclassaccuracyEarly stopping at epoch 66 with validation loss 0.000 and training loss 1.006\n\nTraining:  26%|▎| 66/250 [05:34<15:31,  5.06s/epoch(s), train_multiclassaccuracy\n{'MulticlassAccuracy': tensor(0.6333)}\n/home/chetana/tensorboard/2023-04-18_19-35/model_ckpt_final_full_data.pt\ntrain/Accuracy tensor(0.6333)\nval/Accuracy tensor(0.7205)\n",
  "history_begin_time" : 1681846526291,
  "history_end_time" : 1681846866444,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "c2lqcn",
  "indicator" : "Done"
},{
  "history_id" : "ez8v1xg6knb",
  "history_input" : "from training_and_plot_utils import *\nfrom model_components import *\nfrom get_device_config import *\nfrom tqdm.auto import tqdm\n\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nprint(train_m)\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"MulticlassAccuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"MulticlassAccuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_final_full_data.pt\")\n\nprint(model_path)\n\n\nprint(\"train/Accuracy\", train_m[\"MulticlassAccuracy\"])\nprint(\"val/Accuracy\", val_m[\"MulticlassAccuracy\"])\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 24 samples from /home/chetana/ML_eddie/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-1999_lat14N-46N_lon166W-134W.npz.\nRead 12 samples from /home/chetana/ML_eddie/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\n======================================================================\nWriting Tensorboard logs to /home/chetana/tensorboard/2023-04-18_19-19\n======================================================================\n\nTraining:   0%|                                   | 0/250 [00:00<?, ?epoch(s)/s]\nTraining:   0%| | 0/250 [00:05<?, ?epoch(s)/s, train_multiclassaccuracy=0.239357\nTraining:   0%| | 1/250 [00:05<21:43,  5.23s/epoch(s), train_multiclassaccuracy=\nTraining:   0%| | 1/250 [00:10<21:43,  5.23s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 2/250 [00:10<21:16,  5.15s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 2/250 [00:15<21:16,  5.15s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 3/250 [00:15<21:28,  5.22s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 3/250 [00:20<21:28,  5.22s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 4/250 [00:20<20:58,  5.12s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 4/250 [00:25<20:58,  5.12s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 5/250 [00:25<20:55,  5.12s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 5/250 [00:30<20:55,  5.12s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 6/250 [00:30<20:35,  5.06s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 6/250 [00:35<20:35,  5.06s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 7/250 [00:35<20:25,  5.04s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 7/250 [00:40<20:25,  5.04s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 8/250 [00:40<20:17,  5.03s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 8/250 [00:45<20:17,  5.03s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 9/250 [00:45<19:51,  4.95s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 9/250 [00:50<19:51,  4.95s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 10/250 [00:50<19:46,  4.95s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 10/250 [00:55<19:46,  4.95s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 11/250 [00:55<19:29,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 11/250 [01:00<19:29,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 12/250 [01:00<19:28,  4.91s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 12/250 [01:04<19:28,  4.91s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 13/250 [01:04<19:11,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 13/250 [01:09<19:11,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 14/250 [01:09<19:15,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 14/250 [01:14<19:15,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 15/250 [01:14<18:59,  4.85s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 15/250 [01:19<18:59,  4.85s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 16/250 [01:19<18:59,  4.87s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 16/250 [01:24<18:59,  4.87s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 17/250 [01:24<18:44,  4.83s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 17/250 [01:29<18:44,  4.83s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 18/250 [01:29<18:46,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 18/250 [01:34<18:46,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 19/250 [01:34<18:46,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 19/250 [01:38<18:46,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 20/250 [01:38<18:31,  4.83s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 20/250 [01:43<18:31,  4.83s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 21/250 [01:43<18:39,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 21/250 [01:48<18:39,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 22/250 [01:48<18:26,  4.85s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 22/250 [01:53<18:26,  4.85s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 23/250 [01:53<18:31,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 23/250 [01:58<18:31,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 24/250 [01:58<18:20,  4.87s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 24/250 [02:03<18:20,  4.87s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 25/250 [02:03<18:19,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 25/250 [02:08<18:19,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 26/250 [02:08<18:15,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 26/250 [02:13<18:15,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 27/250 [02:13<18:24,  4.95s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 27/250 [02:18<18:24,  4.95s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 28/250 [02:18<18:07,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 28/250 [02:22<18:07,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 29/250 [02:22<18:03,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 29/250 [02:27<18:03,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 30/250 [02:27<18:00,  4.91s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 30/250 [02:32<18:00,  4.91s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 31/250 [02:32<17:48,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 31/250 [02:37<17:48,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 32/250 [02:37<17:52,  4.92s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 32/250 [02:42<17:52,  4.92s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 33/250 [02:42<17:40,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 33/250 [02:47<17:40,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 34/250 [02:47<17:43,  4.93s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 34/250 [02:52<17:43,  4.93s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 35/250 [02:52<17:31,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 35/250 [02:57<17:31,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 36/250 [02:57<17:27,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 36/250 [03:02<17:27,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 37/250 [03:02<17:12,  4.85s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 37/250 [03:06<17:12,  4.85s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 38/250 [03:06<17:14,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 38/250 [03:11<17:14,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 39/250 [03:11<17:02,  4.85s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 39/250 [03:16<17:02,  4.85s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 40/250 [03:16<17:02,  4.87s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 40/250 [03:21<17:02,  4.87s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 41/250 [03:21<17:06,  4.91s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 41/250 [03:26<17:06,  4.91s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 42/250 [03:26<17:13,  4.97s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 42/250 [03:31<17:13,  4.97s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 43/250 [03:31<17:16,  5.01s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 43/250 [03:36<17:16,  5.01s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 44/250 [03:36<17:11,  5.01s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 44/250 [03:41<17:11,  5.01s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 45/250 [03:41<17:12,  5.04s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 45/250 [03:46<17:12,  5.04s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 46/250 [03:46<17:01,  5.01s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 46/250 [03:51<17:01,  5.01s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 47/250 [03:51<16:57,  5.01s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 47/250 [03:56<16:57,  5.01s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 48/250 [03:56<16:36,  4.94s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 48/250 [04:01<16:36,  4.94s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 49/250 [04:01<16:40,  4.98s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 49/250 [04:06<16:40,  4.98s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 50/250 [04:06<16:26,  4.93s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 50/250 [04:11<16:26,  4.93s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 51/250 [04:11<16:25,  4.95s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 51/250 [04:16<16:25,  4.95s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 52/250 [04:16<16:10,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 52/250 [04:21<16:10,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 53/250 [04:21<16:09,  4.92s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 53/250 [04:26<16:09,  4.92s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 54/250 [04:26<16:07,  4.93s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 54/250 [04:31<16:07,  4.93s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 55/250 [04:31<16:04,  4.94s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 55/250 [04:36<16:04,  4.94s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 56/250 [04:36<15:58,  4.94s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 56/250 [04:41<15:58,  4.94s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 57/250 [04:41<15:53,  4.94s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 57/250 [04:45<15:53,  4.94s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 58/250 [04:45<15:38,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 58/250 [04:50<15:38,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 59/250 [04:50<15:35,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 59/250 [04:55<15:35,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 60/250 [04:55<15:34,  4.92s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 60/250 [05:00<15:34,  4.92s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 61/250 [05:00<15:31,  4.93s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 61/250 [05:05<15:31,  4.93s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▏| 62/250 [05:05<15:17,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▏| 62/250 [05:10<15:17,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▎| 63/250 [05:10<15:14,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▎| 63/250 [05:15<15:14,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 64/250 [05:15<15:11,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 64/250 [05:20<15:11,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 65/250 [05:20<14:58,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 65/250 [05:25<14:58,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 66/250 [05:25<15:01,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 66/250 [05:29<15:01,  4.90s/epoch(s), train_multiclassaccuracyEarly stopping at epoch 66 with validation loss 0.000 and training loss 1.006\n\nTraining:  26%|▎| 66/250 [05:29<15:19,  5.00s/epoch(s), train_multiclassaccuracy\n{'MulticlassAccuracy': tensor(0.6333)}\n/home/chetana/tensorboard/2023-04-18_19-19/model_ckpt_final_full_data.pt\ntrain/Accuracy tensor(0.6333)\nval/Accuracy tensor(0.7205)\n",
  "history_begin_time" : 1681845578728,
  "history_end_time" : 1681845914431,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "c2lqcn",
  "indicator" : "Done"
},{
  "history_id" : "w8bh8zgrsat",
  "history_input" : "from training_and_plot_utils import *\nfrom model_components import *\nfrom get_device_config import *\nfrom tqdm.auto import tqdm\n\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nprint(train_m)\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"MulticlassAccuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"MulticlassAccuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_final_full_data.pt\")\n\nprint(model_path)\n\n\nprint(\"train/Accuracy\", train_m[\"MulticlassAccuracy\"])\nprint(\"val/Accuracy\", val_m[\"MulticlassAccuracy\"])\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 24 samples from /home/chetana/ML_eddie/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-1999_lat14N-46N_lon166W-134W.npz.\nRead 12 samples from /home/chetana/ML_eddie/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\n======================================================================\nWriting Tensorboard logs to /home/chetana/tensorboard/2023-04-18_18-26\n======================================================================\n\nTraining:   0%|                                   | 0/250 [00:00<?, ?epoch(s)/s]\nTraining:   0%| | 0/250 [00:06<?, ?epoch(s)/s, train_multiclassaccuracy=0.239357\nTraining:   0%| | 1/250 [00:06<28:33,  6.88s/epoch(s), train_multiclassaccuracy=\nTraining:   0%| | 1/250 [00:11<28:33,  6.88s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 2/250 [00:11<23:49,  5.76s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 2/250 [00:16<23:49,  5.76s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 3/250 [00:16<22:28,  5.46s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 3/250 [00:21<22:28,  5.46s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 4/250 [00:21<21:39,  5.28s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 4/250 [00:27<21:39,  5.28s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 5/250 [00:27<21:43,  5.32s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 5/250 [00:32<21:43,  5.32s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 6/250 [00:32<21:01,  5.17s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 6/250 [00:37<21:01,  5.17s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 7/250 [00:37<20:55,  5.17s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 7/250 [00:42<20:55,  5.17s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 8/250 [00:42<20:37,  5.11s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 8/250 [00:47<20:37,  5.11s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 9/250 [00:47<20:18,  5.06s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 9/250 [00:52<20:18,  5.06s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 10/250 [00:52<20:17,  5.07s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 10/250 [00:57<20:17,  5.07s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 11/250 [00:57<20:02,  5.03s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 11/250 [01:02<20:02,  5.03s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 12/250 [01:02<20:02,  5.05s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 12/250 [01:07<20:02,  5.05s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 13/250 [01:07<19:42,  4.99s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 13/250 [01:12<19:42,  4.99s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 14/250 [01:12<19:46,  5.03s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 14/250 [01:17<19:46,  5.03s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 15/250 [01:17<19:28,  4.97s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 15/250 [01:22<19:28,  4.97s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 16/250 [01:22<19:48,  5.08s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 16/250 [01:27<19:48,  5.08s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 17/250 [01:27<19:30,  5.02s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 17/250 [01:32<19:30,  5.02s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 18/250 [01:32<19:37,  5.08s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 18/250 [01:37<19:37,  5.08s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 19/250 [01:37<19:27,  5.05s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 19/250 [01:42<19:27,  5.05s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 20/250 [01:42<19:02,  4.97s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 20/250 [01:47<19:02,  4.97s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 21/250 [01:47<18:57,  4.97s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 21/250 [01:52<18:57,  4.97s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 22/250 [01:52<18:42,  4.92s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 22/250 [01:57<18:42,  4.92s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 23/250 [01:57<18:43,  4.95s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 23/250 [02:02<18:43,  4.95s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 24/250 [02:02<18:26,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 24/250 [02:07<18:26,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 25/250 [02:07<18:35,  4.96s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 25/250 [02:11<18:35,  4.96s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 26/250 [02:11<18:19,  4.91s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 26/250 [02:16<18:19,  4.91s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 27/250 [02:16<18:16,  4.91s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 27/250 [02:21<18:16,  4.91s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 28/250 [02:21<18:01,  4.87s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 28/250 [02:26<18:01,  4.87s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 29/250 [02:26<17:58,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 29/250 [02:31<17:58,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 30/250 [02:31<17:55,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 30/250 [02:36<17:55,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 31/250 [02:36<17:44,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 31/250 [02:41<17:44,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 32/250 [02:41<17:50,  4.91s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 32/250 [02:46<17:50,  4.91s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 33/250 [02:46<17:34,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 33/250 [02:50<17:34,  4.86s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 34/250 [02:50<17:37,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 34/250 [02:55<17:37,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 35/250 [02:55<17:28,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 35/250 [03:00<17:28,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 36/250 [03:00<17:31,  4.91s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 36/250 [03:05<17:31,  4.91s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 37/250 [03:05<17:18,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 37/250 [03:10<17:18,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 38/250 [03:10<17:17,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 38/250 [03:15<17:17,  4.90s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 39/250 [03:15<17:03,  4.85s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 39/250 [03:20<17:03,  4.85s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 40/250 [03:20<17:02,  4.87s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 40/250 [03:25<17:02,  4.87s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 41/250 [03:25<16:59,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 41/250 [03:29<16:59,  4.88s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 42/250 [03:29<16:46,  4.84s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 42/250 [03:34<16:46,  4.84s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 43/250 [03:34<16:49,  4.87s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 43/250 [03:39<16:49,  4.87s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 44/250 [03:39<16:39,  4.85s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 44/250 [03:46<16:39,  4.85s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 45/250 [03:46<18:31,  5.42s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 45/250 [03:56<18:31,  5.42s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 46/250 [03:56<23:25,  6.89s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 46/250 [04:06<23:25,  6.89s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 47/250 [04:06<25:48,  7.63s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 47/250 [04:17<25:48,  7.63s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 48/250 [04:17<29:20,  8.71s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 48/250 [04:25<29:20,  8.71s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 49/250 [04:25<29:07,  8.69s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 49/250 [04:35<29:07,  8.69s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 50/250 [04:35<30:05,  9.03s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 50/250 [04:44<30:05,  9.03s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 51/250 [04:44<30:04,  9.07s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 51/250 [04:52<30:04,  9.07s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 52/250 [04:52<28:20,  8.59s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 52/250 [04:57<28:20,  8.59s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 53/250 [04:57<24:35,  7.49s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 53/250 [05:02<24:35,  7.49s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 54/250 [05:02<21:46,  6.67s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 54/250 [05:07<21:46,  6.67s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 55/250 [05:07<20:04,  6.18s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 55/250 [05:11<20:04,  6.18s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 56/250 [05:11<18:35,  5.75s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 56/250 [05:16<18:35,  5.75s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 57/250 [05:16<17:42,  5.50s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 57/250 [05:21<17:42,  5.50s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 58/250 [05:21<16:52,  5.27s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 58/250 [05:26<16:52,  5.27s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 59/250 [05:26<16:26,  5.16s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 59/250 [05:31<16:26,  5.16s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 60/250 [05:31<15:57,  5.04s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 60/250 [05:36<15:57,  5.04s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 61/250 [05:36<15:46,  5.01s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 61/250 [05:40<15:46,  5.01s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▏| 62/250 [05:40<15:27,  4.93s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▏| 62/250 [05:45<15:27,  4.93s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▎| 63/250 [05:45<15:23,  4.94s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▎| 63/250 [05:50<15:23,  4.94s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 64/250 [05:50<15:16,  4.93s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 64/250 [05:55<15:16,  4.93s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 65/250 [05:55<15:04,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 65/250 [06:00<15:04,  4.89s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 66/250 [06:00<15:09,  4.94s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 66/250 [06:05<15:09,  4.94s/epoch(s), train_multiclassaccuracyEarly stopping at epoch 66 with validation loss 0.000 and training loss 1.006\n\nTraining:  26%|▎| 66/250 [06:05<16:58,  5.54s/epoch(s), train_multiclassaccuracy\n{'MulticlassAccuracy': tensor(0.6333)}\n/home/chetana/tensorboard/2023-04-18_18-26/model_ckpt_final_full_data.pt\ntrain/Accuracy tensor(0.6333)\nval/Accuracy tensor(0.7205)\n",
  "history_begin_time" : 1681842378314,
  "history_end_time" : 1681842749655,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "c2lqcn",
  "indicator" : "Done"
},{
  "history_id" : "wibzhsdhv9z",
  "history_input" : "from training_and_plot_utils import *\nfrom model_components import *\nfrom get_device_config import *\nfrom tqdm.auto import tqdm\n\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nprint(train_m)\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"MulticlassAccuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"MulticlassAccuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_final_full_data.pt\")\n\nprint(model_path)\n\n\nprint(\"train/Accuracy\", train_m[\"MulticlassAccuracy\"])\nprint(\"val/Accuracy\", val_m[\"MulticlassAccuracy\"])\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 24 samples from /home/chetana/ML_eddie/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-1999_lat14N-46N_lon166W-134W.npz.\nRead 12 samples from /home/chetana/ML_eddie/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/wibzhsdhv9z/run_model_training.py\", line 2, in <module>\n    from model_components import *\n  File \"/home/chetana/gw-workspace/wibzhsdhv9z/model_components.py\", line 97, in <module>\n    f\"{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M')}\",\nAttributeError: type object 'datetime.datetime' has no attribute 'datetime'\n",
  "history_begin_time" : 1681841830583,
  "history_end_time" : 1681841836236,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "c2lqcn",
  "indicator" : "Failed"
},{
  "history_id" : "pa9t0439zic",
  "history_input" : "from training_and_plot_utils import *\nfrom model_components import *\nfrom get_device_config import *\nfrom tqdm.auto import tqdm\n\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nprint(train_m)\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"MulticlassAccuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"MulticlassAccuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_final_full_data.pt\")\n\nprint(model_path)\n\n\nprint(\"train/Accuracy\": train_m[\"MulticlassAccuracy\"])\nprint(\"val/Accuracy\": val_m[\"MulticlassAccuracy\"])\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "  File \"/home/chetana/gw-workspace/pa9t0439zic/run_model_training.py\", line 78\n    print(\"train/Accuracy\": train_m[\"MulticlassAccuracy\"])\n                          ^\nSyntaxError: invalid syntax\n",
  "history_begin_time" : 1681841574852,
  "history_end_time" : 1681841576354,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "c2lqcn",
  "indicator" : "Done"
},{
  "history_id" : "m05zfgvsz1p",
  "history_input" : "from training_and_plot_utils import *\nfrom model_components import *\nfrom get_device_config import *\nfrom tqdm.auto import tqdm\n\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nprint(train_m)\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"MulticlassAccuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"MulticlassAccuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_final_full_data.pt\")\n\nprint(model_path)\n\n\nprint(\"train/Accuracy\": train_m[\"MulticlassAccuracy\"])\nprint(\"val/Accuracy\": val_m[\"MulticlassAccuracy\"])\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "  File \"/home/chetana/gw-workspace/m05zfgvsz1p/run_model_training.py\", line 78\n    print(\"train/Accuracy\": train_m[\"MulticlassAccuracy\"])\n                          ^\n",
  "history_begin_time" : 1681841183930,
  "history_end_time" : 1681841186148,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "c2lqcn",
  "indicator" : "Failed"
},{
  "history_id" : "uto2dhd5r5l",
  "history_input" : "from model_training_utils import add_hparams, EarlyStopping\nfrom get_device_config import *\nfrom loss_function import *\nfrom set_optmizer_and_scheduler import *\nfrom set_summary_writer import *\nfrom model_utils import *\nfrom torch_metrics_utils import *\nfrom tqdm.auto import tqdm\n\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nprint(train_m)\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"MulticlassAccuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"MulticlassAccuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, \"model_ckpt_final.pt\")\nprint(model_path)\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 21 samples from /home/chetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 12 samples from /home/chetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\n======================================================================\nWriting Tensorboard logs to /home/chetana/tensorboard/2023-04-05_04-05\n======================================================================\n\nTraining:   0%|                                   | 0/250 [00:00<?, ?epoch(s)/s]\nTraining:   0%| | 0/250 [00:05<?, ?epoch(s)/s, train_multiclassaccuracy=0.237458\nTraining:   0%| | 1/250 [00:05<22:39,  5.46s/epoch(s), train_multiclassaccuracy=\nTraining:   0%| | 1/250 [00:10<22:39,  5.46s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 2/250 [00:10<20:54,  5.06s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 2/250 [00:15<20:54,  5.06s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 3/250 [00:15<20:39,  5.02s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 3/250 [00:19<20:39,  5.02s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 4/250 [00:19<20:07,  4.91s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 4/250 [00:24<20:07,  4.91s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 5/250 [00:24<20:05,  4.92s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 5/250 [00:29<20:05,  4.92s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 6/250 [00:29<19:44,  4.85s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 6/250 [00:34<19:44,  4.85s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 7/250 [00:34<19:43,  4.87s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 7/250 [00:39<19:43,  4.87s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 8/250 [00:39<19:35,  4.86s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 8/250 [00:44<19:35,  4.86s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 9/250 [00:44<19:18,  4.81s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 9/250 [00:48<19:18,  4.81s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 10/250 [00:48<19:20,  4.84s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 10/250 [00:53<19:20,  4.84s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 11/250 [00:53<19:09,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 11/250 [00:58<19:09,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 12/250 [00:58<19:08,  4.83s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 12/250 [01:03<19:08,  4.83s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 13/250 [01:03<18:54,  4.79s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 13/250 [01:08<18:54,  4.79s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 14/250 [01:08<18:54,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 14/250 [01:12<18:54,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 15/250 [01:12<18:39,  4.77s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 15/250 [01:17<18:39,  4.77s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 16/250 [01:17<18:39,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 16/250 [01:22<18:39,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 17/250 [01:22<18:23,  4.74s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 17/250 [01:27<18:23,  4.74s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 18/250 [01:27<18:31,  4.79s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 18/250 [01:31<18:31,  4.79s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 19/250 [01:31<18:19,  4.76s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 19/250 [01:36<18:19,  4.76s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 20/250 [01:36<18:19,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 20/250 [01:41<18:19,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 21/250 [01:41<18:15,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 21/250 [01:46<18:15,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 22/250 [01:46<18:02,  4.75s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 22/250 [01:50<18:02,  4.75s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 23/250 [01:50<18:01,  4.76s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 23/250 [01:55<18:01,  4.76s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 24/250 [01:55<17:47,  4.72s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 24/250 [02:00<17:47,  4.72s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 25/250 [02:00<17:48,  4.75s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 25/250 [02:05<17:48,  4.75s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 26/250 [02:05<17:37,  4.72s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 26/250 [02:09<17:37,  4.72s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 27/250 [02:09<17:42,  4.77s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 27/250 [02:14<17:42,  4.77s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 28/250 [02:14<17:31,  4.73s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 28/250 [02:19<17:31,  4.73s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 29/250 [02:19<17:31,  4.76s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 29/250 [02:24<17:31,  4.76s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 30/250 [02:24<17:21,  4.73s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 30/250 [02:28<17:21,  4.73s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 31/250 [02:28<17:25,  4.77s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 31/250 [02:33<17:25,  4.77s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 32/250 [02:33<17:25,  4.79s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 32/250 [02:38<17:25,  4.79s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 33/250 [02:38<17:11,  4.75s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 33/250 [02:43<17:11,  4.75s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 34/250 [02:43<17:12,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 34/250 [02:47<17:12,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 35/250 [02:47<16:59,  4.74s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 35/250 [02:52<16:59,  4.74s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 36/250 [02:52<17:02,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 36/250 [02:57<17:02,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 37/250 [02:57<16:52,  4.75s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 37/250 [03:02<16:52,  4.75s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 38/250 [03:02<16:54,  4.79s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 38/250 [03:07<16:54,  4.79s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 39/250 [03:07<16:48,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 39/250 [03:11<16:48,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 40/250 [03:11<16:47,  4.80s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 40/250 [03:16<16:47,  4.80s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 41/250 [03:16<16:37,  4.77s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 41/250 [03:21<16:37,  4.77s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 42/250 [03:21<16:34,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 42/250 [03:26<16:34,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 43/250 [03:26<16:19,  4.73s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 43/250 [03:30<16:19,  4.73s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 44/250 [03:30<16:17,  4.75s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 44/250 [03:35<16:17,  4.75s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 45/250 [03:35<16:05,  4.71s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 45/250 [03:40<16:05,  4.71s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 46/250 [03:40<16:04,  4.73s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 46/250 [03:44<16:04,  4.73s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 47/250 [03:44<15:54,  4.70s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 47/250 [03:49<15:54,  4.70s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 48/250 [03:49<15:57,  4.74s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 48/250 [03:54<15:57,  4.74s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 49/250 [03:54<16:05,  4.80s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 49/250 [03:59<16:05,  4.80s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 50/250 [03:59<15:55,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 50/250 [04:04<15:55,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 51/250 [04:04<16:00,  4.82s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 51/250 [04:08<16:00,  4.82s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 52/250 [04:08<15:46,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 52/250 [04:13<15:46,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 53/250 [04:13<15:43,  4.79s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 53/250 [04:18<15:43,  4.79s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 54/250 [04:18<15:30,  4.75s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 54/250 [04:23<15:30,  4.75s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 55/250 [04:23<15:31,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 55/250 [04:28<15:31,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 56/250 [04:28<15:26,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 56/250 [04:33<15:26,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 57/250 [04:33<15:33,  4.84s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 57/250 [04:37<15:33,  4.84s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 58/250 [04:37<15:21,  4.80s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 58/250 [04:42<15:21,  4.80s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 59/250 [04:42<15:18,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 59/250 [04:47<15:18,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 60/250 [04:47<15:09,  4.79s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 60/250 [04:52<15:09,  4.79s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 61/250 [04:52<15:08,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 61/250 [04:56<15:08,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▏| 62/250 [04:56<14:59,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▏| 62/250 [05:01<14:59,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▎| 63/250 [05:01<14:59,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▎| 63/250 [05:06<14:59,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 64/250 [05:06<14:47,  4.77s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 64/250 [05:11<14:47,  4.77s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 65/250 [05:11<14:48,  4.80s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 65/250 [05:16<14:48,  4.80s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 66/250 [05:16<14:38,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 66/250 [05:20<14:38,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  27%|▎| 67/250 [05:20<14:40,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:  27%|▎| 67/250 [05:25<14:40,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:  27%|▎| 68/250 [05:25<14:37,  4.82s/epoch(s), train_multiclassaccuracy\nTraining:  27%|▎| 68/250 [05:30<14:37,  4.82s/epoch(s), train_multiclassaccuracyEarly stopping at epoch 68 with validation loss 0.000 and training loss 1.006\n\nTraining:  27%|▎| 68/250 [05:30<14:44,  4.86s/epoch(s), train_multiclassaccuracy\n{'MulticlassAccuracy': tensor(0.6485)}\n/home/chetana/tensorboard/2023-04-05_04-05/model_ckpt_final.pt\n",
  "history_begin_time" : 1680667507343,
  "history_end_time" : 1680667843468,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "c2lqcn",
  "indicator" : "Done"
},{
  "history_id" : "e4q0qc4iegb",
  "history_input" : "from model_training_utils import add_hparams, EarlyStopping\nfrom get_device_config import *\nfrom loss_function import *\nfrom set_optmizer_and_scheduler import *\nfrom set_summary_writer import *\nfrom model_utils import *\nfrom torch_metrics_utils import *\nfrom tqdm.auto import tqdm\n\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nprint(train_m)\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"MulticlassAccuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"MulticlassAccuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, \"model_ckpt_final.pt\")\nprint(model_path)\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 21 samples from /home/chetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 1 samples from /home/chetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\n======================================================================\nWriting Tensorboard logs to /home/chetana/tensorboard/2023-04-05_03-55\n======================================================================\n\nTraining:   0%|                                   | 0/250 [00:00<?, ?epoch(s)/s]\nTraining:   0%|                                   | 0/250 [00:01<?, ?epoch(s)/s]\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/e4q0qc4iegb/run_model_training.py\", line 23, in <module>\n    train_loss, val_loss, train_m, val_m = run_epoch(\n  File \"/home/chetana/gw-workspace/e4q0qc4iegb/model_utils.py\", line 72, in run_epoch\n    indices_ = np.random.choice(\n  File \"mtrand.pyx\", line 965, in numpy.random.mtrand.RandomState.choice\nValueError: Cannot take a larger sample than population when 'replace=False'\n",
  "history_begin_time" : 1680666905847,
  "history_end_time" : 1680666912952,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "c2lqcn",
  "indicator" : "Failed"
},{
  "history_id" : "4l345jmkk9c",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1680032244939,
  "history_end_time" : 1680032244939,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "c2lqcn",
  "indicator" : "Skipped"
},{
  "history_id" : "613htwg0qek",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1680029557968,
  "history_end_time" : 1680029661150,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "c2lqcn",
  "indicator" : "Stopped"
},{
  "history_id" : "ezwirlljpcv",
  "history_input" : "from model_training_utils import add_hparams, EarlyStopping\nfrom get_device_config import *\nfrom loss_function import *\nfrom set_optmizer_and_scheduler import *\nfrom set_summary_writer import *\nfrom model_utils import *\nfrom torch_metrics_utils import *\nfrom tqdm.auto import tqdm\n\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nprint(train_m)\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"MulticlassAccuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"MulticlassAccuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\nprint(model_path)\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 21 samples from /home/chetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 12 samples from /home/chetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\n======================================================================\nWriting Tensorboard logs to /home/chetana/tensorboard/2023-03-15_03-26\n======================================================================\n\nTraining:   0%|                                   | 0/250 [00:00<?, ?epoch(s)/s]\nTraining:   0%| | 0/250 [00:06<?, ?epoch(s)/s, train_multiclassaccuracy=0.23686,\nTraining:   0%| | 1/250 [00:06<28:00,  6.75s/epoch(s), train_multiclassaccuracy=\nTraining:   0%| | 1/250 [00:12<28:00,  6.75s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 2/250 [00:12<24:18,  5.88s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 2/250 [00:17<24:18,  5.88s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 3/250 [00:17<23:42,  5.76s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 3/250 [00:23<23:42,  5.76s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 4/250 [00:23<23:07,  5.64s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 4/250 [00:28<23:07,  5.64s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 5/250 [00:28<22:58,  5.63s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 5/250 [00:33<22:58,  5.63s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 6/250 [00:33<22:22,  5.50s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 6/250 [00:39<22:22,  5.50s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 7/250 [00:39<22:15,  5.50s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 7/250 [00:44<22:15,  5.50s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 8/250 [00:44<22:05,  5.48s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 8/250 [00:50<22:05,  5.48s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 9/250 [00:50<21:56,  5.46s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 9/250 [00:55<21:56,  5.46s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 10/250 [00:55<21:44,  5.43s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 10/250 [01:00<21:44,  5.43s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 11/250 [01:00<21:25,  5.38s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 11/250 [01:06<21:25,  5.38s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 12/250 [01:06<21:30,  5.42s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 12/250 [01:11<21:30,  5.42s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 13/250 [01:11<21:10,  5.36s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 13/250 [01:17<21:10,  5.36s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 14/250 [01:17<21:09,  5.38s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 14/250 [01:22<21:09,  5.38s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 15/250 [01:22<20:54,  5.34s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 15/250 [01:27<20:54,  5.34s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 16/250 [01:27<20:59,  5.38s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 16/250 [01:33<20:59,  5.38s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 17/250 [01:33<20:41,  5.33s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 17/250 [01:38<20:41,  5.33s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 18/250 [01:38<20:46,  5.37s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 18/250 [01:43<20:46,  5.37s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 19/250 [01:43<20:30,  5.33s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 19/250 [01:49<20:30,  5.33s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 20/250 [01:49<20:34,  5.37s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 20/250 [01:54<20:34,  5.37s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 21/250 [01:54<20:38,  5.41s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 21/250 [01:59<20:38,  5.41s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 22/250 [01:59<20:25,  5.37s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 22/250 [02:05<20:25,  5.37s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 23/250 [02:05<20:28,  5.41s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 23/250 [02:10<20:28,  5.41s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 24/250 [02:10<20:06,  5.34s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 24/250 [02:16<20:06,  5.34s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 25/250 [02:16<20:03,  5.35s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 25/250 [02:21<20:03,  5.35s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 26/250 [02:21<19:47,  5.30s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 26/250 [02:26<19:47,  5.30s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 27/250 [02:26<19:55,  5.36s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 27/250 [02:31<19:55,  5.36s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 28/250 [02:31<19:42,  5.33s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 28/250 [02:37<19:42,  5.33s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 29/250 [02:37<19:43,  5.35s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 29/250 [02:42<19:43,  5.35s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 30/250 [02:42<19:33,  5.33s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 30/250 [02:48<19:33,  5.33s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 31/250 [02:48<19:36,  5.37s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 31/250 [02:53<19:36,  5.37s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 32/250 [02:53<19:29,  5.36s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 32/250 [02:58<19:29,  5.36s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 33/250 [02:58<19:12,  5.31s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 33/250 [03:04<19:12,  5.31s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 34/250 [03:04<19:11,  5.33s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 34/250 [03:09<19:11,  5.33s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 35/250 [03:09<19:00,  5.30s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 35/250 [03:14<19:00,  5.30s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 36/250 [03:14<19:02,  5.34s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 36/250 [03:19<19:02,  5.34s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 37/250 [03:19<18:49,  5.30s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 37/250 [03:25<18:49,  5.30s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 38/250 [03:25<18:55,  5.36s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 38/250 [03:30<18:55,  5.36s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 39/250 [03:30<18:42,  5.32s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 39/250 [03:36<18:42,  5.32s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 40/250 [03:36<18:44,  5.35s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 40/250 [03:41<18:44,  5.35s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 41/250 [03:41<18:31,  5.32s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 41/250 [03:47<18:31,  5.32s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 42/250 [03:47<19:31,  5.63s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 42/250 [04:00<19:31,  5.63s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 43/250 [04:00<27:14,  7.90s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 43/250 [04:11<27:14,  7.90s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 44/250 [04:11<30:25,  8.86s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 44/250 [04:21<30:25,  8.86s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 45/250 [04:21<31:27,  9.21s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 45/250 [04:35<31:27,  9.21s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 46/250 [04:35<35:27, 10.43s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 46/250 [04:47<35:27, 10.43s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 47/250 [04:47<37:11, 10.99s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 47/250 [05:03<37:11, 10.99s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 48/250 [05:04<42:30, 12.63s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 48/250 [05:17<42:30, 12.63s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 49/250 [05:17<42:45, 12.77s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 49/250 [05:29<42:45, 12.77s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 50/250 [05:29<42:23, 12.72s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 50/250 [05:42<42:23, 12.72s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 51/250 [05:42<42:07, 12.70s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 51/250 [05:52<42:07, 12.70s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 52/250 [05:52<39:43, 12.04s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 52/250 [05:58<39:43, 12.04s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 53/250 [05:58<33:06, 10.08s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 53/250 [06:03<33:06, 10.08s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 54/250 [06:03<28:28,  8.72s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 54/250 [06:09<28:28,  8.72s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 55/250 [06:09<25:22,  7.81s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 55/250 [06:14<25:22,  7.81s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 56/250 [06:14<22:54,  7.09s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 56/250 [06:20<22:54,  7.09s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 57/250 [06:20<21:22,  6.64s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 57/250 [06:26<21:22,  6.64s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 58/250 [06:26<20:07,  6.29s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 58/250 [06:31<20:07,  6.29s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 59/250 [06:31<19:24,  6.10s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 59/250 [06:37<19:24,  6.10s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 60/250 [06:37<18:37,  5.88s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 60/250 [06:42<18:37,  5.88s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 61/250 [06:42<18:26,  5.86s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 61/250 [06:48<18:26,  5.86s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▏| 62/250 [06:48<18:01,  5.75s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▏| 62/250 [06:54<18:01,  5.75s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▎| 63/250 [06:54<17:54,  5.74s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▎| 63/250 [06:59<17:54,  5.74s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 64/250 [06:59<17:34,  5.67s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 64/250 [07:05<17:34,  5.67s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 65/250 [07:05<17:27,  5.66s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 65/250 [07:10<17:27,  5.66s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 66/250 [07:10<17:06,  5.58s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 66/250 [07:16<17:06,  5.58s/epoch(s), train_multiclassaccuracy\nTraining:  27%|▎| 67/250 [07:16<16:59,  5.57s/epoch(s), train_multiclassaccuracy\nTraining:  27%|▎| 67/250 [07:21<16:59,  5.57s/epoch(s), train_multiclassaccuracyEarly stopping at epoch 67 with validation loss 0.000 and training loss 1.010\n\nTraining:  27%|▎| 67/250 [07:21<20:06,  6.59s/epoch(s), train_multiclassaccuracy\n{'MulticlassAccuracy': tensor(0.6411)}\n/home/chetana/tensorboard/2023-03-15_03-26/model_ckpt_68.pt\n",
  "history_begin_time" : 1678850774929,
  "history_end_time" : 1678851222622,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "ycru82",
  "indicator" : "Done"
},{
  "history_id" : "eirz1k089jx",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678837413885,
  "history_end_time" : 1678837413885,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "ycru82",
  "indicator" : "Skipped"
},{
  "history_id" : "c0argvkmbzh",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678837361474,
  "history_end_time" : 1678837361474,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "ycru82",
  "indicator" : "Skipped"
},{
  "history_id" : "0xf6rpyr59t",
  "history_input" : null,
  "history_output" : "Exhausted available authentication methods",
  "history_begin_time" : 1678250140126,
  "history_end_time" : 1678250142781,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "ycru82",
  "indicator" : "Failed"
},{
  "history_id" : "i9LoOyVLxGxi",
  "history_input" : "from model_training_utils import add_hparams, EarlyStopping\nfrom get_device_config import *\nfrom loss_function import *\nfrom set_optmizer_and_scheduler import *\nfrom set_summary_writer import *\nfrom model_utils import *\nfrom torch_metrics_utils import *\nfrom tqdm.auto import tqdm\n\n\"\"\"\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nprint(train_m)\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"MulticlassAccuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"MulticlassAccuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\"\"\"\n\n# save model to tensorboard folder\nN = 2\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\nprint(model_path)\n#torch.save(model.state_dict(), model_path)",
  "history_output" : "Read 252 samples from /home/chetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 12 samples from /home/chetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\n======================================================================\nWriting Tensorboard logs to /home/chetana/tensorboard/2023-03-07_18-13\n======================================================================\n/home/chetana/tensorboard/2023-03-07_18-13/model_ckpt_3.pt\n",
  "history_begin_time" : 1678212810307,
  "history_end_time" : 1678212815277,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "QJCRHZFLRKl3",
  "history_input" : "from model_training_utils import add_hparams, EarlyStopping\nfrom get_device_config import *\nfrom loss_function import *\nfrom set_optmizer_and_scheduler import *\nfrom set_summary_writer import *\nfrom model_utils import *\nfrom torch_metrics_utils import *\nfrom tqdm.auto import tqdm\n\n\"\"\"\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nprint(train_m)\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"MulticlassAccuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"MulticlassAccuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\"\"\"\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\nprint(model_path)\n#torch.save(model.state_dict(), model_path)",
  "history_output" : "Read 252 samples from /home/chetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 12 samples from /home/chetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\n======================================================================\nWriting Tensorboard logs to /home/chetana/tensorboard/2023-03-07_18-12\n======================================================================\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/QJCRHZFLRKl3/run_model_training.py\", line 77, in <module>\n    model_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\nNameError: name 'N' is not defined\n",
  "history_begin_time" : 1678212770065,
  "history_end_time" : 1678212775033,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "ctcLRcIBvznC",
  "history_input" : "from model_training_utils import add_hparams, EarlyStopping\nfrom get_device_config import *\nfrom loss_function import *\nfrom set_optmizer_and_scheduler import *\nfrom set_summary_writer import *\nfrom model_utils import *\nfrom torch_metrics_utils import *\nfrom tqdm.auto import tqdm\n\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nprint(train_m)\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"MulticlassAccuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"MulticlassAccuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 252 samples from /home/chetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 12 samples from /home/chetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\n======================================================================\nWriting Tensorboard logs to /home/chetana/tensorboard/2023-03-07_17-45\n======================================================================\n\nTraining:   0%|                                   | 0/250 [00:00<?, ?epoch(s)/s]\nTraining:   0%| | 0/250 [00:12<?, ?epoch(s)/s, train_multiclassaccuracy=0.244053\nTraining:   0%| | 1/250 [00:12<51:41, 12.46s/epoch(s), train_multiclassaccuracy=\nTraining:   0%| | 1/250 [00:22<51:41, 12.46s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 2/250 [00:22<46:35, 11.27s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 2/250 [00:33<46:35, 11.27s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 3/250 [00:33<45:00, 10.94s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 3/250 [00:43<45:00, 10.94s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 4/250 [00:43<43:49, 10.69s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 4/250 [00:54<43:49, 10.69s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 5/250 [00:54<43:06, 10.56s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 5/250 [01:04<43:06, 10.56s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 6/250 [01:04<42:40, 10.50s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 6/250 [01:14<42:40, 10.50s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 7/250 [01:14<42:11, 10.42s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 7/250 [01:24<42:11, 10.42s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 8/250 [01:24<41:40, 10.33s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 8/250 [01:35<41:40, 10.33s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 9/250 [01:35<41:34, 10.35s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 9/250 [01:45<41:34, 10.35s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 10/250 [01:45<40:52, 10.22s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 10/250 [01:55<40:52, 10.22s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 11/250 [01:55<40:31, 10.17s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 11/250 [02:05<40:31, 10.17s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 12/250 [02:05<40:15, 10.15s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 12/250 [02:15<40:15, 10.15s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 13/250 [02:15<40:16, 10.20s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 13/250 [02:26<40:16, 10.20s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 14/250 [02:26<40:21, 10.26s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 14/250 [02:35<40:21, 10.26s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 15/250 [02:35<39:45, 10.15s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 15/250 [02:46<39:45, 10.15s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 16/250 [02:46<39:38, 10.16s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 16/250 [02:56<39:38, 10.16s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 17/250 [02:56<39:19, 10.13s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 17/250 [03:06<39:19, 10.13s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 18/250 [03:06<39:18, 10.17s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 18/250 [03:16<39:18, 10.17s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 19/250 [03:16<39:05, 10.15s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 19/250 [03:26<39:05, 10.15s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 20/250 [03:26<39:05, 10.20s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 20/250 [03:36<39:05, 10.20s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 21/250 [03:36<38:50, 10.18s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 21/250 [03:47<38:50, 10.18s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 22/250 [03:47<38:40, 10.18s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 22/250 [03:57<38:40, 10.18s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 23/250 [03:57<38:32, 10.19s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 23/250 [04:07<38:32, 10.19s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 24/250 [04:07<38:07, 10.12s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 24/250 [04:17<38:07, 10.12s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 25/250 [04:17<38:01, 10.14s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 25/250 [04:27<38:01, 10.14s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 26/250 [04:27<37:38, 10.08s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 26/250 [04:37<37:38, 10.08s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 27/250 [04:37<37:42, 10.14s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 27/250 [04:47<37:42, 10.14s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 28/250 [04:47<37:16, 10.08s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 28/250 [04:58<37:16, 10.08s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 29/250 [04:58<37:30, 10.18s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 29/250 [05:07<37:30, 10.18s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 30/250 [05:07<36:56, 10.07s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 30/250 [05:18<36:56, 10.07s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 31/250 [05:18<36:53, 10.11s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 31/250 [05:28<36:53, 10.11s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 32/250 [05:28<36:51, 10.14s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 32/250 [05:38<36:51, 10.14s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 33/250 [05:38<36:40, 10.14s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 33/250 [05:48<36:40, 10.14s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 34/250 [05:48<36:39, 10.18s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 34/250 [05:59<36:39, 10.18s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 35/250 [05:59<36:35, 10.21s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 35/250 [06:09<36:35, 10.21s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 36/250 [06:09<36:29, 10.23s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 36/250 [06:19<36:29, 10.23s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 37/250 [06:19<36:06, 10.17s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 37/250 [06:29<36:06, 10.17s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 38/250 [06:29<35:53, 10.16s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 38/250 [06:39<35:53, 10.16s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 39/250 [06:39<35:37, 10.13s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 39/250 [06:49<35:37, 10.13s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 40/250 [06:49<35:35, 10.17s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 40/250 [06:59<35:35, 10.17s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 41/250 [06:59<35:12, 10.11s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 41/250 [07:10<35:12, 10.11s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 42/250 [07:10<35:12, 10.16s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 42/250 [07:20<35:12, 10.16s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 43/250 [07:20<35:00, 10.15s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 43/250 [07:30<35:00, 10.15s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 44/250 [07:30<35:00, 10.20s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 44/250 [07:40<35:00, 10.20s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 45/250 [07:40<34:55, 10.22s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 45/250 [07:50<34:55, 10.22s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 46/250 [07:50<34:37, 10.19s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 46/250 [08:01<34:37, 10.19s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 47/250 [08:01<34:29, 10.20s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 47/250 [08:11<34:29, 10.20s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 48/250 [08:11<34:11, 10.16s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 48/250 [08:21<34:11, 10.16s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 49/250 [08:21<34:04, 10.17s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 49/250 [08:31<34:04, 10.17s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 50/250 [08:31<33:41, 10.11s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 50/250 [08:41<33:41, 10.11s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 51/250 [08:41<33:33, 10.12s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 51/250 [08:51<33:33, 10.12s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 52/250 [08:51<33:17, 10.09s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 52/250 [09:01<33:17, 10.09s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 53/250 [09:01<33:11, 10.11s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 53/250 [09:11<33:11, 10.11s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 54/250 [09:11<33:00, 10.10s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 54/250 [09:21<33:00, 10.10s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 55/250 [09:21<32:54, 10.13s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 55/250 [09:31<32:54, 10.13s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 56/250 [09:31<32:39, 10.10s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 56/250 [09:42<32:39, 10.10s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 57/250 [09:42<32:32, 10.12s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 57/250 [09:52<32:32, 10.12s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 58/250 [09:52<32:17, 10.09s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 58/250 [10:02<32:17, 10.09s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 59/250 [10:02<32:12, 10.12s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 59/250 [10:12<32:12, 10.12s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 60/250 [10:12<32:03, 10.13s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 60/250 [10:22<32:03, 10.13s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 61/250 [10:22<31:45, 10.08s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 61/250 [10:32<31:45, 10.08s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▏| 62/250 [10:32<31:42, 10.12s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▏| 62/250 [10:42<31:42, 10.12s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▎| 63/250 [10:42<31:27, 10.10s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▎| 63/250 [10:52<31:27, 10.10s/epoch(s), train_multiclassaccuracyEarly stopping at epoch 63 with validation loss 0.000 and training loss 1.010\n\nTraining:  25%|▎| 63/250 [10:52<32:17, 10.36s/epoch(s), train_multiclassaccuracy\n{'MulticlassAccuracy': tensor(0.6377)}\n",
  "history_begin_time" : 1678211089162,
  "history_end_time" : 1678211770583,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "TP46kpenHUD0",
  "history_input" : "from model_training_utils import add_hparams, EarlyStopping\nfrom get_device_config import *\nfrom loss_function import *\nfrom set_optmizer_and_scheduler import *\nfrom set_summary_writer import *\nfrom model_utils import *\nfrom torch_metrics_utils import *\nfrom tqdm.auto import tqdm\n\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nprint(train_m)\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 252 samples from /home/chetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 12 samples from /home/chetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\n======================================================================\nWriting Tensorboard logs to /home/chetana/tensorboard/2023-03-06_19-15\n======================================================================\n\nTraining:   0%|                                   | 0/250 [00:00<?, ?epoch(s)/s]\nTraining:   0%| | 0/250 [00:11<?, ?epoch(s)/s, train_multiclassaccuracy=0.244053\nTraining:   0%| | 1/250 [00:11<46:09, 11.12s/epoch(s), train_multiclassaccuracy=\nTraining:   0%| | 1/250 [00:21<46:09, 11.12s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 2/250 [00:21<43:48, 10.60s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 2/250 [00:31<43:48, 10.60s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 3/250 [00:31<43:42, 10.62s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 3/250 [00:42<43:42, 10.62s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 4/250 [00:42<43:09, 10.53s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 4/250 [00:52<43:09, 10.53s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 5/250 [00:52<42:41, 10.45s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 5/250 [01:02<42:41, 10.45s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 6/250 [01:02<41:54, 10.31s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 6/250 [01:12<41:54, 10.31s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 7/250 [01:12<41:27, 10.24s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 7/250 [01:22<41:27, 10.24s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 8/250 [01:22<41:00, 10.17s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 8/250 [01:33<41:00, 10.17s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 9/250 [01:33<40:50, 10.17s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 9/250 [01:43<40:50, 10.17s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 10/250 [01:43<40:32, 10.13s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 10/250 [01:52<40:32, 10.13s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 11/250 [01:52<40:05, 10.06s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 11/250 [02:03<40:05, 10.06s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 12/250 [02:03<40:03, 10.10s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 12/250 [02:13<40:03, 10.10s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 13/250 [02:13<39:36, 10.03s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 13/250 [02:23<39:36, 10.03s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 14/250 [02:23<39:36, 10.07s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 14/250 [02:33<39:36, 10.07s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 15/250 [02:33<39:14, 10.02s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 15/250 [02:43<39:14, 10.02s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 16/250 [02:43<39:10, 10.05s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 16/250 [02:53<39:10, 10.05s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 17/250 [02:53<38:49, 10.00s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 17/250 [03:03<38:49, 10.00s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 18/250 [03:03<38:39, 10.00s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 18/250 [03:13<38:39, 10.00s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 19/250 [03:13<38:26,  9.99s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 19/250 [03:23<38:26,  9.99s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 20/250 [03:23<38:23, 10.02s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 20/250 [03:32<38:23, 10.02s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 21/250 [03:32<38:00,  9.96s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 21/250 [03:42<38:00,  9.96s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 22/250 [03:42<37:48,  9.95s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 22/250 [03:52<37:48,  9.95s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 23/250 [03:52<37:41,  9.96s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 23/250 [04:02<37:41,  9.96s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 24/250 [04:02<37:33,  9.97s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 24/250 [04:12<37:33,  9.97s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 25/250 [04:12<37:32, 10.01s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 25/250 [04:22<37:32, 10.01s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 26/250 [04:22<37:16,  9.98s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 26/250 [04:33<37:16,  9.98s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 27/250 [04:33<37:21, 10.05s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 27/250 [04:43<37:21, 10.05s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 28/250 [04:43<37:07, 10.03s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 28/250 [04:53<37:07, 10.03s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 29/250 [04:53<37:09, 10.09s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 29/250 [05:03<37:09, 10.09s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 30/250 [05:03<36:43, 10.02s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 30/250 [05:13<36:43, 10.02s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 31/250 [05:13<36:35, 10.03s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 31/250 [05:23<36:35, 10.03s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 32/250 [05:23<36:23, 10.02s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 32/250 [05:33<36:23, 10.02s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 33/250 [05:33<36:03,  9.97s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 33/250 [05:43<36:03,  9.97s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 34/250 [05:43<35:53,  9.97s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 34/250 [05:52<35:53,  9.97s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 35/250 [05:52<35:33,  9.92s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 35/250 [06:02<35:33,  9.92s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 36/250 [06:02<35:34,  9.97s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 36/250 [06:12<35:34,  9.97s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 37/250 [06:12<35:28,  9.99s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 37/250 [06:23<35:28,  9.99s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 38/250 [06:23<35:40, 10.09s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 38/250 [06:33<35:40, 10.09s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 39/250 [06:33<35:20, 10.05s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 39/250 [06:43<35:20, 10.05s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 40/250 [06:43<35:23, 10.11s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 40/250 [06:53<35:23, 10.11s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 41/250 [06:53<35:04, 10.07s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 41/250 [07:03<35:04, 10.07s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 42/250 [07:03<34:58, 10.09s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 42/250 [07:13<34:58, 10.09s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 43/250 [07:13<35:00, 10.15s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 43/250 [07:24<35:00, 10.15s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 44/250 [07:24<35:27, 10.33s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 44/250 [07:35<35:27, 10.33s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 45/250 [07:35<35:42, 10.45s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 45/250 [07:45<35:42, 10.45s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 46/250 [07:45<35:06, 10.33s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 46/250 [07:55<35:06, 10.33s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 47/250 [07:55<34:43, 10.26s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 47/250 [08:05<34:43, 10.26s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 48/250 [08:05<34:32, 10.26s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 48/250 [08:16<34:32, 10.26s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 49/250 [08:16<34:46, 10.38s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 49/250 [08:26<34:46, 10.38s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 50/250 [08:26<34:19, 10.30s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 50/250 [08:37<34:19, 10.30s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 51/250 [08:37<34:33, 10.42s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 51/250 [08:47<34:33, 10.42s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 52/250 [08:47<34:20, 10.41s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 52/250 [08:57<34:20, 10.41s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 53/250 [08:57<34:08, 10.40s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 53/250 [09:07<34:08, 10.40s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 54/250 [09:07<33:25, 10.23s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 54/250 [09:18<33:25, 10.23s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 55/250 [09:18<33:26, 10.29s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 55/250 [09:28<33:26, 10.29s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 56/250 [09:28<32:56, 10.19s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 56/250 [09:38<32:56, 10.19s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 57/250 [09:38<32:47, 10.20s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 57/250 [09:48<32:47, 10.20s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 58/250 [09:48<32:57, 10.30s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 58/250 [09:59<32:57, 10.30s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 59/250 [09:59<33:20, 10.47s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 59/250 [10:10<33:20, 10.47s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 60/250 [10:10<33:24, 10.55s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 60/250 [10:20<33:24, 10.55s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 61/250 [10:20<33:03, 10.49s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 61/250 [10:31<33:03, 10.49s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▏| 62/250 [10:31<32:27, 10.36s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▏| 62/250 [10:41<32:27, 10.36s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▎| 63/250 [10:41<32:11, 10.33s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▎| 63/250 [10:51<32:11, 10.33s/epoch(s), train_multiclassaccuracyEarly stopping at epoch 63 with validation loss 0.000 and training loss 1.010\n\nTraining:  25%|▎| 63/250 [10:51<32:13, 10.34s/epoch(s), train_multiclassaccuracy\n{'MulticlassAccuracy': tensor(0.6377)}\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/TP46kpenHUD0/run_model_training.py\", line 69, in <module>\n    \"train/Accuracy\": train_m[\"Accuracy\"],\nKeyError: 'Accuracy'\n",
  "history_begin_time" : 1678130122322,
  "history_end_time" : 1678130778806,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "26ktdkvhn11",
  "history_input" : "from model_training_utils import add_hparams, EarlyStopping\nfrom get_device_config import *\nfrom loss_function import *\nfrom set_optmizer_and_scheduler import *\nfrom set_summary_writer import *\nfrom model_utils import *\nfrom torch_metrics_utils import *\nfrom tqdm.auto import tqdm\n\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 252 samples from /home/chetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 12 samples from /home/chetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\n======================================================================\nWriting Tensorboard logs to /home/chetana/tensorboard/2023-03-06_18-43\n======================================================================\n\nTraining:   0%|                                   | 0/250 [00:00<?, ?epoch(s)/s]\nTraining:   0%| | 0/250 [00:11<?, ?epoch(s)/s, train_multiclassaccuracy=0.244053\nTraining:   0%| | 1/250 [00:11<46:06, 11.11s/epoch(s), train_multiclassaccuracy=\nTraining:   0%| | 1/250 [00:21<46:06, 11.11s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 2/250 [00:21<44:18, 10.72s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 2/250 [00:32<44:18, 10.72s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 3/250 [00:32<43:54, 10.67s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 3/250 [00:42<43:54, 10.67s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 4/250 [00:42<43:13, 10.54s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 4/250 [00:52<43:13, 10.54s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 5/250 [00:52<42:49, 10.49s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 5/250 [01:03<42:49, 10.49s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 6/250 [01:03<42:13, 10.38s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 6/250 [01:13<42:13, 10.38s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 7/250 [01:13<41:55, 10.35s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 7/250 [01:23<41:55, 10.35s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 8/250 [01:23<41:28, 10.28s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 8/250 [01:33<41:28, 10.28s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 9/250 [01:33<41:31, 10.34s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 9/250 [01:44<41:31, 10.34s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 10/250 [01:44<41:49, 10.46s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 10/250 [01:54<41:49, 10.46s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 11/250 [01:54<41:10, 10.34s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 11/250 [02:05<41:10, 10.34s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 12/250 [02:05<41:03, 10.35s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 12/250 [02:15<41:03, 10.35s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 13/250 [02:15<40:38, 10.29s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 13/250 [02:25<40:38, 10.29s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 14/250 [02:25<40:23, 10.27s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 14/250 [02:35<40:23, 10.27s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 15/250 [02:35<39:50, 10.17s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 15/250 [02:45<39:50, 10.17s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 16/250 [02:45<39:41, 10.18s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 16/250 [02:55<39:41, 10.18s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 17/250 [02:55<39:21, 10.13s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 17/250 [03:05<39:21, 10.13s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 18/250 [03:05<39:01, 10.09s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 18/250 [03:15<39:01, 10.09s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 19/250 [03:15<38:29, 10.00s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 19/250 [03:25<38:29, 10.00s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 20/250 [03:25<38:21, 10.01s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 20/250 [03:35<38:21, 10.01s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 21/250 [03:35<38:12, 10.01s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 21/250 [03:45<38:12, 10.01s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 22/250 [03:45<38:03, 10.01s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 22/250 [03:55<38:03, 10.01s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 23/250 [03:55<37:54, 10.02s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 23/250 [04:05<37:54, 10.02s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 24/250 [04:05<37:34,  9.98s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 24/250 [04:15<37:34,  9.98s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 25/250 [04:15<37:23,  9.97s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 25/250 [04:25<37:23,  9.97s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 26/250 [04:25<37:18,  9.99s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 26/250 [04:35<37:18,  9.99s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 27/250 [04:35<37:20, 10.05s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 27/250 [04:45<37:20, 10.05s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 28/250 [04:45<36:59, 10.00s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 28/250 [04:55<36:59, 10.00s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 29/250 [04:55<36:48,  9.99s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 29/250 [05:05<36:48,  9.99s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 30/250 [05:05<36:37,  9.99s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 30/250 [05:15<36:37,  9.99s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 31/250 [05:15<36:31, 10.01s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 31/250 [05:25<36:31, 10.01s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 32/250 [05:25<36:19, 10.00s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 32/250 [05:35<36:19, 10.00s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 33/250 [05:35<36:05,  9.98s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 33/250 [05:45<36:05,  9.98s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 34/250 [05:45<36:06, 10.03s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 34/250 [05:55<36:06, 10.03s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 35/250 [05:55<35:54, 10.02s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 35/250 [06:05<35:54, 10.02s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 36/250 [06:05<35:59, 10.09s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 36/250 [06:15<35:59, 10.09s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 37/250 [06:15<35:38, 10.04s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 37/250 [06:25<35:38, 10.04s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 38/250 [06:25<35:41, 10.10s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 38/250 [06:35<35:41, 10.10s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 39/250 [06:35<35:24, 10.07s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 39/250 [06:46<35:24, 10.07s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 40/250 [06:46<35:24, 10.12s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 40/250 [06:56<35:24, 10.12s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 41/250 [06:56<35:06, 10.08s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 41/250 [07:06<35:06, 10.08s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 42/250 [07:06<34:59, 10.09s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 42/250 [07:16<34:59, 10.09s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 43/250 [07:16<34:41, 10.06s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 43/250 [07:26<34:41, 10.06s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 44/250 [07:26<34:39, 10.09s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 44/250 [07:36<34:39, 10.09s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 45/250 [07:36<34:29, 10.09s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 45/250 [07:46<34:29, 10.09s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 46/250 [07:46<34:14, 10.07s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 46/250 [07:56<34:14, 10.07s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 47/250 [07:56<34:06, 10.08s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 47/250 [08:06<34:06, 10.08s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 48/250 [08:06<33:49, 10.05s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 48/250 [08:16<33:49, 10.05s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 49/250 [08:16<33:40, 10.05s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 49/250 [08:26<33:40, 10.05s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 50/250 [08:26<33:14,  9.97s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 50/250 [08:36<33:14,  9.97s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 51/250 [08:36<33:16, 10.03s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 51/250 [08:46<33:16, 10.03s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 52/250 [08:46<33:06, 10.03s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 52/250 [08:56<33:06, 10.03s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 53/250 [08:56<33:01, 10.06s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 53/250 [09:06<33:01, 10.06s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 54/250 [09:06<32:37,  9.99s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 54/250 [09:16<32:37,  9.99s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 55/250 [09:16<32:43, 10.07s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 55/250 [09:26<32:43, 10.07s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 56/250 [09:26<32:24, 10.02s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 56/250 [09:36<32:24, 10.02s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 57/250 [09:36<32:16, 10.03s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 57/250 [09:46<32:16, 10.03s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 58/250 [09:46<32:04, 10.02s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 58/250 [09:57<32:04, 10.02s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 59/250 [09:57<32:03, 10.07s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 59/250 [10:07<32:03, 10.07s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 60/250 [10:07<31:56, 10.09s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 60/250 [10:17<31:56, 10.09s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 61/250 [10:17<31:37, 10.04s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 61/250 [10:27<31:37, 10.04s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▏| 62/250 [10:27<31:27, 10.04s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▏| 62/250 [10:37<31:27, 10.04s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▎| 63/250 [10:37<31:10, 10.00s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▎| 63/250 [10:47<31:10, 10.00s/epoch(s), train_multiclassaccuracyEarly stopping at epoch 63 with validation loss 0.000 and training loss 1.010\n\nTraining:  25%|▎| 63/250 [10:47<32:00, 10.27s/epoch(s), train_multiclassaccuracy\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/26ktdkvhn11/run_model_training.py\", line 68, in <module>\n    \"train/Accuracy\": train_m[\"Accuracy\"],\nKeyError: 'Accuracy'\n",
  "history_begin_time" : 1678128199100,
  "history_end_time" : 1678128852414,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "ycru82",
  "indicator" : "Failed"
},{
  "history_id" : "arnbv0uscta",
  "history_input" : "from model_training_utils import add_hparams, EarlyStopping\nfrom get_device_config import *\nfrom loss_function import *\nfrom set_optmizer_and_scheduler import *\nfrom set_summary_writer import *\nfrom model_utils import *\nfrom torch_metrics_utils import *\nfrom tqdm.auto import tqdm\n\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 252 samples from /home/chetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 12 samples from /home/chetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\n======================================================================\nWriting Tensorboard logs to /home/chetana/tensorboard/2023-03-06_18-37\n======================================================================\n\nTraining:   0%|                                   | 0/250 [00:00<?, ?epoch(s)/s]\nTraining:   0%|                                   | 0/250 [00:12<?, ?epoch(s)/s]\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/arnbv0uscta/run_model_training.py\", line 23, in <module>\n    train_loss, val_loss, train_m, val_m = run_epoch(\n  File \"/home/chetana/gw-workspace/arnbv0uscta/model_utils.py\", line 87, in run_epoch\n    num_classes, train_metrics, writer, epoch, \"train\"\nNameError: name 'num_classes' is not defined\n",
  "history_begin_time" : 1678127843574,
  "history_end_time" : 1678127862811,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "ycru82",
  "indicator" : "Failed"
},{
  "history_id" : "ckg23dvxodk",
  "history_input" : "from model_training_utils import add_hparams, EarlyStopping\nfrom get_device_config import *\nfrom loss_function import *\nfrom set_optmizer_and_scheduler import *\nfrom set_summary_writer import *\nfrom model_utils import *\nfrom torch_metrics_utils import *\nfrom tqdm.auto import tqdm\n\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 252 samples from /home/chetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 12 samples from /home/chetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/ckg23dvxodk/run_model_training.py\", line 5, in <module>\n    from set_summary_writer import *\n  File \"/home/chetana/gw-workspace/ckg23dvxodk/set_summary_writer.py\", line 14\n    f\"{''.join(['=']*(28 + len(writer.log_dir)))}\n    ^\nSyntaxError: unterminated string literal (detected at line 14)\n",
  "history_begin_time" : 1678127441382,
  "history_end_time" : 1678127449404,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "ycru82",
  "indicator" : "Failed"
},{
  "history_id" : "uyghs2mcflc",
  "history_input" : "from model_training_utils import add_hparams, EarlyStopping\nfrom get_device_config import *\nfrom loss_function import *\nfrom set_optmizer_and_scheduler import *\nfrom set_summary_writer import *\nfrom model_utils import *\nfrom torch_metrics_utils import *\nfrom tqdm.auto import tqdm\n\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 252 samples from /home/chetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 12 samples from /home/chetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/uyghs2mcflc/run_model_training.py\", line 4, in <module>\n    from set_optmizer_and_scheduler import *\n  File \"/home/chetana/gw-workspace/uyghs2mcflc/set_optmizer_and_scheduler.py\", line 7, in <module>\n    optimizer = torch.optim.Adam(model.parameters(), lr=max_lr)\nNameError: name 'model' is not defined\n",
  "history_begin_time" : 1678125764664,
  "history_end_time" : 1678125770764,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "ycru82",
  "indicator" : "Failed"
},{
  "history_id" : "l9quwab3rwy",
  "history_input" : "from model_training_utils import add_hparams, EarlyStopping\nfrom get_device_config import *\nfrom loss_function import *\nfrom set_optmizer_and_scheduler import *\nfrom set_summary_writer import *\nfrom model_utils import *\nfrom torch_metrics_utils import *\nfrom tqdm.auto import tqdm\n\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"Accuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"Accuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, f\"model_ckpt_{N+1}.pt\")\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Traceback (most recent call last):\n  File \"/home/chetana/gw-workspace/l9quwab3rwy/run_model_training.py\", line 1, in <module>\n    from model_training_utils import add_hparams, EarlyStopping\n  File \"/home/chetana/gw-workspace/l9quwab3rwy/model_training_utils.py\", line 5, in <module>\n    from torch.utils.tensorboard.summary import hparams\n  File \"/home/chetana/anaconda3/envs/ranjan/lib/python3.10/site-packages/torch/utils/tensorboard/__init__.py\", line 1, in <module>\n    import tensorboard\nModuleNotFoundError: No module named 'tensorboard'\n",
  "history_begin_time" : 1677775410240,
  "history_end_time" : 1677775414369,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "ycru82",
  "indicator" : "Failed"
},{
  "history_id" : "13aod1ubjpp",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1678838010076,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "ycru82",
  "indicator" : "Stopped"
},{
  "history_id" : "pi6b594ajsi",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1678850059452,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "ycru82",
  "indicator" : "Stopped"
},{
  "history_id" : "4vxrws3jrki",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1678250276599,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "ycru82",
  "indicator" : "Stopped"
},{
  "history_id" : "slwkxs0xuj9",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1678249820052,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "ycru82",
  "indicator" : "Stopped"
},{
  "history_id" : "jk03l3eo87d",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1680666547111,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "c2lqcn",
  "indicator" : "Stopped"
},{
  "history_id" : "ubxhahmqdma",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1680666655739,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "c2lqcn",
  "indicator" : "Stopped"
},{
  "history_id" : "vva3525u0ju",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1680666772311,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "c2lqcn",
  "indicator" : "Stopped"
},{
  "history_id" : "7eb488ce1i6",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1680666801839,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "c2lqcn",
  "indicator" : "Stopped"
},]
