[{
  "history_id" : "1mxx4qs6dej",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1682444006971,
  "history_end_time" : 1682444006971,
  "history_notes" : null,
  "history_process" : "xdwq7e",
  "host_id" : "c2lqcn",
  "indicator" : "Skipped"
},{
  "history_id" : "ly6ctqobju9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1682443926517,
  "history_end_time" : 1682443979655,
  "history_notes" : null,
  "history_process" : "xdwq7e",
  "host_id" : "c2lqcn",
  "indicator" : "Stopped"
},{
  "history_id" : "366rja4dv90",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1682443685802,
  "history_end_time" : 1682443925656,
  "history_notes" : null,
  "history_process" : "xdwq7e",
  "host_id" : "c2lqcn",
  "indicator" : "Stopped"
},{
  "history_id" : "mg2eivx3hjb",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1682442641209,
  "history_end_time" : 1682442641209,
  "history_notes" : null,
  "history_process" : "xdwq7e",
  "host_id" : "c2lqcn",
  "indicator" : "Skipped"
},{
  "history_id" : "b8k4c8vy9qn",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1682442532361,
  "history_end_time" : 1682442550972,
  "history_notes" : null,
  "history_process" : "xdwq7e",
  "host_id" : "c2lqcn",
  "indicator" : "Stopped"
},{
  "history_id" : "t86b220lwvc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1682441929642,
  "history_end_time" : 1682441929642,
  "history_notes" : null,
  "history_process" : "xdwq7e",
  "host_id" : "c2lqcn",
  "indicator" : "Skipped"
},{
  "history_id" : "zlamulx1m9a",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1682440386658,
  "history_end_time" : 1682440474437,
  "history_notes" : null,
  "history_process" : "xdwq7e",
  "host_id" : "c2lqcn",
  "indicator" : "Stopped"
},{
  "history_id" : "3lf5c41qpx6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1682440235245,
  "history_end_time" : 1682440385658,
  "history_notes" : null,
  "history_process" : "xdwq7e",
  "host_id" : "c2lqcn",
  "indicator" : "Stopped"
},{
  "history_id" : "w3xbvapqyph",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1682440181474,
  "history_end_time" : 1682440234545,
  "history_notes" : null,
  "history_process" : "xdwq7e",
  "host_id" : "c2lqcn",
  "indicator" : "Stopped"
},{
  "history_id" : "566bhv8i0r0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1682439988637,
  "history_end_time" : 1682440138046,
  "history_notes" : null,
  "history_process" : "xdwq7e",
  "host_id" : "c2lqcn",
  "indicator" : "Stopped"
},{
  "history_id" : "cdnvjyt1gog",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1682439640531,
  "history_end_time" : 1682439640531,
  "history_notes" : null,
  "history_process" : "xdwq7e",
  "host_id" : "c2lqcn",
  "indicator" : "Skipped"
},{
  "history_id" : "23l97qj26hp",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1682439488954,
  "history_end_time" : 1682439488954,
  "history_notes" : null,
  "history_process" : "xdwq7e",
  "host_id" : "c2lqcn",
  "indicator" : "Skipped"
},{
  "history_id" : "alpv8r97mis",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1682439471673,
  "history_end_time" : 1682439471673,
  "history_notes" : null,
  "history_process" : "xdwq7e",
  "host_id" : "c2lqcn",
  "indicator" : "Skipped"
},{
  "history_id" : "y3vss4va1ea",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1682437191024,
  "history_end_time" : 1682437217449,
  "history_notes" : null,
  "history_process" : "xdwq7e",
  "host_id" : "c2lqcn",
  "indicator" : "Stopped"
},{
  "history_id" : "swixye5xhdb",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1682434977831,
  "history_end_time" : 1682435038076,
  "history_notes" : null,
  "history_process" : "xdwq7e",
  "host_id" : "c2lqcn",
  "indicator" : "Stopped"
},{
  "history_id" : "rrb15czx2t0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1681846897182,
  "history_end_time" : 1681846897182,
  "history_notes" : null,
  "history_process" : "xdwq7e",
  "host_id" : "c2lqcn",
  "indicator" : "Skipped"
},{
  "history_id" : "ib1zrpuntmz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1681846509209,
  "history_end_time" : 1681846782382,
  "history_notes" : null,
  "history_process" : "xdwq7e",
  "host_id" : "c2lqcn",
  "indicator" : "Stopped"
},{
  "history_id" : "guu6n7zdxvm",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1681845552441,
  "history_end_time" : 1681845552441,
  "history_notes" : null,
  "history_process" : "xdwq7e",
  "host_id" : "c2lqcn",
  "indicator" : "Skipped"
},{
  "history_id" : "z218jmva9u6",
  "history_input" : "import collections\nfrom itertools import repeat\nfrom typing import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass EddyNet(nn.Module):\n    \"\"\"\n    PyTorch implementation of EddyNet from Lguensat et al. (2018)\n    Original implementation in TensorFlow: https://github.com/redouanelg/EddyNet\n    \"\"\"\n    def __init__(self, num_classes, num_filters, kernel_size):\n        super(EddyNet, self).__init__()\n        # encoder\n        self.encoder1 = EddyNet._block(1, num_filters, kernel_size, \"enc1\", dropout=0.2)\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder2 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc2\", dropout=0.3\n        )\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder3 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc3\", dropout=0.4\n        )\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder4 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc4\", dropout=0.5\n        )\n\n        # decoder\n        self.decoder3 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec3\", dropout=0.4\n        )\n        self.decoder2 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec2\", dropout=0.3\n        )\n        self.decoder1 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec1\", dropout=0.2\n        )\n\n        # final layer\n        self.final_conv = nn.Conv2d(\n            num_filters, num_classes, kernel_size=1, padding=0, bias=False\n        )\n\n    @staticmethod\n    def conv_block(in_channels, out_channels, kernel_size, name, num, dropout=0):\n        layers = {\n            f\"{name}_conv{num}\": Conv2dSame(in_channels, out_channels, kernel_size),\n            f\"{name}_bn{num}\": nn.BatchNorm2d(out_channels),\n            f\"{name}_relu{num}\": nn.ReLU(inplace=True),\n        }\n        if dropout > 0:\n            layers[f\"{name}_dropout\"] = nn.Dropout(p=dropout)\n\n        return nn.Sequential(OrderedDict(layers))\n\n    @staticmethod\n    def _block(in_channels, out_channels, kernel_size, name, dropout=0):\n        conv1 = EddyNet.conv_block(in_channels, out_channels, kernel_size, name, 1)\n        conv2 = EddyNet.conv_block(\n            out_channels, out_channels, kernel_size, name, 2, dropout=dropout\n        )\n        return nn.Sequential(conv1, conv2)\n\n    @staticmethod\n    def decoder_block(in_channels, out_channels, kernel_size, name, dropout=0):\n        return EddyNet._block(in_channels, out_channels, kernel_size, name, dropout)\n\n    def forward(self, x):\n        # encoder\n        enc1 = self.encoder1(x)\n        pool1 = self.pool1(enc1)\n\n        enc2 = self.encoder2(pool1)\n        pool2 = self.pool2(enc2)\n\n        enc3 = self.encoder3(pool2)\n        pool3 = self.pool3(enc3)\n\n        # bottleneck?\n        enc4 = self.encoder4(pool3)\n\n        # decoder\n        dec3 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(enc4)\n        dec3 = torch.cat((dec3, enc3), dim=1)\n        dec3 = self.decoder3(dec3)\n\n        dec2 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec3)\n        dec2 = torch.cat((dec2, enc2), dim=1)\n        dec2 = self.decoder2(dec2)\n\n        dec1 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec2)\n        dec1 = torch.cat((dec1, enc1), dim=1)\n        dec1 = self.decoder1(dec1)\n\n        # final layer\n        final = self.final_conv(dec1)\n\n        # softmax\n        final = nn.Softmax(dim=1)(final)\n\n        return final\n\n\nclass Conv2dSame(nn.Module):\n    \"\"\"Manual convolution with same padding\n    https://discuss.pytorch.org/t/same-padding-equivalent-in-pytorch/85121/9\n    Although PyTorch >= 1.10.0 supports ``padding='same'`` as a keyword\n    argument, this does not export to CoreML as of coremltools 5.1.0,\n    so we need to implement the internal torch logic manually.\n\n    Currently the ``RuntimeError`` is\n\n    \"PyTorch convert function for op '_convolution_mode' not implemented\"\n    \"\"\"\n\n    def __init__(\n        self, in_channels, out_channels, kernel_size, stride=1, dilation=1, **kwargs\n    ):\n        \"\"\"Wrap base convolution layer\n\n        See official PyTorch documentation for parameter details\n        https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n        \"\"\"\n        super().__init__()\n        self.conv = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            dilation=dilation,\n            **kwargs,\n        )\n\n        # Setup internal representations\n        kernel_size_ = _pair(kernel_size)\n        dilation_ = _pair(dilation)\n        self._reversed_padding_repeated_twice = [0, 0] * len(kernel_size_)\n\n        # Follow the logic from ``nn/modules/conv.py:_ConvNd``\n        for d, k, i in zip(\n            dilation_, kernel_size_, range(len(kernel_size_) - 1, -1, -1)\n        ):\n            total_padding = d * (k - 1)\n            left_pad = total_padding // 2\n            self._reversed_padding_repeated_twice[2 * i] = left_pad\n            self._reversed_padding_repeated_twice[2 * i + 1] = total_padding - left_pad\n\n    def forward(self, imgs):\n        \"\"\"Setup padding so same spatial dimensions are returned\n\n        All shapes (input/output) are ``(N, C, W, H)`` convention\n\n        :param torch.Tensor imgs:\n        :return torch.Tensor:\n        \"\"\"\n        padded = F.pad(imgs, self._reversed_padding_repeated_twice)\n        return self.conv(padded)\n\n\ndef _ntuple(n):\n    \"\"\"Copy from PyTorch since internal function is not importable\n\n    See ``nn/modules/utils.py:6``\n    \"\"\"\n\n    def parse(x):\n        if isinstance(x, collections.abc.Iterable):\n            return tuple(x)\n        return tuple(repeat(x, n))\n\n    return parse\n\n\n_pair = _ntuple(2)",
  "history_output" : "",
  "history_begin_time" : 1681842356475,
  "history_end_time" : 1681842360229,
  "history_notes" : null,
  "history_process" : "xdwq7e",
  "host_id" : "c2lqcn",
  "indicator" : "Done"
},{
  "history_id" : "pwql4rlnxmr",
  "history_input" : "import collections\nfrom itertools import repeat\nfrom typing import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass EddyNet(nn.Module):\n    \"\"\"\n    PyTorch implementation of EddyNet from Lguensat et al. (2018)\n    Original implementation in TensorFlow: https://github.com/redouanelg/EddyNet\n    \"\"\"\n    def __init__(self, num_classes, num_filters, kernel_size):\n        super(EddyNet, self).__init__()\n        # encoder\n        self.encoder1 = EddyNet._block(1, num_filters, kernel_size, \"enc1\", dropout=0.2)\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder2 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc2\", dropout=0.3\n        )\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder3 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc3\", dropout=0.4\n        )\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder4 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc4\", dropout=0.5\n        )\n\n        # decoder\n        self.decoder3 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec3\", dropout=0.4\n        )\n        self.decoder2 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec2\", dropout=0.3\n        )\n        self.decoder1 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec1\", dropout=0.2\n        )\n\n        # final layer\n        self.final_conv = nn.Conv2d(\n            num_filters, num_classes, kernel_size=1, padding=0, bias=False\n        )\n\n    @staticmethod\n    def conv_block(in_channels, out_channels, kernel_size, name, num, dropout=0):\n        layers = {\n            f\"{name}_conv{num}\": Conv2dSame(in_channels, out_channels, kernel_size),\n            f\"{name}_bn{num}\": nn.BatchNorm2d(out_channels),\n            f\"{name}_relu{num}\": nn.ReLU(inplace=True),\n        }\n        if dropout > 0:\n            layers[f\"{name}_dropout\"] = nn.Dropout(p=dropout)\n\n        return nn.Sequential(OrderedDict(layers))\n\n    @staticmethod\n    def _block(in_channels, out_channels, kernel_size, name, dropout=0):\n        conv1 = EddyNet.conv_block(in_channels, out_channels, kernel_size, name, 1)\n        conv2 = EddyNet.conv_block(\n            out_channels, out_channels, kernel_size, name, 2, dropout=dropout\n        )\n        return nn.Sequential(conv1, conv2)\n\n    @staticmethod\n    def decoder_block(in_channels, out_channels, kernel_size, name, dropout=0):\n        return EddyNet._block(in_channels, out_channels, kernel_size, name, dropout)\n\n    def forward(self, x):\n        # encoder\n        enc1 = self.encoder1(x)\n        pool1 = self.pool1(enc1)\n\n        enc2 = self.encoder2(pool1)\n        pool2 = self.pool2(enc2)\n\n        enc3 = self.encoder3(pool2)\n        pool3 = self.pool3(enc3)\n\n        # bottleneck?\n        enc4 = self.encoder4(pool3)\n\n        # decoder\n        dec3 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(enc4)\n        dec3 = torch.cat((dec3, enc3), dim=1)\n        dec3 = self.decoder3(dec3)\n\n        dec2 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec3)\n        dec2 = torch.cat((dec2, enc2), dim=1)\n        dec2 = self.decoder2(dec2)\n\n        dec1 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec2)\n        dec1 = torch.cat((dec1, enc1), dim=1)\n        dec1 = self.decoder1(dec1)\n\n        # final layer\n        final = self.final_conv(dec1)\n\n        # softmax\n        final = nn.Softmax(dim=1)(final)\n\n        return final\n\n\nclass Conv2dSame(nn.Module):\n    \"\"\"Manual convolution with same padding\n    https://discuss.pytorch.org/t/same-padding-equivalent-in-pytorch/85121/9\n    Although PyTorch >= 1.10.0 supports ``padding='same'`` as a keyword\n    argument, this does not export to CoreML as of coremltools 5.1.0,\n    so we need to implement the internal torch logic manually.\n\n    Currently the ``RuntimeError`` is\n\n    \"PyTorch convert function for op '_convolution_mode' not implemented\"\n    \"\"\"\n\n    def __init__(\n        self, in_channels, out_channels, kernel_size, stride=1, dilation=1, **kwargs\n    ):\n        \"\"\"Wrap base convolution layer\n\n        See official PyTorch documentation for parameter details\n        https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n        \"\"\"\n        super().__init__()\n        self.conv = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            dilation=dilation,\n            **kwargs,\n        )\n\n        # Setup internal representations\n        kernel_size_ = _pair(kernel_size)\n        dilation_ = _pair(dilation)\n        self._reversed_padding_repeated_twice = [0, 0] * len(kernel_size_)\n\n        # Follow the logic from ``nn/modules/conv.py:_ConvNd``\n        for d, k, i in zip(\n            dilation_, kernel_size_, range(len(kernel_size_) - 1, -1, -1)\n        ):\n            total_padding = d * (k - 1)\n            left_pad = total_padding // 2\n            self._reversed_padding_repeated_twice[2 * i] = left_pad\n            self._reversed_padding_repeated_twice[2 * i + 1] = total_padding - left_pad\n\n    def forward(self, imgs):\n        \"\"\"Setup padding so same spatial dimensions are returned\n\n        All shapes (input/output) are ``(N, C, W, H)`` convention\n\n        :param torch.Tensor imgs:\n        :return torch.Tensor:\n        \"\"\"\n        padded = F.pad(imgs, self._reversed_padding_repeated_twice)\n        return self.conv(padded)\n\n\ndef _ntuple(n):\n    \"\"\"Copy from PyTorch since internal function is not importable\n\n    See ``nn/modules/utils.py:6``\n    \"\"\"\n\n    def parse(x):\n        if isinstance(x, collections.abc.Iterable):\n            return tuple(x)\n        return tuple(repeat(x, n))\n\n    return parse\n\n\n_pair = _ntuple(2)",
  "history_output" : "",
  "history_begin_time" : 1681841806509,
  "history_end_time" : 1681841810693,
  "history_notes" : null,
  "history_process" : "xdwq7e",
  "host_id" : "c2lqcn",
  "indicator" : "Done"
},{
  "history_id" : "o0l51n8rj7d",
  "history_input" : "import collections\nfrom itertools import repeat\nfrom typing import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass EddyNet(nn.Module):\n    \"\"\"\n    PyTorch implementation of EddyNet from Lguensat et al. (2018)\n    Original implementation in TensorFlow: https://github.com/redouanelg/EddyNet\n    \"\"\"\n    def __init__(self, num_classes, num_filters, kernel_size):\n        super(EddyNet, self).__init__()\n        # encoder\n        self.encoder1 = EddyNet._block(1, num_filters, kernel_size, \"enc1\", dropout=0.2)\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder2 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc2\", dropout=0.3\n        )\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder3 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc3\", dropout=0.4\n        )\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder4 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc4\", dropout=0.5\n        )\n\n        # decoder\n        self.decoder3 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec3\", dropout=0.4\n        )\n        self.decoder2 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec2\", dropout=0.3\n        )\n        self.decoder1 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec1\", dropout=0.2\n        )\n\n        # final layer\n        self.final_conv = nn.Conv2d(\n            num_filters, num_classes, kernel_size=1, padding=0, bias=False\n        )\n\n    @staticmethod\n    def conv_block(in_channels, out_channels, kernel_size, name, num, dropout=0):\n        layers = {\n            f\"{name}_conv{num}\": Conv2dSame(in_channels, out_channels, kernel_size),\n            f\"{name}_bn{num}\": nn.BatchNorm2d(out_channels),\n            f\"{name}_relu{num}\": nn.ReLU(inplace=True),\n        }\n        if dropout > 0:\n            layers[f\"{name}_dropout\"] = nn.Dropout(p=dropout)\n\n        return nn.Sequential(OrderedDict(layers))\n\n    @staticmethod\n    def _block(in_channels, out_channels, kernel_size, name, dropout=0):\n        conv1 = EddyNet.conv_block(in_channels, out_channels, kernel_size, name, 1)\n        conv2 = EddyNet.conv_block(\n            out_channels, out_channels, kernel_size, name, 2, dropout=dropout\n        )\n        return nn.Sequential(conv1, conv2)\n\n    @staticmethod\n    def decoder_block(in_channels, out_channels, kernel_size, name, dropout=0):\n        return EddyNet._block(in_channels, out_channels, kernel_size, name, dropout)\n\n    def forward(self, x):\n        # encoder\n        enc1 = self.encoder1(x)\n        pool1 = self.pool1(enc1)\n\n        enc2 = self.encoder2(pool1)\n        pool2 = self.pool2(enc2)\n\n        enc3 = self.encoder3(pool2)\n        pool3 = self.pool3(enc3)\n\n        # bottleneck?\n        enc4 = self.encoder4(pool3)\n\n        # decoder\n        dec3 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(enc4)\n        dec3 = torch.cat((dec3, enc3), dim=1)\n        dec3 = self.decoder3(dec3)\n\n        dec2 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec3)\n        dec2 = torch.cat((dec2, enc2), dim=1)\n        dec2 = self.decoder2(dec2)\n\n        dec1 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec2)\n        dec1 = torch.cat((dec1, enc1), dim=1)\n        dec1 = self.decoder1(dec1)\n\n        # final layer\n        final = self.final_conv(dec1)\n\n        # softmax\n        final = nn.Softmax(dim=1)(final)\n\n        return final\n\n\nclass Conv2dSame(nn.Module):\n    \"\"\"Manual convolution with same padding\n    https://discuss.pytorch.org/t/same-padding-equivalent-in-pytorch/85121/9\n    Although PyTorch >= 1.10.0 supports ``padding='same'`` as a keyword\n    argument, this does not export to CoreML as of coremltools 5.1.0,\n    so we need to implement the internal torch logic manually.\n\n    Currently the ``RuntimeError`` is\n\n    \"PyTorch convert function for op '_convolution_mode' not implemented\"\n    \"\"\"\n\n    def __init__(\n        self, in_channels, out_channels, kernel_size, stride=1, dilation=1, **kwargs\n    ):\n        \"\"\"Wrap base convolution layer\n\n        See official PyTorch documentation for parameter details\n        https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n        \"\"\"\n        super().__init__()\n        self.conv = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            dilation=dilation,\n            **kwargs,\n        )\n\n        # Setup internal representations\n        kernel_size_ = _pair(kernel_size)\n        dilation_ = _pair(dilation)\n        self._reversed_padding_repeated_twice = [0, 0] * len(kernel_size_)\n\n        # Follow the logic from ``nn/modules/conv.py:_ConvNd``\n        for d, k, i in zip(\n            dilation_, kernel_size_, range(len(kernel_size_) - 1, -1, -1)\n        ):\n            total_padding = d * (k - 1)\n            left_pad = total_padding // 2\n            self._reversed_padding_repeated_twice[2 * i] = left_pad\n            self._reversed_padding_repeated_twice[2 * i + 1] = total_padding - left_pad\n\n    def forward(self, imgs):\n        \"\"\"Setup padding so same spatial dimensions are returned\n\n        All shapes (input/output) are ``(N, C, W, H)`` convention\n\n        :param torch.Tensor imgs:\n        :return torch.Tensor:\n        \"\"\"\n        padded = F.pad(imgs, self._reversed_padding_repeated_twice)\n        return self.conv(padded)\n\n\ndef _ntuple(n):\n    \"\"\"Copy from PyTorch since internal function is not importable\n\n    See ``nn/modules/utils.py:6``\n    \"\"\"\n\n    def parse(x):\n        if isinstance(x, collections.abc.Iterable):\n            return tuple(x)\n        return tuple(repeat(x, n))\n\n    return parse\n\n\n_pair = _ntuple(2)",
  "history_output" : "Running",
  "history_begin_time" : 1681841546691,
  "history_end_time" : 1681841549645,
  "history_notes" : null,
  "history_process" : "xdwq7e",
  "host_id" : "c2lqcn",
  "indicator" : "Done"
},{
  "history_id" : "kak8p41o35a",
  "history_input" : "import collections\nfrom itertools import repeat\nfrom typing import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass EddyNet(nn.Module):\n    \"\"\"\n    PyTorch implementation of EddyNet from Lguensat et al. (2018)\n    Original implementation in TensorFlow: https://github.com/redouanelg/EddyNet\n    \"\"\"\n    def __init__(self, num_classes, num_filters, kernel_size):\n        super(EddyNet, self).__init__()\n        # encoder\n        self.encoder1 = EddyNet._block(1, num_filters, kernel_size, \"enc1\", dropout=0.2)\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder2 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc2\", dropout=0.3\n        )\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder3 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc3\", dropout=0.4\n        )\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder4 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc4\", dropout=0.5\n        )\n\n        # decoder\n        self.decoder3 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec3\", dropout=0.4\n        )\n        self.decoder2 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec2\", dropout=0.3\n        )\n        self.decoder1 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec1\", dropout=0.2\n        )\n\n        # final layer\n        self.final_conv = nn.Conv2d(\n            num_filters, num_classes, kernel_size=1, padding=0, bias=False\n        )\n\n    @staticmethod\n    def conv_block(in_channels, out_channels, kernel_size, name, num, dropout=0):\n        layers = {\n            f\"{name}_conv{num}\": Conv2dSame(in_channels, out_channels, kernel_size),\n            f\"{name}_bn{num}\": nn.BatchNorm2d(out_channels),\n            f\"{name}_relu{num}\": nn.ReLU(inplace=True),\n        }\n        if dropout > 0:\n            layers[f\"{name}_dropout\"] = nn.Dropout(p=dropout)\n\n        return nn.Sequential(OrderedDict(layers))\n\n    @staticmethod\n    def _block(in_channels, out_channels, kernel_size, name, dropout=0):\n        conv1 = EddyNet.conv_block(in_channels, out_channels, kernel_size, name, 1)\n        conv2 = EddyNet.conv_block(\n            out_channels, out_channels, kernel_size, name, 2, dropout=dropout\n        )\n        return nn.Sequential(conv1, conv2)\n\n    @staticmethod\n    def decoder_block(in_channels, out_channels, kernel_size, name, dropout=0):\n        return EddyNet._block(in_channels, out_channels, kernel_size, name, dropout)\n\n    def forward(self, x):\n        # encoder\n        enc1 = self.encoder1(x)\n        pool1 = self.pool1(enc1)\n\n        enc2 = self.encoder2(pool1)\n        pool2 = self.pool2(enc2)\n\n        enc3 = self.encoder3(pool2)\n        pool3 = self.pool3(enc3)\n\n        # bottleneck?\n        enc4 = self.encoder4(pool3)\n\n        # decoder\n        dec3 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(enc4)\n        dec3 = torch.cat((dec3, enc3), dim=1)\n        dec3 = self.decoder3(dec3)\n\n        dec2 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec3)\n        dec2 = torch.cat((dec2, enc2), dim=1)\n        dec2 = self.decoder2(dec2)\n\n        dec1 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec2)\n        dec1 = torch.cat((dec1, enc1), dim=1)\n        dec1 = self.decoder1(dec1)\n\n        # final layer\n        final = self.final_conv(dec1)\n\n        # softmax\n        final = nn.Softmax(dim=1)(final)\n\n        return final\n\n\nclass Conv2dSame(nn.Module):\n    \"\"\"Manual convolution with same padding\n    https://discuss.pytorch.org/t/same-padding-equivalent-in-pytorch/85121/9\n    Although PyTorch >= 1.10.0 supports ``padding='same'`` as a keyword\n    argument, this does not export to CoreML as of coremltools 5.1.0,\n    so we need to implement the internal torch logic manually.\n\n    Currently the ``RuntimeError`` is\n\n    \"PyTorch convert function for op '_convolution_mode' not implemented\"\n    \"\"\"\n\n    def __init__(\n        self, in_channels, out_channels, kernel_size, stride=1, dilation=1, **kwargs\n    ):\n        \"\"\"Wrap base convolution layer\n\n        See official PyTorch documentation for parameter details\n        https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n        \"\"\"\n        super().__init__()\n        self.conv = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            dilation=dilation,\n            **kwargs,\n        )\n\n        # Setup internal representations\n        kernel_size_ = _pair(kernel_size)\n        dilation_ = _pair(dilation)\n        self._reversed_padding_repeated_twice = [0, 0] * len(kernel_size_)\n\n        # Follow the logic from ``nn/modules/conv.py:_ConvNd``\n        for d, k, i in zip(\n            dilation_, kernel_size_, range(len(kernel_size_) - 1, -1, -1)\n        ):\n            total_padding = d * (k - 1)\n            left_pad = total_padding // 2\n            self._reversed_padding_repeated_twice[2 * i] = left_pad\n            self._reversed_padding_repeated_twice[2 * i + 1] = total_padding - left_pad\n\n    def forward(self, imgs):\n        \"\"\"Setup padding so same spatial dimensions are returned\n\n        All shapes (input/output) are ``(N, C, W, H)`` convention\n\n        :param torch.Tensor imgs:\n        :return torch.Tensor:\n        \"\"\"\n        padded = F.pad(imgs, self._reversed_padding_repeated_twice)\n        return self.conv(padded)\n\n\ndef _ntuple(n):\n    \"\"\"Copy from PyTorch since internal function is not importable\n\n    See ``nn/modules/utils.py:6``\n    \"\"\"\n\n    def parse(x):\n        if isinstance(x, collections.abc.Iterable):\n            return tuple(x)\n        return tuple(repeat(x, n))\n\n    return parse\n\n\n_pair = _ntuple(2)",
  "history_output" : "Running",
  "history_begin_time" : 1681841143474,
  "history_end_time" : 1681841156305,
  "history_notes" : null,
  "history_process" : "xdwq7e",
  "host_id" : "c2lqcn",
  "indicator" : "Done"
},{
  "history_id" : "7i67ct5tteb",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1680667091231,
  "history_end_time" : 1680667091231,
  "history_notes" : null,
  "history_process" : "xdwq7e",
  "host_id" : "c2lqcn",
  "indicator" : "Skipped"
},{
  "history_id" : "jnfhyjr2v41",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1680666804796,
  "history_end_time" : 1680666804796,
  "history_notes" : null,
  "history_process" : "xdwq7e",
  "host_id" : "c2lqcn",
  "indicator" : "Skipped"
},{
  "history_id" : "056zvtaymfz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1680666776157,
  "history_end_time" : 1680666801838,
  "history_notes" : null,
  "history_process" : "xdwq7e",
  "host_id" : "c2lqcn",
  "indicator" : "Stopped"
},{
  "history_id" : "nxeq3efa35k",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1680666722509,
  "history_end_time" : 1680666772311,
  "history_notes" : null,
  "history_process" : "xdwq7e",
  "host_id" : "c2lqcn",
  "indicator" : "Stopped"
},{
  "history_id" : "wwayr2r00vi",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1680666673148,
  "history_end_time" : 1680666673148,
  "history_notes" : null,
  "history_process" : "xdwq7e",
  "host_id" : "c2lqcn",
  "indicator" : "Skipped"
},{
  "history_id" : "5cbbqmxf9st",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1680666590251,
  "history_end_time" : 1680666655738,
  "history_notes" : null,
  "history_process" : "xdwq7e",
  "host_id" : "c2lqcn",
  "indicator" : "Stopped"
},{
  "history_id" : "6fr5wm2cxwm",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1680666547664,
  "history_end_time" : 1680666547664,
  "history_notes" : null,
  "history_process" : "xdwq7e",
  "host_id" : "c2lqcn",
  "indicator" : "Skipped"
},{
  "history_id" : "lcultych54g",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1680666518402,
  "history_end_time" : 1680666547109,
  "history_notes" : null,
  "history_process" : "xdwq7e",
  "host_id" : "c2lqcn",
  "indicator" : "Stopped"
},{
  "history_id" : "cb96t18qciz",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1680032244937,
  "history_end_time" : 1680032244937,
  "history_notes" : null,
  "history_process" : "xdwq7e",
  "host_id" : "c2lqcn",
  "indicator" : "Skipped"
},{
  "history_id" : "gq58shyrxuy",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1680029557950,
  "history_end_time" : 1680029661148,
  "history_notes" : null,
  "history_process" : "xdwq7e",
  "host_id" : "c2lqcn",
  "indicator" : "Stopped"
},{
  "history_id" : "6pmffvo3ba4",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678850346167,
  "history_end_time" : 1678850346167,
  "history_notes" : null,
  "history_process" : "xdwq7e",
  "host_id" : "ycru82",
  "indicator" : "Skipped"
},{
  "history_id" : "zfj3gd0uqmj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678838074008,
  "history_end_time" : 1678850059446,
  "history_notes" : null,
  "history_process" : "xdwq7e",
  "host_id" : "ycru82",
  "indicator" : "Stopped"
},{
  "history_id" : "lu1dwrzkfcm",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678837910848,
  "history_end_time" : 1678838010070,
  "history_notes" : null,
  "history_process" : "xdwq7e",
  "host_id" : "ycru82",
  "indicator" : "Stopped"
},{
  "history_id" : "fw2gu1zxtlh",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678837413876,
  "history_end_time" : 1678837413876,
  "history_notes" : null,
  "history_process" : "xdwq7e",
  "host_id" : "ycru82",
  "indicator" : "Skipped"
},{
  "history_id" : "sppitbgxk6a",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678837361463,
  "history_end_time" : 1678837361463,
  "history_notes" : null,
  "history_process" : "xdwq7e",
  "host_id" : "ycru82",
  "indicator" : "Skipped"
},{
  "history_id" : "jfp65xc6fld",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678250309063,
  "history_end_time" : 1678250309063,
  "history_notes" : null,
  "history_process" : "xdwq7e",
  "host_id" : "ycru82",
  "indicator" : "Skipped"
},{
  "history_id" : "sjoh9qytwvw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678250165322,
  "history_end_time" : 1678250276596,
  "history_notes" : null,
  "history_process" : "xdwq7e",
  "host_id" : "ycru82",
  "indicator" : "Stopped"
},{
  "history_id" : "83b5dmkegjy",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678250120286,
  "history_end_time" : 1678250120286,
  "history_notes" : null,
  "history_process" : "xdwq7e",
  "host_id" : "ycru82",
  "indicator" : "Skipped"
},{
  "history_id" : "02ms2p2zmku",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678213366154,
  "history_end_time" : 1678249820046,
  "history_notes" : null,
  "history_process" : "xdwq7e",
  "host_id" : "ycru82",
  "indicator" : "Stopped"
},{
  "history_id" : "hg9cwaanjlu",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678128154751,
  "history_end_time" : 1678128154751,
  "history_notes" : null,
  "history_process" : "xdwq7e",
  "host_id" : "ycru82",
  "indicator" : "Skipped"
},{
  "history_id" : "oahpk9t0jd5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678127800453,
  "history_end_time" : 1678127800453,
  "history_notes" : null,
  "history_process" : "xdwq7e",
  "host_id" : "ycru82",
  "indicator" : "Skipped"
},{
  "history_id" : "f1uesx77c2l",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678127389150,
  "history_end_time" : 1678127389150,
  "history_notes" : null,
  "history_process" : "xdwq7e",
  "host_id" : "ycru82",
  "indicator" : "Skipped"
},{
  "history_id" : "im6yzq9330j",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1678125719458,
  "history_end_time" : 1678125719458,
  "history_notes" : null,
  "history_process" : "xdwq7e",
  "host_id" : "ycru82",
  "indicator" : "Skipped"
},{
  "history_id" : "nSM3XriYnGUP",
  "history_input" : "import collections\nfrom itertools import repeat\nfrom typing import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass EddyNet(nn.Module):\n    \"\"\"\n    PyTorch implementation of EddyNet from Lguensat et al. (2018)\n    Original implementation in TensorFlow: https://github.com/redouanelg/EddyNet\n    \"\"\"\n    def __init__(self, num_classes, num_filters, kernel_size):\n        super(EddyNet, self).__init__()\n        # encoder\n        self.encoder1 = EddyNet._block(1, num_filters, kernel_size, \"enc1\", dropout=0.2)\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder2 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc2\", dropout=0.3\n        )\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder3 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc3\", dropout=0.4\n        )\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder4 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc4\", dropout=0.5\n        )\n\n        # decoder\n        self.decoder3 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec3\", dropout=0.4\n        )\n        self.decoder2 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec2\", dropout=0.3\n        )\n        self.decoder1 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec1\", dropout=0.2\n        )\n\n        # final layer\n        self.final_conv = nn.Conv2d(\n            num_filters, num_classes, kernel_size=1, padding=0, bias=False\n        )\n\n    @staticmethod\n    def conv_block(in_channels, out_channels, kernel_size, name, num, dropout=0):\n        layers = {\n            f\"{name}_conv{num}\": Conv2dSame(in_channels, out_channels, kernel_size),\n            f\"{name}_bn{num}\": nn.BatchNorm2d(out_channels),\n            f\"{name}_relu{num}\": nn.ReLU(inplace=True),\n        }\n        if dropout > 0:\n            layers[f\"{name}_dropout\"] = nn.Dropout(p=dropout)\n\n        return nn.Sequential(OrderedDict(layers))\n\n    @staticmethod\n    def _block(in_channels, out_channels, kernel_size, name, dropout=0):\n        conv1 = EddyNet.conv_block(in_channels, out_channels, kernel_size, name, 1)\n        conv2 = EddyNet.conv_block(\n            out_channels, out_channels, kernel_size, name, 2, dropout=dropout\n        )\n        return nn.Sequential(conv1, conv2)\n\n    @staticmethod\n    def decoder_block(in_channels, out_channels, kernel_size, name, dropout=0):\n        return EddyNet._block(in_channels, out_channels, kernel_size, name, dropout)\n\n    def forward(self, x):\n        # encoder\n        enc1 = self.encoder1(x)\n        pool1 = self.pool1(enc1)\n\n        enc2 = self.encoder2(pool1)\n        pool2 = self.pool2(enc2)\n\n        enc3 = self.encoder3(pool2)\n        pool3 = self.pool3(enc3)\n\n        # bottleneck?\n        enc4 = self.encoder4(pool3)\n\n        # decoder\n        dec3 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(enc4)\n        dec3 = torch.cat((dec3, enc3), dim=1)\n        dec3 = self.decoder3(dec3)\n\n        dec2 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec3)\n        dec2 = torch.cat((dec2, enc2), dim=1)\n        dec2 = self.decoder2(dec2)\n\n        dec1 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec2)\n        dec1 = torch.cat((dec1, enc1), dim=1)\n        dec1 = self.decoder1(dec1)\n\n        # final layer\n        final = self.final_conv(dec1)\n\n        # softmax\n        final = nn.Softmax(dim=1)(final)\n\n        return final\n\n\nclass Conv2dSame(nn.Module):\n    \"\"\"Manual convolution with same padding\n    https://discuss.pytorch.org/t/same-padding-equivalent-in-pytorch/85121/9\n    Although PyTorch >= 1.10.0 supports ``padding='same'`` as a keyword\n    argument, this does not export to CoreML as of coremltools 5.1.0,\n    so we need to implement the internal torch logic manually.\n\n    Currently the ``RuntimeError`` is\n\n    \"PyTorch convert function for op '_convolution_mode' not implemented\"\n    \"\"\"\n\n    def __init__(\n        self, in_channels, out_channels, kernel_size, stride=1, dilation=1, **kwargs\n    ):\n        \"\"\"Wrap base convolution layer\n\n        See official PyTorch documentation for parameter details\n        https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n        \"\"\"\n        super().__init__()\n        self.conv = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            dilation=dilation,\n            **kwargs,\n        )\n\n        # Setup internal representations\n        kernel_size_ = _pair(kernel_size)\n        dilation_ = _pair(dilation)\n        self._reversed_padding_repeated_twice = [0, 0] * len(kernel_size_)\n\n        # Follow the logic from ``nn/modules/conv.py:_ConvNd``\n        for d, k, i in zip(\n            dilation_, kernel_size_, range(len(kernel_size_) - 1, -1, -1)\n        ):\n            total_padding = d * (k - 1)\n            left_pad = total_padding // 2\n            self._reversed_padding_repeated_twice[2 * i] = left_pad\n            self._reversed_padding_repeated_twice[2 * i + 1] = total_padding - left_pad\n\n    def forward(self, imgs):\n        \"\"\"Setup padding so same spatial dimensions are returned\n\n        All shapes (input/output) are ``(N, C, W, H)`` convention\n\n        :param torch.Tensor imgs:\n        :return torch.Tensor:\n        \"\"\"\n        padded = F.pad(imgs, self._reversed_padding_repeated_twice)\n        return self.conv(padded)\n\n\ndef _ntuple(n):\n    \"\"\"Copy from PyTorch since internal function is not importable\n\n    See ``nn/modules/utils.py:6``\n    \"\"\"\n\n    def parse(x):\n        if isinstance(x, collections.abc.Iterable):\n            return tuple(x)\n        return tuple(repeat(x, n))\n\n    return parse\n\n\n_pair = _ntuple(2)",
  "history_output" : "",
  "history_begin_time" : 1678125468843,
  "history_end_time" : 1678125471230,
  "history_notes" : null,
  "history_process" : "xdwq7e",
  "host_id" : null,
  "indicator" : "Done"
},{
  "history_id" : "99825wcvbyb",
  "history_input" : "import collections\nfrom itertools import repeat\nfrom typing import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass EddyNet(nn.Module):\n    \"\"\"\n    PyTorch implementation of EddyNet from Lguensat et al. (2018)\n    Original implementation in TensorFlow: https://github.com/redouanelg/EddyNet\n    \"\"\"\n    def __init__(self, num_classes, num_filters, kernel_size):\n        super(EddyNet, self).__init__()\n        # encoder\n        self.encoder1 = EddyNet._block(1, num_filters, kernel_size, \"enc1\", dropout=0.2)\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder2 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc2\", dropout=0.3\n        )\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder3 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc3\", dropout=0.4\n        )\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder4 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc4\", dropout=0.5\n        )\n\n        # decoder\n        self.decoder3 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec3\", dropout=0.4\n        )\n        self.decoder2 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec2\", dropout=0.3\n        )\n        self.decoder1 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec1\", dropout=0.2\n        )\n\n        # final layer\n        self.final_conv = nn.Conv2d(\n            num_filters, num_classes, kernel_size=1, padding=0, bias=False\n        )\n\n    @staticmethod\n    def conv_block(in_channels, out_channels, kernel_size, name, num, dropout=0):\n        layers = {\n            f\"{name}_conv{num}\": Conv2dSame(in_channels, out_channels, kernel_size),\n            f\"{name}_bn{num}\": nn.BatchNorm2d(out_channels),\n            f\"{name}_relu{num}\": nn.ReLU(inplace=True),\n        }\n        if dropout > 0:\n            layers[f\"{name}_dropout\"] = nn.Dropout(p=dropout)\n\n        return nn.Sequential(OrderedDict(layers))\n\n    @staticmethod\n    def _block(in_channels, out_channels, kernel_size, name, dropout=0):\n        conv1 = EddyNet.conv_block(in_channels, out_channels, kernel_size, name, 1)\n        conv2 = EddyNet.conv_block(\n            out_channels, out_channels, kernel_size, name, 2, dropout=dropout\n        )\n        return nn.Sequential(conv1, conv2)\n\n    @staticmethod\n    def decoder_block(in_channels, out_channels, kernel_size, name, dropout=0):\n        return EddyNet._block(in_channels, out_channels, kernel_size, name, dropout)\n\n    def forward(self, x):\n        # encoder\n        enc1 = self.encoder1(x)\n        pool1 = self.pool1(enc1)\n\n        enc2 = self.encoder2(pool1)\n        pool2 = self.pool2(enc2)\n\n        enc3 = self.encoder3(pool2)\n        pool3 = self.pool3(enc3)\n\n        # bottleneck?\n        enc4 = self.encoder4(pool3)\n\n        # decoder\n        dec3 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(enc4)\n        dec3 = torch.cat((dec3, enc3), dim=1)\n        dec3 = self.decoder3(dec3)\n\n        dec2 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec3)\n        dec2 = torch.cat((dec2, enc2), dim=1)\n        dec2 = self.decoder2(dec2)\n\n        dec1 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec2)\n        dec1 = torch.cat((dec1, enc1), dim=1)\n        dec1 = self.decoder1(dec1)\n\n        # final layer\n        final = self.final_conv(dec1)\n\n        # softmax\n        final = nn.Softmax(dim=1)(final)\n\n        return final\n\n\nclass Conv2dSame(nn.Module):\n    \"\"\"Manual convolution with same padding\n    https://discuss.pytorch.org/t/same-padding-equivalent-in-pytorch/85121/9\n    Although PyTorch >= 1.10.0 supports ``padding='same'`` as a keyword\n    argument, this does not export to CoreML as of coremltools 5.1.0,\n    so we need to implement the internal torch logic manually.\n\n    Currently the ``RuntimeError`` is\n\n    \"PyTorch convert function for op '_convolution_mode' not implemented\"\n    \"\"\"\n\n    def __init__(\n        self, in_channels, out_channels, kernel_size, stride=1, dilation=1, **kwargs\n    ):\n        \"\"\"Wrap base convolution layer\n\n        See official PyTorch documentation for parameter details\n        https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n        \"\"\"\n        super().__init__()\n        self.conv = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            dilation=dilation,\n            **kwargs,\n        )\n\n        # Setup internal representations\n        kernel_size_ = _pair(kernel_size)\n        dilation_ = _pair(dilation)\n        self._reversed_padding_repeated_twice = [0, 0] * len(kernel_size_)\n\n        # Follow the logic from ``nn/modules/conv.py:_ConvNd``\n        for d, k, i in zip(\n            dilation_, kernel_size_, range(len(kernel_size_) - 1, -1, -1)\n        ):\n            total_padding = d * (k - 1)\n            left_pad = total_padding // 2\n            self._reversed_padding_repeated_twice[2 * i] = left_pad\n            self._reversed_padding_repeated_twice[2 * i + 1] = total_padding - left_pad\n\n    def forward(self, imgs):\n        \"\"\"Setup padding so same spatial dimensions are returned\n\n        All shapes (input/output) are ``(N, C, W, H)`` convention\n\n        :param torch.Tensor imgs:\n        :return torch.Tensor:\n        \"\"\"\n        padded = F.pad(imgs, self._reversed_padding_repeated_twice)\n        return self.conv(padded)\n\n\ndef _ntuple(n):\n    \"\"\"Copy from PyTorch since internal function is not importable\n\n    See ``nn/modules/utils.py:6``\n    \"\"\"\n\n    def parse(x):\n        if isinstance(x, collections.abc.Iterable):\n            return tuple(x)\n        return tuple(repeat(x, n))\n\n    return parse\n\n\n_pair = _ntuple(2)",
  "history_output" : "",
  "history_begin_time" : 1677776104354,
  "history_end_time" : 1677776107987,
  "history_notes" : null,
  "history_process" : "xdwq7e",
  "host_id" : "ycru82",
  "indicator" : "Done"
},{
  "history_id" : "sbui2c92n35",
  "history_input" : "import collections\nfrom itertools import repeat\nfrom typing import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass EddyNet(nn.Module):\n    \"\"\"\n    PyTorch implementation of EddyNet from Lguensat et al. (2018)\n    Original implementation in TensorFlow: https://github.com/redouanelg/EddyNet\n    \"\"\"\n    def __init__(self, num_classes, num_filters, kernel_size):\n        super(EddyNet, self).__init__()\n        # encoder\n        self.encoder1 = EddyNet._block(1, num_filters, kernel_size, \"enc1\", dropout=0.2)\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder2 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc2\", dropout=0.3\n        )\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder3 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc3\", dropout=0.4\n        )\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder4 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc4\", dropout=0.5\n        )\n\n        # decoder\n        self.decoder3 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec3\", dropout=0.4\n        )\n        self.decoder2 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec2\", dropout=0.3\n        )\n        self.decoder1 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec1\", dropout=0.2\n        )\n\n        # final layer\n        self.final_conv = nn.Conv2d(\n            num_filters, num_classes, kernel_size=1, padding=0, bias=False\n        )\n\n    @staticmethod\n    def conv_block(in_channels, out_channels, kernel_size, name, num, dropout=0):\n        layers = {\n            f\"{name}_conv{num}\": Conv2dSame(in_channels, out_channels, kernel_size),\n            f\"{name}_bn{num}\": nn.BatchNorm2d(out_channels),\n            f\"{name}_relu{num}\": nn.ReLU(inplace=True),\n        }\n        if dropout > 0:\n            layers[f\"{name}_dropout\"] = nn.Dropout(p=dropout)\n\n        return nn.Sequential(OrderedDict(layers))\n\n    @staticmethod\n    def _block(in_channels, out_channels, kernel_size, name, dropout=0):\n        conv1 = EddyNet.conv_block(in_channels, out_channels, kernel_size, name, 1)\n        conv2 = EddyNet.conv_block(\n            out_channels, out_channels, kernel_size, name, 2, dropout=dropout\n        )\n        return nn.Sequential(conv1, conv2)\n\n    @staticmethod\n    def decoder_block(in_channels, out_channels, kernel_size, name, dropout=0):\n        return EddyNet._block(in_channels, out_channels, kernel_size, name, dropout)\n\n    def forward(self, x):\n        # encoder\n        enc1 = self.encoder1(x)\n        pool1 = self.pool1(enc1)\n\n        enc2 = self.encoder2(pool1)\n        pool2 = self.pool2(enc2)\n\n        enc3 = self.encoder3(pool2)\n        pool3 = self.pool3(enc3)\n\n        # bottleneck?\n        enc4 = self.encoder4(pool3)\n\n        # decoder\n        dec3 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(enc4)\n        dec3 = torch.cat((dec3, enc3), dim=1)\n        dec3 = self.decoder3(dec3)\n\n        dec2 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec3)\n        dec2 = torch.cat((dec2, enc2), dim=1)\n        dec2 = self.decoder2(dec2)\n\n        dec1 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec2)\n        dec1 = torch.cat((dec1, enc1), dim=1)\n        dec1 = self.decoder1(dec1)\n\n        # final layer\n        final = self.final_conv(dec1)\n\n        # softmax\n        final = nn.Softmax(dim=1)(final)\n\n        return final\n\n\nclass Conv2dSame(nn.Module):\n    \"\"\"Manual convolution with same padding\n    https://discuss.pytorch.org/t/same-padding-equivalent-in-pytorch/85121/9\n    Although PyTorch >= 1.10.0 supports ``padding='same'`` as a keyword\n    argument, this does not export to CoreML as of coremltools 5.1.0,\n    so we need to implement the internal torch logic manually.\n\n    Currently the ``RuntimeError`` is\n\n    \"PyTorch convert function for op '_convolution_mode' not implemented\"\n    \"\"\"\n\n    def __init__(\n        self, in_channels, out_channels, kernel_size, stride=1, dilation=1, **kwargs\n    ):\n        \"\"\"Wrap base convolution layer\n\n        See official PyTorch documentation for parameter details\n        https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n        \"\"\"\n        super().__init__()\n        self.conv = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            dilation=dilation,\n            **kwargs,\n        )\n\n        # Setup internal representations\n        kernel_size_ = _pair(kernel_size)\n        dilation_ = _pair(dilation)\n        self._reversed_padding_repeated_twice = [0, 0] * len(kernel_size_)\n\n        # Follow the logic from ``nn/modules/conv.py:_ConvNd``\n        for d, k, i in zip(\n            dilation_, kernel_size_, range(len(kernel_size_) - 1, -1, -1)\n        ):\n            total_padding = d * (k - 1)\n            left_pad = total_padding // 2\n            self._reversed_padding_repeated_twice[2 * i] = left_pad\n            self._reversed_padding_repeated_twice[2 * i + 1] = total_padding - left_pad\n\n    def forward(self, imgs):\n        \"\"\"Setup padding so same spatial dimensions are returned\n\n        All shapes (input/output) are ``(N, C, W, H)`` convention\n\n        :param torch.Tensor imgs:\n        :return torch.Tensor:\n        \"\"\"\n        padded = F.pad(imgs, self._reversed_padding_repeated_twice)\n        return self.conv(padded)\n\n\ndef _ntuple(n):\n    \"\"\"Copy from PyTorch since internal function is not importable\n\n    See ``nn/modules/utils.py:6``\n    \"\"\"\n\n    def parse(x):\n        if isinstance(x, collections.abc.Iterable):\n            return tuple(x)\n        return tuple(repeat(x, n))\n\n    return parse\n\n\n_pair = _ntuple(2)",
  "history_output" : "Running",
  "history_begin_time" : 1677775001906,
  "history_end_time" : 1677775014189,
  "history_notes" : null,
  "history_process" : "xdwq7e",
  "host_id" : "ycru82",
  "indicator" : "Done"
},]
