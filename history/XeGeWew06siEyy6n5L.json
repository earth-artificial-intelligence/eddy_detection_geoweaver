[{
  "history_id" : "8dl57zn2u6w",
  "history_input" : "# All frequently used dependency are here\n\nimport os\nimport numpy as np\nimport logging as logger\n\nfrom datetime import datetime\nfrom matplotlib import pyplot as plt\nfrom py_eddy_tracker import data\nfrom py_eddy_tracker.dataset.grid import RegularGridDataset\nfrom copy import deepcopy\n",
  "history_output" : "",
  "history_begin_time" : 1680667092156,
  "history_end_time" : 1680667096060,
  "history_notes" : null,
  "history_process" : "0ajbp0",
  "host_id" : "c2lqcn",
  "indicator" : "Done"
},{
  "history_id" : "vjtwhstons2",
  "history_input" : "from dependency import *\n\ndata_root = os.path.join(os.path.expanduser(\"~\"), \"ML_eddies\")\ntrain_folder = os.path.join(data_root, \"cds_ssh_1998-2018_10day_interval\")\ntest_folder = os.path.join(data_root, \"cds_ssh_2019_10day_interval\")\n\nexample_file = os.path.join(test_folder, \"dt_global_twosat_phy_l4_20190110_vDT2021.nc\")",
  "history_output" : "Running",
  "history_begin_time" : 1680667115327,
  "history_end_time" : 1680667118241,
  "history_notes" : null,
  "history_process" : "0ps7es",
  "host_id" : "c2lqcn",
  "indicator" : "Done"
},{
  "history_id" : "m4fokwyxycd",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1680667091213,
  "history_end_time" : 1680667091213,
  "history_notes" : null,
  "history_process" : "ag4g86",
  "host_id" : "c2lqcn",
  "indicator" : "Skipped"
},{
  "history_id" : "r77e2enrld0",
  "history_input" : "# sea surface height (SSH preprocessing)\n\n\nfrom dependency import *\nfrom plot_utils import plot_variable, save_fig_and_relesase_memory\nfrom data_loader import example_file\n\n\ndate = datetime(2019, 1, 1)\ng = RegularGridDataset(example_file, \"longitude\", \"latitude\")\n\nax, m, fig = plot_variable(\n    g,\n    \"adt\",\n    f\"ADT (m) before high-pass filter\",\n    vmin=-0.15,\n    vmax=0.15,\n)\n\nsave_fig_and_relesase_memory(ax, m, fig)\n\nwavelength_km = 700\ng_filtered = deepcopy(g)\ng_filtered.bessel_high_filter(\"adt\", wavelength_km)\n\nax, m, fig = plot_variable(\n    g_filtered,\n    \"adt\",\n    f\"ADT (m) filtered (Final: {wavelength_km} km)\",\n    vmin=-0.15,\n    vmax=0.15,\n)\n\nsave_fig_and_relesase_memory(ax, m, fig)\n\n",
  "history_output" : "We assume pixel position of grid is centered for /home/chetana/ML_eddies/cds_ssh_2019_10day_interval/dt_global_twosat_phy_l4_20190110_vDT2021.nc\nNo filtering above 85.000000 degrees of latitude\n",
  "history_begin_time" : 1680667119382,
  "history_end_time" : 1680667130859,
  "history_notes" : null,
  "history_process" : "nzlslh",
  "host_id" : "c2lqcn",
  "indicator" : "Done"
},{
  "history_id" : "8i1flliauhv",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1680667091215,
  "history_end_time" : 1680667091215,
  "history_notes" : null,
  "history_process" : "jajowz",
  "host_id" : "c2lqcn",
  "indicator" : "Skipped"
},{
  "history_id" : "6wmonrm7khj",
  "history_input" : "# Generate ground truth on a global scale helper functions\n\nimport multiprocessing\n\nfrom ground_truth_utils import *\n\ndef generate_masks_in_parallel(\n    files,\n    dates,\n    ssh_var=\"adt\",\n    u_var=\"ugosa\",\n    v_var=\"vgosa\",\n    high_pass_wavelength_km=700,\n    x_offset=-180,\n    y_offset=0,\n    num_processes=8,\n    plot=False,\n    save=True,\n    test=False,\n):\n    args = [\n        (file, date, ssh_var, u_var, v_var, high_pass_wavelength_km, x_offset, y_offset)\n        for file, date in zip(files, dates)\n    ]\n    pool = multiprocessing.Pool(processes=num_processes)\n    results = pool.starmap(generate_segmentation_mask_from_file, args)\n\n    vars_ = []\n    vars_filtered = []\n    masks = []\n    for result in results:\n        vars_.append(result[0])\n        vars_filtered.append(result[1])\n        masks.append(result[2])\n\n    # concatenate list into single numpy array and return\n    \n    masks = np.stack(masks, axis=0)\n    vars_ = np.stack(vars_, axis=0).astype(np.float32)\n    vars_filtered = np.stack(vars_filtered, axis=0).astype(np.float32)\n\n    if save:\n        # find common folder across all files\n        common_folder = os.path.commonpath(files)\n        if test is True:\n            common_folder = \"/home/chetana/ML_eddies/cds_ssh_2019_10day_interval\"\n        years = sorted(set([date.year for date in dates]))\n        year_str = f\"{years[0]}\" if len(years) == 1 else f\"{min(years)}-{max(years)}\"\n        save_path = os.path.join(\n            common_folder, f\"global_pet_masks_with_{ssh_var}_{year_str}.npz\"\n        )\n        np.savez_compressed(\n            save_path,\n            masks=masks,\n            dates=dates,\n            var=vars_,\n            var_filtered=vars_filtered,\n        )\n        print(f\"Saved masks to {save_path}\")\n\n    return vars_, vars_filtered, masks\n\n\nfrom itertools import product\n\n\ndef get_dates_and_files(years, months, days, folder, file_pattern):\n    \"\"\"\n    Given a filename pattern and a list of years months and days,\n    fill in the filename pattern with the date and return\n    a list of filenames and a list of associated `datetime` objects.\n\n    Args:\n        years (list): list of years, e.g., [1993, 1994, 1995, 1996]\n        months (list): list of months, e.g., [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n        days (list): list of days, e.g., [1, 10, 20, 30] for every 10th day\n        folder (str): folder where the files are located\n        file_pattern (str): filename pattern, e.g.,\n            \"dt_global_twosat_phy_l4_{year:04d}{month:02d}{day:02d}_vDT2021.nc\"\n\n    Returns:\n        files (list): full/absolute path to each netCDF file in the list of dates\n        dates (list): list of `datetime` objects formed from the combination of years, months and days\n    \"\"\"\n    dates, files = [], []\n    for y, m, d in product(years, months, days):  # cartesian product\n        try:\n            date = datetime(y, m, d)\n            file = os.path.join(folder, file_pattern.format(year=y, month=m, day=d))\n            dates.append(date)\n            files.append(file)\n        # catch ValueError thrown by datetime if date is not valid\n        except ValueError:\n            pass\n    years = f\"{years[0]}\" if len(years) == 1 else f\"{min(years)}-{max(years)}\"\n    print(f\"Found {len(dates)} files for {years}.\")\n    return dates, files\n",
  "history_output" : "Running",
  "history_begin_time" : 1680667132530,
  "history_end_time" : 1680667135568,
  "history_notes" : null,
  "history_process" : "zhsdwn",
  "host_id" : "c2lqcn",
  "indicator" : "Done"
},{
  "history_id" : "p2neubagpx2",
  "history_input" : "# Generate ground truth on a global scale helper functions\n\nimport multiprocessing\n\nfrom ground_truth_utils import *\n\ndef generate_masks_in_parallel(\n    files,\n    dates,\n    ssh_var=\"adt\",\n    u_var=\"ugosa\",\n    v_var=\"vgosa\",\n    high_pass_wavelength_km=700,\n    x_offset=-180,\n    y_offset=0,\n    num_processes=8,\n    plot=False,\n    save=True,\n    test=False,\n):\n    args = [\n        (file, date, ssh_var, u_var, v_var, high_pass_wavelength_km, x_offset, y_offset)\n        for file, date in zip(files, dates)\n    ]\n    pool = multiprocessing.Pool(processes=num_processes)\n    results = pool.starmap(generate_segmentation_mask_from_file, args)\n\n    vars_ = []\n    vars_filtered = []\n    masks = []\n    for result in results:\n        vars_.append(result[0])\n        vars_filtered.append(result[1])\n        masks.append(result[2])\n\n    # concatenate list into single numpy array and return\n    \n    masks = np.stack(masks, axis=0)\n    vars_ = np.stack(vars_, axis=0).astype(np.float32)\n    vars_filtered = np.stack(vars_filtered, axis=0).astype(np.float32)\n\n    if save:\n        # find common folder across all files\n        common_folder = os.path.commonpath(files)\n        if test is True:\n            common_folder = \"/home/chetana/ML_eddies/cds_ssh_2019_10day_interval\"\n        years = sorted(set([date.year for date in dates]))\n        year_str = f\"{years[0]}\" if len(years) == 1 else f\"{min(years)}-{max(years)}\"\n        save_path = os.path.join(\n            common_folder, f\"global_pet_masks_with_{ssh_var}_{year_str}.npz\"\n        )\n        np.savez_compressed(\n            save_path,\n            masks=masks,\n            dates=dates,\n            var=vars_,\n            var_filtered=vars_filtered,\n        )\n        print(f\"Saved masks to {save_path}\")\n\n    return vars_, vars_filtered, masks\n\n\nfrom itertools import product\n\n\ndef get_dates_and_files(years, months, days, folder, file_pattern):\n    \"\"\"\n    Given a filename pattern and a list of years months and days,\n    fill in the filename pattern with the date and return\n    a list of filenames and a list of associated `datetime` objects.\n\n    Args:\n        years (list): list of years, e.g., [1993, 1994, 1995, 1996]\n        months (list): list of months, e.g., [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n        days (list): list of days, e.g., [1, 10, 20, 30] for every 10th day\n        folder (str): folder where the files are located\n        file_pattern (str): filename pattern, e.g.,\n            \"dt_global_twosat_phy_l4_{year:04d}{month:02d}{day:02d}_vDT2021.nc\"\n\n    Returns:\n        files (list): full/absolute path to each netCDF file in the list of dates\n        dates (list): list of `datetime` objects formed from the combination of years, months and days\n    \"\"\"\n    dates, files = [], []\n    for y, m, d in product(years, months, days):  # cartesian product\n        try:\n            date = datetime(y, m, d)\n            file = os.path.join(folder, file_pattern.format(year=y, month=m, day=d))\n            dates.append(date)\n            files.append(file)\n        # catch ValueError thrown by datetime if date is not valid\n        except ValueError:\n            pass\n    years = f\"{years[0]}\" if len(years) == 1 else f\"{min(years)}-{max(years)}\"\n    print(f\"Found {len(dates)} files for {years}.\")\n    return dates, files\n",
  "history_output" : "",
  "history_begin_time" : 1680667091695,
  "history_end_time" : 1680667096043,
  "history_notes" : null,
  "history_process" : "zhsdwn",
  "host_id" : "c2lqcn",
  "indicator" : "Done"
},{
  "history_id" : "5vkugb96bfq",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1680667091220,
  "history_end_time" : 1680667091220,
  "history_notes" : null,
  "history_process" : "g85teu",
  "host_id" : "c2lqcn",
  "indicator" : "Skipped"
},{
  "history_id" : "bihwz23w4cu",
  "history_input" : "# This generates segmentation efficiently in parallel on a gloabal scale\n\n\nimport logging\n\nfrom generate_ground_truth_parallel_utils import *\nfrom data_loader import *\n\n\nlogging.getLogger(\"pet\").setLevel(logging.ERROR)\n\n# enter the AVISO filename pattern\n# year, month, and day in file_pattern will be filled in get_dates_and_files:\nfile_pattern = \"dt_global_twosat_phy_l4_{year:04d}{month:02d}{day:02d}_vDT2021.nc\"\n\n# training set: 1998 - 2018\ntrain_dates, train_files = get_dates_and_files(\n    range(1998, 2019), range(1, 2), [1], train_folder, file_pattern\n)\ntrain_adt, train_adt_filtered, train_masks = generate_masks_in_parallel(\n    train_files, train_dates\n)\n\n\n# test set: 2019\ntest_dates, test_files = get_dates_and_files(\n    [2019], range(1, 13), [10], test_folder, file_pattern\n)\ntest_adt, test_adt_filtered, test_masks = generate_masks_in_parallel(\n    test_files, test_dates\n)",
  "history_output" : "Found 21 files for 1998-2018.\n/home/chetana/anaconda3/envs/ranjan/lib/python3.10/site-packages/numpy/lib/function_base.py:4650: UserWarning: Warning: 'partition' will ignore the 'mask' of the MaskedArray.\n  arr.partition(\n/home/chetana/anaconda3/envs/ranjan/lib/python3.10/site-packages/numpy/lib/function_base.py:4650: UserWarning: Warning: 'partition' will ignore the 'mask' of the MaskedArray.\n  arr.partition(\n/home/chetana/anaconda3/envs/ranjan/lib/python3.10/site-packages/numpy/lib/function_base.py:4650: UserWarning: Warning: 'partition' will ignore the 'mask' of the MaskedArray.\n  arr.partition(\n/home/chetana/anaconda3/envs/ranjan/lib/python3.10/site-packages/numpy/lib/function_base.py:4650: UserWarning: Warning: 'partition' will ignore the 'mask' of the MaskedArray.\n  arr.partition(\n/home/chetana/anaconda3/envs/ranjan/lib/python3.10/site-packages/numpy/lib/function_base.py:4650: UserWarning: Warning: 'partition' will ignore the 'mask' of the MaskedArray.\n  arr.partition(\n/home/chetana/anaconda3/envs/ranjan/lib/python3.10/site-packages/numpy/lib/function_base.py:4650: UserWarning: Warning: 'partition' will ignore the 'mask' of the MaskedArray.\n  arr.partition(\n/home/chetana/anaconda3/envs/ranjan/lib/python3.10/site-packages/numpy/lib/function_base.py:4650: UserWarning: Warning: 'partition' will ignore the 'mask' of the MaskedArray.\n  arr.partition(\n/home/chetana/anaconda3/envs/ranjan/lib/python3.10/site-packages/numpy/lib/function_base.py:4650: UserWarning: Warning: 'partition' will ignore the 'mask' of the MaskedArray.\n  arr.partition(\nSaved masks to /home/chetana/ML_eddies/cds_ssh_1998-2018_10day_interval/global_pet_masks_with_adt_1998-2018.npz\nFound 12 files for 2019.\n/home/chetana/anaconda3/envs/ranjan/lib/python3.10/site-packages/numpy/lib/function_base.py:4650: UserWarning: Warning: 'partition' will ignore the 'mask' of the MaskedArray.\n  arr.partition(\n/home/chetana/anaconda3/envs/ranjan/lib/python3.10/site-packages/numpy/lib/function_base.py:4650: UserWarning: Warning: 'partition' will ignore the 'mask' of the MaskedArray.\n  arr.partition(\n/home/chetana/anaconda3/envs/ranjan/lib/python3.10/site-packages/numpy/lib/function_base.py:4650: UserWarning: Warning: 'partition' will ignore the 'mask' of the MaskedArray.\n  arr.partition(\n/home/chetana/anaconda3/envs/ranjan/lib/python3.10/site-packages/numpy/lib/function_base.py:4650: UserWarning: Warning: 'partition' will ignore the 'mask' of the MaskedArray.\n  arr.partition(\n/home/chetana/anaconda3/envs/ranjan/lib/python3.10/site-packages/numpy/lib/function_base.py:4650: UserWarning: Warning: 'partition' will ignore the 'mask' of the MaskedArray.\n  arr.partition(\n/home/chetana/anaconda3/envs/ranjan/lib/python3.10/site-packages/numpy/lib/function_base.py:4650: UserWarning: Warning: 'partition' will ignore the 'mask' of the MaskedArray.\n  arr.partition(\n/home/chetana/anaconda3/envs/ranjan/lib/python3.10/site-packages/numpy/lib/function_base.py:4650: UserWarning: Warning: 'partition' will ignore the 'mask' of the MaskedArray.\n  arr.partition(\n/home/chetana/anaconda3/envs/ranjan/lib/python3.10/site-packages/numpy/lib/function_base.py:4650: UserWarning: Warning: 'partition' will ignore the 'mask' of the MaskedArray.\n  arr.partition(\nSaved masks to /home/chetana/ML_eddies/cds_ssh_2019_10day_interval/global_pet_masks_with_adt_2019.npz\n",
  "history_begin_time" : 1680667136887,
  "history_end_time" : 1680667305352,
  "history_notes" : null,
  "history_process" : "q20jvx",
  "host_id" : "c2lqcn",
  "indicator" : "Done"
},{
  "history_id" : "40lpn31uv18",
  "history_input" : "# Use the segmask_and_ssh_utils to generate compress file\n\nfrom data_loader import *\nfrom generate_segmentation_in_parallel import *\nfrom segmask_and_ssh_utils import *\n\nlon_range = (-166, -134)\nlat_range = (14, 46)\n\ntrain_subset = subset_arrays(\n    train_masks,\n    train_adt,\n    train_adt_filtered,\n    train_dates,\n    lon_range,\n    lat_range,\n    plot=False,\n    resolution_deg=0.25,\n    save_folder=train_folder,\n)\n\ntest_subset = subset_arrays(\n    test_masks,\n    test_adt,\n    test_adt_filtered,\n    test_dates,\n    lon_range,\n    lat_range,\n    plot=True,\n    resolution_deg=0.25,\n    save_folder=test_folder,\n)",
  "history_output" : "Found 21 files for 1998-2018.\n/home/chetana/anaconda3/envs/ranjan/lib/python3.10/site-packages/numpy/lib/function_base.py:4650: UserWarning: Warning: 'partition' will ignore the 'mask' of the MaskedArray.\n  arr.partition(\n/home/chetana/anaconda3/envs/ranjan/lib/python3.10/site-packages/numpy/lib/function_base.py:4650: UserWarning: Warning: 'partition' will ignore the 'mask' of the MaskedArray.\n  arr.partition(\n/home/chetana/anaconda3/envs/ranjan/lib/python3.10/site-packages/numpy/lib/function_base.py:4650: UserWarning: Warning: 'partition' will ignore the 'mask' of the MaskedArray.\n  arr.partition(\n/home/chetana/anaconda3/envs/ranjan/lib/python3.10/site-packages/numpy/lib/function_base.py:4650: UserWarning: Warning: 'partition' will ignore the 'mask' of the MaskedArray.\n  arr.partition(\n/home/chetana/anaconda3/envs/ranjan/lib/python3.10/site-packages/numpy/lib/function_base.py:4650: UserWarning: Warning: 'partition' will ignore the 'mask' of the MaskedArray.\n  arr.partition(\n/home/chetana/anaconda3/envs/ranjan/lib/python3.10/site-packages/numpy/lib/function_base.py:4650: UserWarning: Warning: 'partition' will ignore the 'mask' of the MaskedArray.\n  arr.partition(\n/home/chetana/anaconda3/envs/ranjan/lib/python3.10/site-packages/numpy/lib/function_base.py:4650: UserWarning: Warning: 'partition' will ignore the 'mask' of the MaskedArray.\n  arr.partition(\n/home/chetana/anaconda3/envs/ranjan/lib/python3.10/site-packages/numpy/lib/function_base.py:4650: UserWarning: Warning: 'partition' will ignore the 'mask' of the MaskedArray.\n  arr.partition(\nSaved masks to /home/chetana/ML_eddies/cds_ssh_1998-2018_10day_interval/global_pet_masks_with_adt_1998-2018.npz\nFound 12 files for 2019.\n/home/chetana/anaconda3/envs/ranjan/lib/python3.10/site-packages/numpy/lib/function_base.py:4650: UserWarning: Warning: 'partition' will ignore the 'mask' of the MaskedArray.\n  arr.partition(\n/home/chetana/anaconda3/envs/ranjan/lib/python3.10/site-packages/numpy/lib/function_base.py:4650: UserWarning: Warning: 'partition' will ignore the 'mask' of the MaskedArray.\n  arr.partition(\n/home/chetana/anaconda3/envs/ranjan/lib/python3.10/site-packages/numpy/lib/function_base.py:4650: UserWarning: Warning: 'partition' will ignore the 'mask' of the MaskedArray.\n  arr.partition(\n/home/chetana/anaconda3/envs/ranjan/lib/python3.10/site-packages/numpy/lib/function_base.py:4650: UserWarning: Warning: 'partition' will ignore the 'mask' of the MaskedArray.\n  arr.partition(\n/home/chetana/anaconda3/envs/ranjan/lib/python3.10/site-packages/numpy/lib/function_base.py:4650: UserWarning: Warning: 'partition' will ignore the 'mask' of the MaskedArray.\n  arr.partition(\n/home/chetana/anaconda3/envs/ranjan/lib/python3.10/site-packages/numpy/lib/function_base.py:4650: UserWarning: Warning: 'partition' will ignore the 'mask' of the MaskedArray.\n  arr.partition(\n/home/chetana/anaconda3/envs/ranjan/lib/python3.10/site-packages/numpy/lib/function_base.py:4650: UserWarning: Warning: 'partition' will ignore the 'mask' of the MaskedArray.\n  arr.partition(\n/home/chetana/anaconda3/envs/ranjan/lib/python3.10/site-packages/numpy/lib/function_base.py:4650: UserWarning: Warning: 'partition' will ignore the 'mask' of the MaskedArray.\n  arr.partition(\nSaved masks to /home/chetana/ML_eddies/cds_ssh_2019_10day_interval/global_pet_masks_with_adt_2019.npz\nSaved mask subset to /home/chetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz\nSaved mask subset to /home/chetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz\n",
  "history_begin_time" : 1680667306163,
  "history_end_time" : 1680667471324,
  "history_notes" : null,
  "history_process" : "yddm1o",
  "host_id" : "c2lqcn",
  "indicator" : "Done"
},{
  "history_id" : "899xakwdf1a",
  "history_input" : "from dependency import *\nfrom unzip_utils import *\nfrom get_data import *\n\n\nos.chdir(os.path.expanduser(\"~\"))\ncurrent_working_dir = os.getcwd()\nprint(current_working_dir)\n\n# Directory names\nroot_dir_name = \"ML_eddies\"\ntrain_dir_name = \"cds_ssh_1998-2018_10day_interval\"\ntest_dir_name = \"cds_ssh_2019_10day_interval\"\n\n# Build dir paths\nroot_path = os.path.join(current_working_dir, root_dir_name)\ntrain_path = os.path.join(root_path, train_dir_name)\ntest_path= os.path.join(root_path, test_dir_name)\n\n# Check if dir exists\nis_root_dir_exists = os.path.exists(root_path)\nis_train_dir_exists = os.path.exists(train_path)\nis_test_dir_exists = os.path.exists(test_path)\n\n\ndef create_directory(directory_name):\n    try:\n        os.mkdir(directory_name)\n        logger.info(\"Successfully created folder\")\n    except:\n        logger.error(\"Something went wrong while creating folder\")\n\n\n\nif is_root_dir_exists != True:\n    print(root_path)\n    create_directory(root_path)\n    print(\"created:\",root_path)\n    create_directory(train_path)\n    create_directory(test_path)\n    train_file, test_file = download_data()\n\n    unzip_file( os.path.join(current_working_dir,train_file), train_path)\n    unzip_file( os.path.join(current_working_dir,test_file), test_path)\n\n\nif is_root_dir_exists and is_train_dir_exists != True:\n    create_directory(\"cds_ssh_1998-2018_10day_interval\")\n    train_file = download_train_data()\n    unzip_file( os.path.join(current_working_dir,train_file), train_path)\n\nif  is_root_dir_exists and is_test_dir_exists != True:\n    create_directory(\"cds_ssh_2019_10day_interval\")\n    test_file = download_test_data()\n    unzip_file( os.path.join(current_working_dir,test_file), test_path)\n\n",
  "history_output" : "/home/chetana\n",
  "history_begin_time" : 1680667109224,
  "history_end_time" : 1680667113349,
  "history_notes" : null,
  "history_process" : "dhjb5i",
  "host_id" : "c2lqcn",
  "indicator" : "Done"
},{
  "history_id" : "qecjllh4bz9",
  "history_input" : "from dependency import logger\nfrom zipfile import ZipFile\n\ndef unzip_file(zip_file_path, extract_to_path):\n    try:\n        with ZipFile(zip_file_path) as zip_file_object:          \n            zip_file_object.extractall(extract_to_path)\n            \n    except:\n        logger.error(\"Something went wrong while extracting File\" )\n",
  "history_output" : "Running",
  "history_begin_time" : 1680667099252,
  "history_end_time" : 1680667102191,
  "history_notes" : null,
  "history_process" : "zbt6sg",
  "host_id" : "c2lqcn",
  "indicator" : "Done"
},{
  "history_id" : "soac1wfttc3",
  "history_input" : "from dependency import logger\n\nimport cdsapi\n\nclient = cdsapi.Client()\n\n\ndef download_train_data():\n    try:\n        client.retrieve(\n            'satellite-sea-level-global',\n            {\n                'version': 'vDT2021',\n                'variable': 'all',\n                'format': 'zip',\n                'year': [\n                    '1998', '1999', '2000',\n                    '2001', '2002', '2003',\n                    '2004', '2005', '2006',\n                    '2007', '2008', '2009',\n                    '2010', '2011', '2012',\n                    '2013', '2014', '2015',\n                    '2016', '2017', '2018',\n                ],\n                'month': [\n                    '01', '02', '03',\n                    '04', '05', '06',\n                    '07', '08', '09',\n                    '10', '11', '12',\n                ],\n                'day': ['01', '10', '20', '30'],\n            },\n            'train_data.zip')\n        return 'train_data.zip'\n    except:\n        logger.error(\"Something went wrong while downloading training data\")\n\n\ndef download_test_data():\n    try:\n        client.retrieve(\n            'satellite-sea-level-global',\n            {\n                'version': 'vDT2021',\n                'variable': 'all',\n                'format': 'zip',\n                'year': ['2019'],\n                'month': [\n                    '01', '02', '03',\n                    '04', '05', '06',\n                    '07', '08', '09',\n                    '10', '11', '12',\n                ],\n                'day': ['01', '10', '20', '30'],\n            },\n            'test_data.zip')\n        return 'test_data.zip'\n    except:\n        logger.error(\"Something went wrong while downloading test data\")\n\n\ndef download_data():\n    train_zip_file = download_train_data()\n    test_zip_file = download_test_data()\n    return train_zip_file, test_zip_file\n\n\n\ndef download_test_date(year, month, day):\n    if len(month) < 2:\n        month = '0'+month\n\n    if len(day) < 2:\n        day = '0'+day\n\n    fileName = year + \"_\" + month + \"_\" + day + \"_test.zip\"\n\n    try:\n        client.retrieve(\n            'satellite-sea-level-global',\n            {\n                'version': 'vDT2021',\n                'variable': 'all',\n                'format': 'zip',\n                'year': [str(year)],\n                'month': [str(month)],\n                'day': [str(day)],\n            },\n            fileName)\n        return fileName\n    except:\n        logger.error(\"Something went wrong while downloading daily test data\")\n",
  "history_output" : "",
  "history_begin_time" : 1680667104090,
  "history_end_time" : 1680667108126,
  "history_notes" : null,
  "history_process" : "g7a3zf",
  "host_id" : "c2lqcn",
  "indicator" : "Done"
},{
  "history_id" : "dcfkb815kyt",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1680667091225,
  "history_end_time" : 1680667091225,
  "history_notes" : null,
  "history_process" : "mnmaq1",
  "host_id" : "c2lqcn",
  "indicator" : "Skipped"
},{
  "history_id" : "22mfwwvv2d7",
  "history_input" : "from dependency import os\n\ndata_root = os.path.join(os.path.expanduser(\"~\"), \"ML_eddies\")\ntrain_folder = os.path.join(data_root, \"cds_ssh_1998-2018_10day_interval\")\nval_folder = os.path.join(data_root, \"cds_ssh_2019_10day_interval\")\ntrain_file = os.path.join(train_folder, \"subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz\")\nval_file = os.path.join(val_folder, \"subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz\")",
  "history_output" : "Running",
  "history_begin_time" : 1680667473218,
  "history_end_time" : 1680667476154,
  "history_notes" : null,
  "history_process" : "w3hmlz",
  "host_id" : "c2lqcn",
  "indicator" : "Done"
},{
  "history_id" : "mvtiqt7mdxz",
  "history_input" : "from get_device_config import *\nfrom link_npz_files import *\nfrom torch_data_loader_utils import get_eddy_dataloader\n\n# set binary = false if we want to distinguish between cyclonic and anticyclonic\nbinary = False\nnum_classes = 2 if binary else 3\ntrain_loader, _ = get_eddy_dataloader(train_file, binary=binary, batch_size=batch_size)\nval_loader, _ = get_eddy_dataloader(\n    val_file, binary=binary, batch_size=batch_size, shuffle=False\n)",
  "history_output" : "Read 21 samples from /home/chetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 12 samples from /home/chetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\n",
  "history_begin_time" : 1680667477356,
  "history_end_time" : 1680667482749,
  "history_notes" : null,
  "history_process" : "28zx21",
  "host_id" : "c2lqcn",
  "indicator" : "Done"
},{
  "history_id" : "xxo29qk4lsc",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1680667091228,
  "history_end_time" : 1680667091228,
  "history_notes" : null,
  "history_process" : "d6b94y",
  "host_id" : "c2lqcn",
  "indicator" : "Skipped"
},{
  "history_id" : "sv2ohbztotq",
  "history_input" : "from convert_to_pytorch_data_loader import *\n\nimport numpy as np\ntrain_masks = train_loader.dataset.masks.copy()\nclass_frequency = np.bincount(train_masks.flatten())\ntotal_pixels = sum(class_frequency)\n\n\nprint(\n    f\"Total number of pixels in training set: {total_pixels/1e6:.2f} megapixels\"\n    f\" across {len(train_masks)} SSH maps\\\\n\"\n    f\"Number of pixels that are not eddies: {class_frequency[0]/1e6:.2f} megapixels \"\n    f\"({class_frequency[0]/total_pixels * 100:.2f}%)\\\\n\"\n    f\"Number of pixels that are anticyclonic eddies: {class_frequency[1]/1e6:.2f} megapixels \"\n    f\"({class_frequency[1]/total_pixels * 100:.2f}%)\\\\n\"\n    f\"Number of pixels that are cyclonic eddies: {class_frequency[2]/1e6:.2f} megapixels \"\n    f\"({class_frequency[2]/total_pixels * 100:.2f}%)\\\\n\"\n)",
  "history_output" : "Read 21 samples from /home/chetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 12 samples from /home/chetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\nTotal number of pixels in training set: 0.34 megapixels across 21 SSH maps\nNumber of pixels that are not eddies: 0.26 megapixels (74.33%)\nNumber of pixels that are anticyclonic eddies: 0.04 megapixels (13.04%)\nNumber of pixels that are cyclonic eddies: 0.04 megapixels (12.63%)\n\n",
  "history_begin_time" : 1680667483192,
  "history_end_time" : 1680667488853,
  "history_notes" : null,
  "history_process" : "mh6f0e",
  "host_id" : "c2lqcn",
  "indicator" : "Done"
},{
  "history_id" : "eh0yrvh4myb",
  "history_input" : "import torch\nfrom data_utils import EddyNet\nfrom convert_to_pytorch_data_loader import *\n\nnum_classes = 2 if binary else 3\nmodel_name = \"eddynet\"  # we'll log this in Tensorboard\nmodel = EddyNet(num_classes, num_filters=16, kernel_size=3)\nif torch.cuda.is_available():\n    model.to(device=\"cuda\")",
  "history_output" : "Read 21 samples from /home/chetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 12 samples from /home/chetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\n",
  "history_begin_time" : 1680667489781,
  "history_end_time" : 1680667495221,
  "history_notes" : null,
  "history_process" : "0w1lsj",
  "host_id" : "c2lqcn",
  "indicator" : "Done"
},{
  "history_id" : "gaq446rbf89",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1680667091230,
  "history_end_time" : 1680667091230,
  "history_notes" : null,
  "history_process" : "suoxrn",
  "host_id" : "c2lqcn",
  "indicator" : "Skipped"
},{
  "history_id" : "oe1mjbfhppt",
  "history_input" : "import torch\n\nloss_fn = torch.nn.CrossEntropyLoss()",
  "history_output" : "",
  "history_begin_time" : 1680667495880,
  "history_end_time" : 1680667499302,
  "history_notes" : null,
  "history_process" : "ax7g0d",
  "host_id" : "c2lqcn",
  "indicator" : "Done"
},{
  "history_id" : "7i67ct5tteb",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1680667091231,
  "history_end_time" : 1680667091231,
  "history_notes" : null,
  "history_process" : "xdwq7e",
  "host_id" : "c2lqcn",
  "indicator" : "Skipped"
},{
  "history_id" : "9tjmrtdugzn",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1680667091232,
  "history_end_time" : 1680667091232,
  "history_notes" : null,
  "history_process" : "uf6vbr",
  "host_id" : "c2lqcn",
  "indicator" : "Skipped"
},{
  "history_id" : "h0nrrneqiym",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1680667091232,
  "history_end_time" : 1680667091232,
  "history_notes" : null,
  "history_process" : "cxz2e7",
  "host_id" : "c2lqcn",
  "indicator" : "Skipped"
},{
  "history_id" : "09d3i44y3gv",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1680667091232,
  "history_end_time" : 1680667091232,
  "history_notes" : null,
  "history_process" : "t0vkxi",
  "host_id" : "c2lqcn",
  "indicator" : "Skipped"
},{
  "history_id" : "uto2dhd5r5l",
  "history_input" : "from model_training_utils import add_hparams, EarlyStopping\nfrom get_device_config import *\nfrom loss_function import *\nfrom set_optmizer_and_scheduler import *\nfrom set_summary_writer import *\nfrom model_utils import *\nfrom torch_metrics_utils import *\nfrom tqdm.auto import tqdm\n\n\n# create some aliases\nloss, opt, sched = loss_fn, optimizer, scheduler\n\ncheckpoint_path = os.path.join(tensorboard_dir, \"model_ckpt_{epoch}.pt\")\nearly_stopping = EarlyStopping(\n    patience=10,\n    path=checkpoint_path,\n    min_epochs=30,\n)\n\nprogress_bar = tqdm(range(num_epochs), desc=\"Training: \", unit=\"epoch(s)\")\nfor N in progress_bar:\n    train_loss, val_loss, train_m, val_m = run_epoch(\n        N,\n        model,\n        loss,\n        opt,\n        sched,\n        train_loader,\n        val_loader,\n        train_metrics,\n        val_metrics,\n        writer,\n    )\n\n    # update progress bar\n    train_m_copy = {f\"train_{k}\".lower(): v.cpu().numpy() for k, v in train_m.items()}\n    val_m_copy = {f\"val_{k}\".lower(): v.cpu().numpy() for k, v in val_m.items()}\n    progress_bar.set_postfix(**train_m_copy, **val_m_copy)\n\n    # early stopping when validation loss stops improving\n    early_stopping.path = checkpoint_path.format(epoch=N)\n    early_stopping(val_loss, model)\n    if early_stopping.early_stop:\n        print(\n            f\"Early stopping at epoch {N}\"\n            f\" with validation loss {val_loss:.3f}\"\n            f\" and training loss {train_loss:.3f}\"\n        )\n        break\n\n    # TODO (homework): save checkpoint every 10 epochs\n\n# add hyperparameters and corresponding results to tensorboard HParams table\nhparam_dict = {\n    \"backbone\": model_name,\n    \"num_epochs\": num_epochs,\n    \"batch_size\": batch_size,\n    \"num_classes\": num_classes,\n    \"binary_mask\": binary,\n    \"optimizer\": optimizer.__class__.__name__,\n    \"max_lr\": max_lr,\n    \"loss_function\": loss_fn.__class__.__name__,\n}\nprint(train_m)\nmetrics_dict = {\n    \"train/end_epoch\": N,\n    \"train/loss\": train_loss,\n    \"train/Accuracy\": train_m[\"MulticlassAccuracy\"],\n    \"val/loss\": val_loss,\n    \"val/Accuracy\": val_m[\"MulticlassAccuracy\"],\n}\nadd_hparams(writer, hparam_dict, metrics_dict, epoch_num=N)\nwriter.close()\n\n# save model to tensorboard folder\nmodel_path = os.path.join(tensorboard_dir, \"model_ckpt_final.pt\")\nprint(model_path)\ntorch.save(model.state_dict(), model_path)",
  "history_output" : "Read 21 samples from /home/chetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 12 samples from /home/chetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\n======================================================================\nWriting Tensorboard logs to /home/chetana/tensorboard/2023-04-05_04-05\n======================================================================\n\nTraining:   0%|                                   | 0/250 [00:00<?, ?epoch(s)/s]\nTraining:   0%| | 0/250 [00:05<?, ?epoch(s)/s, train_multiclassaccuracy=0.237458\nTraining:   0%| | 1/250 [00:05<22:39,  5.46s/epoch(s), train_multiclassaccuracy=\nTraining:   0%| | 1/250 [00:10<22:39,  5.46s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 2/250 [00:10<20:54,  5.06s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 2/250 [00:15<20:54,  5.06s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 3/250 [00:15<20:39,  5.02s/epoch(s), train_multiclassaccuracy=\nTraining:   1%| | 3/250 [00:19<20:39,  5.02s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 4/250 [00:19<20:07,  4.91s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 4/250 [00:24<20:07,  4.91s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 5/250 [00:24<20:05,  4.92s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 5/250 [00:29<20:05,  4.92s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 6/250 [00:29<19:44,  4.85s/epoch(s), train_multiclassaccuracy=\nTraining:   2%| | 6/250 [00:34<19:44,  4.85s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 7/250 [00:34<19:43,  4.87s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 7/250 [00:39<19:43,  4.87s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 8/250 [00:39<19:35,  4.86s/epoch(s), train_multiclassaccuracy=\nTraining:   3%| | 8/250 [00:44<19:35,  4.86s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 9/250 [00:44<19:18,  4.81s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 9/250 [00:48<19:18,  4.81s/epoch(s), train_multiclassaccuracy=\nTraining:   4%| | 10/250 [00:48<19:20,  4.84s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 10/250 [00:53<19:20,  4.84s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 11/250 [00:53<19:09,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:   4%| | 11/250 [00:58<19:09,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 12/250 [00:58<19:08,  4.83s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 12/250 [01:03<19:08,  4.83s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 13/250 [01:03<18:54,  4.79s/epoch(s), train_multiclassaccuracy\nTraining:   5%| | 13/250 [01:08<18:54,  4.79s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 14/250 [01:08<18:54,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 14/250 [01:12<18:54,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 15/250 [01:12<18:39,  4.77s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 15/250 [01:17<18:39,  4.77s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 16/250 [01:17<18:39,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:   6%| | 16/250 [01:22<18:39,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 17/250 [01:22<18:23,  4.74s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 17/250 [01:27<18:23,  4.74s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 18/250 [01:27<18:31,  4.79s/epoch(s), train_multiclassaccuracy\nTraining:   7%| | 18/250 [01:31<18:31,  4.79s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 19/250 [01:31<18:19,  4.76s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 19/250 [01:36<18:19,  4.76s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 20/250 [01:36<18:19,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 20/250 [01:41<18:19,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 21/250 [01:41<18:15,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:   8%| | 21/250 [01:46<18:15,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 22/250 [01:46<18:02,  4.75s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 22/250 [01:50<18:02,  4.75s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 23/250 [01:50<18:01,  4.76s/epoch(s), train_multiclassaccuracy\nTraining:   9%| | 23/250 [01:55<18:01,  4.76s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 24/250 [01:55<17:47,  4.72s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 24/250 [02:00<17:47,  4.72s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 25/250 [02:00<17:48,  4.75s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 25/250 [02:05<17:48,  4.75s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 26/250 [02:05<17:37,  4.72s/epoch(s), train_multiclassaccuracy\nTraining:  10%| | 26/250 [02:09<17:37,  4.72s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 27/250 [02:09<17:42,  4.77s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 27/250 [02:14<17:42,  4.77s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 28/250 [02:14<17:31,  4.73s/epoch(s), train_multiclassaccuracy\nTraining:  11%| | 28/250 [02:19<17:31,  4.73s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 29/250 [02:19<17:31,  4.76s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 29/250 [02:24<17:31,  4.76s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 30/250 [02:24<17:21,  4.73s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 30/250 [02:28<17:21,  4.73s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 31/250 [02:28<17:25,  4.77s/epoch(s), train_multiclassaccuracy\nTraining:  12%| | 31/250 [02:33<17:25,  4.77s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 32/250 [02:33<17:25,  4.79s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 32/250 [02:38<17:25,  4.79s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 33/250 [02:38<17:11,  4.75s/epoch(s), train_multiclassaccuracy\nTraining:  13%|▏| 33/250 [02:43<17:11,  4.75s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 34/250 [02:43<17:12,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 34/250 [02:47<17:12,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 35/250 [02:47<16:59,  4.74s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 35/250 [02:52<16:59,  4.74s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 36/250 [02:52<17:02,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  14%|▏| 36/250 [02:57<17:02,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 37/250 [02:57<16:52,  4.75s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 37/250 [03:02<16:52,  4.75s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 38/250 [03:02<16:54,  4.79s/epoch(s), train_multiclassaccuracy\nTraining:  15%|▏| 38/250 [03:07<16:54,  4.79s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 39/250 [03:07<16:48,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 39/250 [03:11<16:48,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 40/250 [03:11<16:47,  4.80s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 40/250 [03:16<16:47,  4.80s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 41/250 [03:16<16:37,  4.77s/epoch(s), train_multiclassaccuracy\nTraining:  16%|▏| 41/250 [03:21<16:37,  4.77s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 42/250 [03:21<16:34,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 42/250 [03:26<16:34,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 43/250 [03:26<16:19,  4.73s/epoch(s), train_multiclassaccuracy\nTraining:  17%|▏| 43/250 [03:30<16:19,  4.73s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 44/250 [03:30<16:17,  4.75s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 44/250 [03:35<16:17,  4.75s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 45/250 [03:35<16:05,  4.71s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 45/250 [03:40<16:05,  4.71s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 46/250 [03:40<16:04,  4.73s/epoch(s), train_multiclassaccuracy\nTraining:  18%|▏| 46/250 [03:44<16:04,  4.73s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 47/250 [03:44<15:54,  4.70s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 47/250 [03:49<15:54,  4.70s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 48/250 [03:49<15:57,  4.74s/epoch(s), train_multiclassaccuracy\nTraining:  19%|▏| 48/250 [03:54<15:57,  4.74s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 49/250 [03:54<16:05,  4.80s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 49/250 [03:59<16:05,  4.80s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 50/250 [03:59<15:55,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 50/250 [04:04<15:55,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 51/250 [04:04<16:00,  4.82s/epoch(s), train_multiclassaccuracy\nTraining:  20%|▏| 51/250 [04:08<16:00,  4.82s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 52/250 [04:08<15:46,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 52/250 [04:13<15:46,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 53/250 [04:13<15:43,  4.79s/epoch(s), train_multiclassaccuracy\nTraining:  21%|▏| 53/250 [04:18<15:43,  4.79s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 54/250 [04:18<15:30,  4.75s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 54/250 [04:23<15:30,  4.75s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 55/250 [04:23<15:31,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 55/250 [04:28<15:31,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 56/250 [04:28<15:26,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  22%|▏| 56/250 [04:33<15:26,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 57/250 [04:33<15:33,  4.84s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 57/250 [04:37<15:33,  4.84s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 58/250 [04:37<15:21,  4.80s/epoch(s), train_multiclassaccuracy\nTraining:  23%|▏| 58/250 [04:42<15:21,  4.80s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 59/250 [04:42<15:18,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 59/250 [04:47<15:18,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 60/250 [04:47<15:09,  4.79s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 60/250 [04:52<15:09,  4.79s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 61/250 [04:52<15:08,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:  24%|▏| 61/250 [04:56<15:08,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▏| 62/250 [04:56<14:59,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▏| 62/250 [05:01<14:59,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▎| 63/250 [05:01<14:59,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:  25%|▎| 63/250 [05:06<14:59,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 64/250 [05:06<14:47,  4.77s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 64/250 [05:11<14:47,  4.77s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 65/250 [05:11<14:48,  4.80s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 65/250 [05:16<14:48,  4.80s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 66/250 [05:16<14:38,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  26%|▎| 66/250 [05:20<14:38,  4.78s/epoch(s), train_multiclassaccuracy\nTraining:  27%|▎| 67/250 [05:20<14:40,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:  27%|▎| 67/250 [05:25<14:40,  4.81s/epoch(s), train_multiclassaccuracy\nTraining:  27%|▎| 68/250 [05:25<14:37,  4.82s/epoch(s), train_multiclassaccuracy\nTraining:  27%|▎| 68/250 [05:30<14:37,  4.82s/epoch(s), train_multiclassaccuracyEarly stopping at epoch 68 with validation loss 0.000 and training loss 1.006\n\nTraining:  27%|▎| 68/250 [05:30<14:44,  4.86s/epoch(s), train_multiclassaccuracy\n{'MulticlassAccuracy': tensor(0.6485)}\n/home/chetana/tensorboard/2023-04-05_04-05/model_ckpt_final.pt\n",
  "history_begin_time" : 1680667507343,
  "history_end_time" : 1680667843468,
  "history_notes" : null,
  "history_process" : "2x5xrm",
  "host_id" : "c2lqcn",
  "indicator" : "Done"
},{
  "history_id" : "dkfp1j33bca",
  "history_input" : "\nimport cv2  # use cv2 to count eddies by drawing contours around segmentation masks\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nfrom get_device_config import *\nfrom tqdm.auto import tqdm\nfrom model_training_utils import run_batch, write_metrics_to_tensorboard, filter_scalar_metrics, EarlyStopping\nfrom create_eddy_net import *\n\nnum_plots_in_tensorboard = 5\n# will populate this later with random numbers:\nrandom_plot_indices = np.zeros((num_plots_in_tensorboard,), np.uint8)\n\n\ndef run_epoch(\n    epoch,\n    model,\n    loss_fn,\n    optimizer,\n    scheduler,\n    train_loader,\n    val_loader,\n    train_metrics,\n    val_metrics,\n    writer,\n):\n    leave = epoch == num_epochs - 1  # leave progress bar on screen after last epoch\n\n    model.train()\n    # training set\n    for batch_num, (gvs, seg_masks, date_indices) in enumerate(train_loader):\n        train_loss = run_batch(\n            model, loss_fn, gvs, seg_masks, optimizer, scheduler, train_metrics\n        )\n        iter_num = epoch * len(train_loader) + batch_num\n        writer.add_scalar(\"train/lr\", scheduler.get_last_lr()[-1], iter_num)\n\n    # validation set\n    images, preds, labels, dates = [], [], [], []\n    model.eval()\n    with torch.no_grad():\n        val_loss = num_examples = 0\n        for gvs, masks, date_indices in val_loader:\n            # continue\n            loss_, pred_batch = run_batch(\n                model, loss_fn, gvs, masks, metrics=val_metrics, return_pred=True\n            )\n            val_loss += loss_\n            num_examples += np.prod(gvs.shape)\n            # keep track of images, preds, labels for plotting\n            images.append(gvs)\n            preds.append(pred_batch)\n            labels.append(masks)\n            dates.append(date_indices)\n\n    # calculate average validation loss across all samples\n    # num_examples should be equal to sum of all pixels\n    val_loss = val_loss / num_examples\n\n    # plot validation images and log to tensorboard\n    ## move images, preds, labels, dates to cpu\n    images = torch.cat(images).cpu().numpy()\n    labels = torch.cat(labels).cpu().numpy()\n    preds = torch.cat(preds).cpu().numpy()\n    dates = torch.cat(dates).cpu().numpy()\n    ## convert indices to actual dates\n    dates = [val_loader.dataset.dates[i].strftime(\"%Y-%m-%d\") for i in dates]\n\n    # take random images from validation set\n    if epoch == 0:\n        indices_ = np.random.choice(\n            len(images), num_plots_in_tensorboard, replace=False\n        )\n        for i, idx in enumerate(indices_):\n            random_plot_indices[i] = idx\n    fig, ax = plt.subplots(num_plots_in_tensorboard, 3, figsize=(20, 30))\n    for n, i in enumerate(random_plot_indices):\n        date, img, mask, pred = dates[i], images[i], labels[i], preds[i]\n        artists = plot_eddies_on_axes(\n            date, img, mask, pred, ax[n, 0], ax[n, 1], ax[n, 2]\n        )\n    plt.tight_layout()\n    writer.add_figure(f\"val/sample_prediction\", fig, global_step=epoch)\n\n    # Update tensorboard\n    train_m = write_metrics_to_tensorboard(\n        num_classes, train_metrics, writer, epoch, \"train\"\n    )\n    val_m = write_metrics_to_tensorboard(num_classes, val_metrics, writer, epoch, \"val\")\n\n    writer.add_scalar(\"train/loss\", train_loss, epoch)\n    writer.add_scalar(\"val/loss\", val_loss, epoch)\n\n    # reset metrics after each epoch\n    train_metrics.reset()\n    val_metrics.reset()\n\n    train_m = filter_scalar_metrics(train_m)\n    val_m = filter_scalar_metrics(val_m)\n\n    return train_loss, val_loss, train_m, val_m\n\n\ndef plot_eddies_on_axes(date, img, mask, pred, a1, a2, a3):\n    im1 = a1.imshow(img.squeeze(), cmap=\"viridis\")\n\n    # blit canvas for a1 a2 a3\n    a1.figure.canvas.draw()\n    a1.figure.canvas.flush_events()\n    a2.figure.canvas.draw()\n    a2.figure.canvas.flush_events()\n    a3.figure.canvas.draw()\n    a3.figure.canvas.flush_events()\n\n    # https://stackoverflow.com/a/49159236\n    t1 = a1.text(\n        0.5,\n        1.05,\n        f\"ADT {date}\",\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a1.transAxes,\n    )\n    # set axis off\n    a1.axis(\"off\")\n\n    # count number of eddies in mask and pred\n    mask_anticyclonic = count_eddies(mask, \"anticyclonic\")\n    mask_cyclonic = count_eddies(mask, \"cyclonic\")\n    pred_anticyclonic = count_eddies(pred, \"anticyclonic\")\n    pred_cyclonic = count_eddies(pred, \"cyclonic\")\n\n    # calculate accuracy between pred and mask\n    acc = np.sum(pred == mask) / mask.size\n    im2 = a2.imshow(pred, cmap=\"viridis\")\n    t2 = a2.text(\n        0.5,\n        1.05,\n        (\n            f\"Prediction (Acc = {acc:.3f} |\"\n            f\" Num. anticyclonic = {pred_anticyclonic} |\"\n            f\" Num. cyclonic = {pred_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a2.transAxes,\n    )\n    a2.axis(\"off\")\n    im3 = a3.imshow(mask, cmap=\"viridis\")\n    t3 = a3.text(\n        0.5,\n        1.05,\n        (\n            f\"Ground Truth\"\n            f\" (Num. anticyclonic: {mask_anticyclonic} |\"\n            f\" Num. cyclonic: {mask_cyclonic})\"\n        ),\n        size=plt.rcParams[\"axes.titlesize\"],\n        ha=\"center\",\n        transform=a3.transAxes,\n    )\n    a3.axis(\"off\")\n\n    return im1, t1, im2, t2, im3, t3\n\n\ndef count_eddies(arr, eddy_type=\"both\"):\n    mask = np.zeros(arr.shape, dtype=np.uint8)\n    if eddy_type == \"anticyclonic\":\n        mask[arr == 1] = 1\n    elif eddy_type == \"cyclonic\":\n        mask[arr == 2] = 1\n    else:\n        mask[arr > 0] = 1\n    contours, hierarchy = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n    return len(contours)",
  "history_output" : "Read 21 samples from /home/chetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 12 samples from /home/chetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\n",
  "history_begin_time" : 1680667501246,
  "history_end_time" : 1680667506959,
  "history_notes" : null,
  "history_process" : "3z0gs7",
  "host_id" : "c2lqcn",
  "indicator" : "Done"
},{
  "history_id" : "ccctcjfuqj3",
  "history_input" : "from matplotlib.animation import ArtistAnimation\nfrom model_utils import *\nfrom set_summary_writer import *\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.load_state_dict(torch.load(\"/home/chetana/tensorboard/2023-03-15_03-26/model_ckpt_final.pt\"))\nmodel.eval()\nwith torch.no_grad():\n    fig, ax = plt.subplots(1, 3, figsize=(25, 10))\n    artists = []\n    # loop through all SSH maps and eddy masks in 2019\n    # and run the model to generate predicted eddy masks\n    for n, (ssh_vars, seg_masks, date_indices) in enumerate(val_loader):\n        ssh_vars = ssh_vars.to(device)\n        seg_masks = seg_masks.to(device)\n        # Run the model to generate predictions\n        preds = model(ssh_vars)\n\n        # For each pixel, EddyNet outputs predictions in probabilities,\n        # so choose the channels (0, 1, or 2) with the highest prob.\n        preds = preds.argmax(dim=1)\n\n        # Loop through all SSH maps, eddy masks, and predicted masks\n        # in this minibatch and generate a video\n        preds = preds.cpu().numpy()\n        seg_masks = seg_masks.cpu().numpy()\n        ssh_vars = ssh_vars.cpu().numpy()\n        date_indices = date_indices.cpu().numpy()\n        for i in range(len(ssh_vars)):\n            date, img, mask, pred = date_indices[i], ssh_vars[i], seg_masks[i], preds[i]\n            img1, title1, img2, title2, img3, title3 = plot_eddies_on_axes(\n                date, img, mask, pred, ax[0], ax[1], ax[2]\n            )\n            artists.append([img1, title1, img2, title2, img3, title3])\n            fig.canvas.draw()\n            fig.canvas.flush_events()\n    animation = ArtistAnimation(fig, artists, interval=200, blit=True)\n    plt.close()\n\nprint(os.path.join(tensorboard_dir, \"val_predictions.gif\"))\nanimation.save(os.path.join(tensorboard_dir, \"val_predictions.gif\"), writer=\"pillow\")\n\n# HTML(animation.to_jshtml())\n\n#plot contour\n\np = preds[0].astype(np.uint8)\n\nprint(f\"Number of anticyclonic eddies: {count_eddies(p, eddy_type='anticyclonic')}\")\nprint(f\"Number of cyclonic eddies: {count_eddies(p, eddy_type='cyclonic')}\")\nprint(f\"Number of both eddies: {count_eddies(p, eddy_type='both')}\")\n\n# draw contours on the image\nthr = cv2.threshold(p, 0, 1, cv2.THRESH_BINARY)[1].astype(np.uint8)\ncontours, hierarchy = cv2.findContours(thr, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\nimg = np.zeros(p.shape, np.uint8)\ncv2.drawContours(img, contours, -1, (255, 255, 255), 1)\nfileName = os.path.join(\"/home/chetana/plots/\",\"contours.png\")\ncv2.imwrite(fileName, img)\nplt.imshow(img, cmap=\"gray\")\nplt.axis(\"off\")\n\n# get average contour area\narea = 0\nfor cnt in contours:\n    area += cv2.contourArea(cnt)\narea /= len(contours)\nprint(f\"Average contour area: {area:.2f} sq. pixels\")",
  "history_output" : "Read 21 samples from /home/chetana/ML_eddies/cds_ssh_1998-2018_10day_interval/subset_pet_masks_with_adt_1998-2018_lat14N-46N_lon166W-134W.npz.\nRead 12 samples from /home/chetana/ML_eddies/cds_ssh_2019_10day_interval/subset_pet_masks_with_adt_2019_lat14N-46N_lon166W-134W.npz.\n======================================================================\nWriting Tensorboard logs to /home/chetana/tensorboard/2023-04-05_04-10\n======================================================================\nTraceback (most recent call last):\n  File \"/home/chetana/gw-workspace/ccctcjfuqj3/eval_on_validation_set.py\", line 6, in <module>\n    model.load_state_dict(torch.load(\"/home/chetana/tensorboard/2023-03-15_03-26/model_ckpt_final.pt\"))\n  File \"/home/chetana/anaconda3/envs/ranjan/lib/python3.10/site-packages/torch/serialization.py\", line 771, in load\n    with _open_file_like(f, 'rb') as opened_file:\n  File \"/home/chetana/anaconda3/envs/ranjan/lib/python3.10/site-packages/torch/serialization.py\", line 270, in _open_file_like\n    return _open_file(name_or_buffer, mode)\n  File \"/home/chetana/anaconda3/envs/ranjan/lib/python3.10/site-packages/torch/serialization.py\", line 251, in __init__\n    super(_open_file, self).__init__(open(name, mode))\nFileNotFoundError: [Errno 2] No such file or directory: '/home/chetana/tensorboard/2023-03-15_03-26/model_ckpt_final.pt'\n",
  "history_begin_time" : 1680667844135,
  "history_end_time" : 1680667849491,
  "history_notes" : null,
  "history_process" : "tcr60i",
  "host_id" : "c2lqcn",
  "indicator" : "Failed"
},{
  "history_id" : "pict5k7l456",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1680667091235,
  "history_end_time" : 1680667091235,
  "history_notes" : null,
  "history_process" : "ejp9sg",
  "host_id" : "c2lqcn",
  "indicator" : "Skipped"
},{
  "history_id" : "ljeqtnk3me3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1680667091236,
  "history_end_time" : 1680667091236,
  "history_notes" : null,
  "history_process" : "wn1y5m",
  "host_id" : "c2lqcn",
  "indicator" : "Skipped"
},{
  "history_id" : "g39agy3fwbk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1680667091238,
  "history_end_time" : 1680667091238,
  "history_notes" : null,
  "history_process" : "f6bwfv",
  "host_id" : "c2lqcn",
  "indicator" : "Skipped"
},{
  "history_id" : "j6m358ufex9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1680667091238,
  "history_end_time" : 1680667091238,
  "history_notes" : null,
  "history_process" : "znr9qa",
  "host_id" : "c2lqcn",
  "indicator" : "Skipped"
}]
