[{
  "history_id" : "u65qzhef7mh",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1668659941277,
  "history_end_time" : 1668659941277,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "h58txihy2fv",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1668659552209,
  "history_end_time" : 1668659937658,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "qgkb7xx62k8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1668657107185,
  "history_end_time" : 1668659548555,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "j6iicmucklv",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1668628447183,
  "history_end_time" : 1668628447183,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "qpewxnlrs1f",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1668624225184,
  "history_end_time" : 1668624225184,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "9b4ptw5i2g0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1668623875373,
  "history_end_time" : 1668623875373,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "pkd1j6rmgth",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1668623442820,
  "history_end_time" : 1668623442820,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "v0oskoh3v5o",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1668623284852,
  "history_end_time" : 1668623428598,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "cd29cffulyl",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1668623195092,
  "history_end_time" : 1668623195092,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "rnwtlqv04cn",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1668623059697,
  "history_end_time" : 1668623191191,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "77s8eyex8kr",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1668622991342,
  "history_end_time" : 1668623057321,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "eba0tdtwwnm",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1668622934240,
  "history_end_time" : 1668622988823,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "zdsd83su57b",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1668621793530,
  "history_end_time" : 1668621793530,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ku8ziugaca1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1668620779659,
  "history_end_time" : 1668621623247,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "8n37c1wm6y5",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1668619675933,
  "history_end_time" : 1668620774903,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "3zqcovoljtd",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1668619500780,
  "history_end_time" : 1668619500780,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "9t4301oo5kg",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1668619487875,
  "history_end_time" : 1668619496010,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "3ibt1c42i0l",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1668619430788,
  "history_end_time" : 1668619430788,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "oqomlkul0lg",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1668618475720,
  "history_end_time" : 1668619427440,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ihfhuzg583m",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1668617176519,
  "history_end_time" : 1668617179383,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "rkyyxofogw8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1668614586855,
  "history_end_time" : 1668614586855,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "hotw8tr19y3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1668613041904,
  "history_end_time" : 1668613059650,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "w92w44gunkl",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1668606689797,
  "history_end_time" : 1668611987880,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ihhj5bpgeg8",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1668602171360,
  "history_end_time" : 1668606680274,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "uo21q2te322",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667848843256,
  "history_end_time" : 1667848843256,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "q7wiem6ci1g",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667848672351,
  "history_end_time" : 1667848802747,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "h0nfodt29xj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667835946384,
  "history_end_time" : 1667835946384,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "b64xbn7dgtv",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667834754454,
  "history_end_time" : 1667834755698,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "z5btnex6r1y",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667833562837,
  "history_end_time" : 1667834752818,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "x4c8rrsadk6",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667656271503,
  "history_end_time" : 1667656271503,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "uun4q4z18o9",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667654259259,
  "history_end_time" : 1667656266963,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "a2ubl3416a0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667652444914,
  "history_end_time" : 1667652444914,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "tsacr2q5sxa",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667578123266,
  "history_end_time" : 1667578123266,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "f0qlsuqflle",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667567849420,
  "history_end_time" : 1667578113313,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "auxq0jtfxsk",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667567845463,
  "history_end_time" : 1667567845463,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "tkq30e22lsr",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667566836964,
  "history_end_time" : 1667567844469,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "rhvpyii61q1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667566406656,
  "history_end_time" : 1667566406656,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "exgsrw1c5jw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667564943877,
  "history_end_time" : 1667566400553,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "l2m7gg3s2ps",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667524736000,
  "history_end_time" : 1667562870470,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "d0gpojtalw2",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667188267833,
  "history_end_time" : 1667188267833,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ml89znso6un",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667187373668,
  "history_end_time" : 1667188257036,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "ty9hasdshco",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667187034127,
  "history_end_time" : 1667187364181,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "81xgneafvox",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667186333408,
  "history_end_time" : 1667186333408,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "vajc17ydurd",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667186213067,
  "history_end_time" : 1667186314347,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "s28l6myn3j3",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667186212841,
  "history_end_time" : 1667186212841,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "vcexp4pl40l",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667185556601,
  "history_end_time" : 1667186204434,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "k7qblpmtg0w",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667185421097,
  "history_end_time" : 1667185421097,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "7iosncghde7",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667185410502,
  "history_end_time" : 1667185419421,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "yr153qsm302",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667184378583,
  "history_end_time" : 1667185392923,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "dkgdob02d76",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667184365811,
  "history_end_time" : 1667184373461,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "qjtms7lszp1",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667184279363,
  "history_end_time" : 1667184298771,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "onkjgfs6j1r",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667184189674,
  "history_end_time" : 1667184270211,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "jlrh2i5wzxx",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667180717092,
  "history_end_time" : 1667180717092,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "2zwjnkfbw5p",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667180445964,
  "history_end_time" : 1667180628910,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "v7jj86595fj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667180388558,
  "history_end_time" : 1667180438894,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "maslbk6x9bf",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667180305887,
  "history_end_time" : 1667180359945,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "p99wcmml6ke",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667180257739,
  "history_end_time" : 1667180257739,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ubswj2av2hh",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667091246126,
  "history_end_time" : 1667091390353,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "oztpjumxu2z",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1667085498691,
  "history_end_time" : 1667091234856,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "pj4sdqlqzsw",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1666715954467,
  "history_end_time" : 1666715960611,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "rw9925hb5h0",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1666715918555,
  "history_end_time" : 1666715941361,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "msnw9u605pl",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1666205807401,
  "history_end_time" : 1666205807401,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "ua36vdblh3u",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1666205806332,
  "history_end_time" : 1666205806332,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "pg86zqsy36z",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1666203643349,
  "history_end_time" : 1666203643349,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "8i0xycc0xtl",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1666203442062,
  "history_end_time" : 1666203442062,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "az24hto728p",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1666203334993,
  "history_end_time" : 1666203334993,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "m8c7svua9qe",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1666203334075,
  "history_end_time" : 1666203334075,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "mv5j6rtcu16",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1666203001482,
  "history_end_time" : 1666203001482,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "h826o14jnsj",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1666202962832,
  "history_end_time" : 1666202962832,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "0qc0x15mdne",
  "history_input" : "No code saved",
  "history_output" : "Skipped",
  "history_begin_time" : 1666202865213,
  "history_end_time" : 1666202865213,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Skipped"
},{
  "history_id" : "j6qtrcetzqt",
  "history_input" : "#Eddynet\nimport collections\nfrom itertools import repeat\nfrom typing import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass EddyNet(nn.Module):\n    \"\"\"\n    PyTorch implementation of EddyNet from Lguensat et al. (2018)\n    Original implementation in TensorFlow: https://github.com/redouanelg/EddyNet\n    \"\"\"\n    def __init__(self, num_classes, num_filters, kernel_size):\n        super(EddyNet, self).__init__()\n        # encoder\n        self.encoder1 = EddyNet._block(1, num_filters, kernel_size, \"enc1\", dropout=0.2)\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder2 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc2\", dropout=0.3\n        )\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder3 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc3\", dropout=0.4\n        )\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder4 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc4\", dropout=0.5\n        )\n\n        # decoder\n        self.decoder3 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec3\", dropout=0.4\n        )\n        self.decoder2 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec2\", dropout=0.3\n        )\n        self.decoder1 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec1\", dropout=0.2\n        )\n\n        # final layer\n        self.final_conv = nn.Conv2d(\n            num_filters, num_classes, kernel_size=1, padding=0, bias=False\n        )\n\n    @staticmethod\n    def conv_block(in_channels, out_channels, kernel_size, name, num, dropout=0):\n        layers = {\n            f\"{name}_conv{num}\": Conv2dSame(in_channels, out_channels, kernel_size),\n            f\"{name}_bn{num}\": nn.BatchNorm2d(out_channels),\n            f\"{name}_relu{num}\": nn.ReLU(inplace=True),\n        }\n        if dropout > 0:\n            layers[f\"{name}_dropout\"] = nn.Dropout(p=dropout)\n\n        return nn.Sequential(OrderedDict(layers))\n\n    @staticmethod\n    def _block(in_channels, out_channels, kernel_size, name, dropout=0):\n        conv1 = EddyNet.conv_block(in_channels, out_channels, kernel_size, name, 1)\n        conv2 = EddyNet.conv_block(\n            out_channels, out_channels, kernel_size, name, 2, dropout=dropout\n        )\n        return nn.Sequential(conv1, conv2)\n\n    @staticmethod\n    def decoder_block(in_channels, out_channels, kernel_size, name, dropout=0):\n        return EddyNet._block(in_channels, out_channels, kernel_size, name, dropout)\n\n    def forward(self, x):\n        # encoder\n        enc1 = self.encoder1(x)\n        pool1 = self.pool1(enc1)\n\n        enc2 = self.encoder2(pool1)\n        pool2 = self.pool2(enc2)\n\n        enc3 = self.encoder3(pool2)\n        pool3 = self.pool3(enc3)\n\n        # bottleneck?\n        enc4 = self.encoder4(pool3)\n\n        # decoder\n        dec3 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(enc4)\n        dec3 = torch.cat((dec3, enc3), dim=1)\n        dec3 = self.decoder3(dec3)\n\n        dec2 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec3)\n        dec2 = torch.cat((dec2, enc2), dim=1)\n        dec2 = self.decoder2(dec2)\n\n        dec1 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec2)\n        dec1 = torch.cat((dec1, enc1), dim=1)\n        dec1 = self.decoder1(dec1)\n\n        # final layer\n        final = self.final_conv(dec1)\n\n        # softmax\n        final = nn.Softmax(dim=1)(final)\n\n        return final\n\n\nclass Conv2dSame(nn.Module):\n    \"\"\"Manual convolution with same padding\n    https://discuss.pytorch.org/t/same-padding-equivalent-in-pytorch/85121/9\n    Although PyTorch >= 1.10.0 supports ``padding='same'`` as a keyword\n    argument, this does not export to CoreML as of coremltools 5.1.0,\n    so we need to implement the internal torch logic manually.\n    Currently the ``RuntimeError`` is\n    \"PyTorch convert function for op '_convolution_mode' not implemented\"\n    \"\"\"\n\n    def __init__(\n        self, in_channels, out_channels, kernel_size, stride=1, dilation=1, **kwargs\n    ):\n        \"\"\"Wrap base convolution layer\n        See official PyTorch documentation for parameter details\n        https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n        \"\"\"\n        super().__init__()\n        self.conv = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            dilation=dilation,\n            **kwargs,\n        )\n\n        # Setup internal representations\n        kernel_size_ = _pair(kernel_size)\n        dilation_ = _pair(dilation)\n        self._reversed_padding_repeated_twice = [0, 0] * len(kernel_size_)\n\n        # Follow the logic from ``nn/modules/conv.py:_ConvNd``\n        for d, k, i in zip(\n            dilation_, kernel_size_, range(len(kernel_size_) - 1, -1, -1)\n        ):\n            total_padding = d * (k - 1)\n            left_pad = total_padding // 2\n            self._reversed_padding_repeated_twice[2 * i] = left_pad\n            self._reversed_padding_repeated_twice[2 * i + 1] = total_padding - left_pad\n\n    def forward(self, imgs):\n        \"\"\"Setup padding so same spatial dimensions are returned\n        All shapes (input/output) are ``(N, C, W, H)`` convention\n        :param torch.Tensor imgs:\n        :return torch.Tensor:\n        \"\"\"\n        padded = F.pad(imgs, self._reversed_padding_repeated_twice)\n        return self.conv(padded)\n\n\ndef _ntuple(n):\n    \"\"\"Copy from PyTorch since internal function is not importable\n    See ``nn/modules/utils.py:6``\n    \"\"\"\n\n    def parse(x):\n        if isinstance(x, collections.abc.Iterable):\n            return tuple(x)\n        return tuple(repeat(x, n))\n\n    return parse\n\n\n_pair = _ntuple(2)\nFooter",
  "history_output" : "Traceback (most recent call last):\n  File \"/Users/lakshmichetana/gw-workspace/j6qtrcetzqt/eddynet.py\", line 173, in <module>\n    Footer\nNameError: name 'Footer' is not defined\n",
  "history_begin_time" : 1665754744906,
  "history_end_time" : 1665757923189,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "why3xxzwb3f",
  "history_input" : "#Eddynet\nimport collections\nfrom itertools import repeat\nfrom typing import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass EddyNet(nn.Module):\n    \"\"\"\n    PyTorch implementation of EddyNet from Lguensat et al. (2018)\n    Original implementation in TensorFlow: https://github.com/redouanelg/EddyNet\n    \"\"\"\n    def __init__(self, num_classes, num_filters, kernel_size):\n        super(EddyNet, self).__init__()\n        # encoder\n        self.encoder1 = EddyNet._block(1, num_filters, kernel_size, \"enc1\", dropout=0.2)\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder2 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc2\", dropout=0.3\n        )\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder3 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc3\", dropout=0.4\n        )\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder4 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc4\", dropout=0.5\n        )\n\n        # decoder\n        self.decoder3 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec3\", dropout=0.4\n        )\n        self.decoder2 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec2\", dropout=0.3\n        )\n        self.decoder1 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec1\", dropout=0.2\n        )\n\n        # final layer\n        self.final_conv = nn.Conv2d(\n            num_filters, num_classes, kernel_size=1, padding=0, bias=False\n        )\n\n    @staticmethod\n    def conv_block(in_channels, out_channels, kernel_size, name, num, dropout=0):\n        layers = {\n            f\"{name}_conv{num}\": Conv2dSame(in_channels, out_channels, kernel_size),\n            f\"{name}_bn{num}\": nn.BatchNorm2d(out_channels),\n            f\"{name}_relu{num}\": nn.ReLU(inplace=True),\n        }\n        if dropout > 0:\n            layers[f\"{name}_dropout\"] = nn.Dropout(p=dropout)\n\n        return nn.Sequential(OrderedDict(layers))\n\n    @staticmethod\n    def _block(in_channels, out_channels, kernel_size, name, dropout=0):\n        conv1 = EddyNet.conv_block(in_channels, out_channels, kernel_size, name, 1)\n        conv2 = EddyNet.conv_block(\n            out_channels, out_channels, kernel_size, name, 2, dropout=dropout\n        )\n        return nn.Sequential(conv1, conv2)\n\n    @staticmethod\n    def decoder_block(in_channels, out_channels, kernel_size, name, dropout=0):\n        return EddyNet._block(in_channels, out_channels, kernel_size, name, dropout)\n\n    def forward(self, x):\n        # encoder\n        enc1 = self.encoder1(x)\n        pool1 = self.pool1(enc1)\n\n        enc2 = self.encoder2(pool1)\n        pool2 = self.pool2(enc2)\n\n        enc3 = self.encoder3(pool2)\n        pool3 = self.pool3(enc3)\n\n        # bottleneck?\n        enc4 = self.encoder4(pool3)\n\n        # decoder\n        dec3 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(enc4)\n        dec3 = torch.cat((dec3, enc3), dim=1)\n        dec3 = self.decoder3(dec3)\n\n        dec2 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec3)\n        dec2 = torch.cat((dec2, enc2), dim=1)\n        dec2 = self.decoder2(dec2)\n\n        dec1 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec2)\n        dec1 = torch.cat((dec1, enc1), dim=1)\n        dec1 = self.decoder1(dec1)\n\n        # final layer\n        final = self.final_conv(dec1)\n\n        # softmax\n        final = nn.Softmax(dim=1)(final)\n\n        return final\n\n\nclass Conv2dSame(nn.Module):\n    \"\"\"Manual convolution with same padding\n    https://discuss.pytorch.org/t/same-padding-equivalent-in-pytorch/85121/9\n    Although PyTorch >= 1.10.0 supports ``padding='same'`` as a keyword\n    argument, this does not export to CoreML as of coremltools 5.1.0,\n    so we need to implement the internal torch logic manually.\n    Currently the ``RuntimeError`` is\n    \"PyTorch convert function for op '_convolution_mode' not implemented\"\n    \"\"\"\n\n    def __init__(\n        self, in_channels, out_channels, kernel_size, stride=1, dilation=1, **kwargs\n    ):\n        \"\"\"Wrap base convolution layer\n        See official PyTorch documentation for parameter details\n        https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n        \"\"\"\n        super().__init__()\n        self.conv = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            dilation=dilation,\n            **kwargs,\n        )\n\n        # Setup internal representations\n        kernel_size_ = _pair(kernel_size)\n        dilation_ = _pair(dilation)\n        self._reversed_padding_repeated_twice = [0, 0] * len(kernel_size_)\n\n        # Follow the logic from ``nn/modules/conv.py:_ConvNd``\n        for d, k, i in zip(\n            dilation_, kernel_size_, range(len(kernel_size_) - 1, -1, -1)\n        ):\n            total_padding = d * (k - 1)\n            left_pad = total_padding // 2\n            self._reversed_padding_repeated_twice[2 * i] = left_pad\n            self._reversed_padding_repeated_twice[2 * i + 1] = total_padding - left_pad\n\n    def forward(self, imgs):\n        \"\"\"Setup padding so same spatial dimensions are returned\n        All shapes (input/output) are ``(N, C, W, H)`` convention\n        :param torch.Tensor imgs:\n        :return torch.Tensor:\n        \"\"\"\n        padded = F.pad(imgs, self._reversed_padding_repeated_twice)\n        return self.conv(padded)\n\n\ndef _ntuple(n):\n    \"\"\"Copy from PyTorch since internal function is not importable\n    See ``nn/modules/utils.py:6``\n    \"\"\"\n\n    def parse(x):\n        if isinstance(x, collections.abc.Iterable):\n            return tuple(x)\n        return tuple(repeat(x, n))\n\n    return parse\n\n\n_pair = _ntuple(2)\nFooter",
  "history_output" : "Traceback (most recent call last):\n  File \"/Users/lakshmichetana/gw-workspace/why3xxzwb3f/eddynet.py\", line 173, in <module>\n    Footer\nNameError: name 'Footer' is not defined\n",
  "history_begin_time" : 1665488495451,
  "history_end_time" : 1665492180752,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "mc3n55lq151",
  "history_input" : "#Eddynet\nimport collections\nfrom itertools import repeat\nfrom typing import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass EddyNet(nn.Module):\n    \"\"\"\n    PyTorch implementation of EddyNet from Lguensat et al. (2018)\n    Original implementation in TensorFlow: https://github.com/redouanelg/EddyNet\n    \"\"\"\n    def __init__(self, num_classes, num_filters, kernel_size):\n        super(EddyNet, self).__init__()\n        # encoder\n        self.encoder1 = EddyNet._block(1, num_filters, kernel_size, \"enc1\", dropout=0.2)\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder2 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc2\", dropout=0.3\n        )\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder3 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc3\", dropout=0.4\n        )\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder4 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc4\", dropout=0.5\n        )\n\n        # decoder\n        self.decoder3 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec3\", dropout=0.4\n        )\n        self.decoder2 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec2\", dropout=0.3\n        )\n        self.decoder1 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec1\", dropout=0.2\n        )\n\n        # final layer\n        self.final_conv = nn.Conv2d(\n            num_filters, num_classes, kernel_size=1, padding=0, bias=False\n        )\n\n    @staticmethod\n    def conv_block(in_channels, out_channels, kernel_size, name, num, dropout=0):\n        layers = {\n            f\"{name}_conv{num}\": Conv2dSame(in_channels, out_channels, kernel_size),\n            f\"{name}_bn{num}\": nn.BatchNorm2d(out_channels),\n            f\"{name}_relu{num}\": nn.ReLU(inplace=True),\n        }\n        if dropout > 0:\n            layers[f\"{name}_dropout\"] = nn.Dropout(p=dropout)\n\n        return nn.Sequential(OrderedDict(layers))\n\n    @staticmethod\n    def _block(in_channels, out_channels, kernel_size, name, dropout=0):\n        conv1 = EddyNet.conv_block(in_channels, out_channels, kernel_size, name, 1)\n        conv2 = EddyNet.conv_block(\n            out_channels, out_channels, kernel_size, name, 2, dropout=dropout\n        )\n        return nn.Sequential(conv1, conv2)\n\n    @staticmethod\n    def decoder_block(in_channels, out_channels, kernel_size, name, dropout=0):\n        return EddyNet._block(in_channels, out_channels, kernel_size, name, dropout)\n\n    def forward(self, x):\n        # encoder\n        enc1 = self.encoder1(x)\n        pool1 = self.pool1(enc1)\n\n        enc2 = self.encoder2(pool1)\n        pool2 = self.pool2(enc2)\n\n        enc3 = self.encoder3(pool2)\n        pool3 = self.pool3(enc3)\n\n        # bottleneck?\n        enc4 = self.encoder4(pool3)\n\n        # decoder\n        dec3 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(enc4)\n        dec3 = torch.cat((dec3, enc3), dim=1)\n        dec3 = self.decoder3(dec3)\n\n        dec2 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec3)\n        dec2 = torch.cat((dec2, enc2), dim=1)\n        dec2 = self.decoder2(dec2)\n\n        dec1 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec2)\n        dec1 = torch.cat((dec1, enc1), dim=1)\n        dec1 = self.decoder1(dec1)\n\n        # final layer\n        final = self.final_conv(dec1)\n\n        # softmax\n        final = nn.Softmax(dim=1)(final)\n\n        return final\n\n\nclass Conv2dSame(nn.Module):\n    \"\"\"Manual convolution with same padding\n    https://discuss.pytorch.org/t/same-padding-equivalent-in-pytorch/85121/9\n    Although PyTorch >= 1.10.0 supports ``padding='same'`` as a keyword\n    argument, this does not export to CoreML as of coremltools 5.1.0,\n    so we need to implement the internal torch logic manually.\n    Currently the ``RuntimeError`` is\n    \"PyTorch convert function for op '_convolution_mode' not implemented\"\n    \"\"\"\n\n    def __init__(\n        self, in_channels, out_channels, kernel_size, stride=1, dilation=1, **kwargs\n    ):\n        \"\"\"Wrap base convolution layer\n        See official PyTorch documentation for parameter details\n        https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n        \"\"\"\n        super().__init__()\n        self.conv = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            dilation=dilation,\n            **kwargs,\n        )\n\n        # Setup internal representations\n        kernel_size_ = _pair(kernel_size)\n        dilation_ = _pair(dilation)\n        self._reversed_padding_repeated_twice = [0, 0] * len(kernel_size_)\n\n        # Follow the logic from ``nn/modules/conv.py:_ConvNd``\n        for d, k, i in zip(\n            dilation_, kernel_size_, range(len(kernel_size_) - 1, -1, -1)\n        ):\n            total_padding = d * (k - 1)\n            left_pad = total_padding // 2\n            self._reversed_padding_repeated_twice[2 * i] = left_pad\n            self._reversed_padding_repeated_twice[2 * i + 1] = total_padding - left_pad\n\n    def forward(self, imgs):\n        \"\"\"Setup padding so same spatial dimensions are returned\n        All shapes (input/output) are ``(N, C, W, H)`` convention\n        :param torch.Tensor imgs:\n        :return torch.Tensor:\n        \"\"\"\n        padded = F.pad(imgs, self._reversed_padding_repeated_twice)\n        return self.conv(padded)\n\n\ndef _ntuple(n):\n    \"\"\"Copy from PyTorch since internal function is not importable\n    See ``nn/modules/utils.py:6``\n    \"\"\"\n\n    def parse(x):\n        if isinstance(x, collections.abc.Iterable):\n            return tuple(x)\n        return tuple(repeat(x, n))\n\n    return parse\n\n\n_pair = _ntuple(2)\nFooter",
  "history_output" : "Traceback (most recent call last):\n  File \"/Users/lakshmichetana/gw-workspace/mc3n55lq151/eddynet.py\", line 173, in <module>\n    Footer\nNameError: name 'Footer' is not defined\n",
  "history_begin_time" : 1665418703665,
  "history_end_time" : 1665454136969,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "d2zkbttruf4",
  "history_input" : "#Eddynet\nimport collections\nfrom itertools import repeat\nfrom typing import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass EddyNet(nn.Module):\n    \"\"\"\n    PyTorch implementation of EddyNet from Lguensat et al. (2018)\n    Original implementation in TensorFlow: https://github.com/redouanelg/EddyNet\n    \"\"\"\n    def __init__(self, num_classes, num_filters, kernel_size):\n        super(EddyNet, self).__init__()\n        # encoder\n        self.encoder1 = EddyNet._block(1, num_filters, kernel_size, \"enc1\", dropout=0.2)\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder2 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc2\", dropout=0.3\n        )\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder3 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc3\", dropout=0.4\n        )\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder4 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc4\", dropout=0.5\n        )\n\n        # decoder\n        self.decoder3 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec3\", dropout=0.4\n        )\n        self.decoder2 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec2\", dropout=0.3\n        )\n        self.decoder1 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec1\", dropout=0.2\n        )\n\n        # final layer\n        self.final_conv = nn.Conv2d(\n            num_filters, num_classes, kernel_size=1, padding=0, bias=False\n        )\n\n    @staticmethod\n    def conv_block(in_channels, out_channels, kernel_size, name, num, dropout=0):\n        layers = {\n            f\"{name}_conv{num}\": Conv2dSame(in_channels, out_channels, kernel_size),\n            f\"{name}_bn{num}\": nn.BatchNorm2d(out_channels),\n            f\"{name}_relu{num}\": nn.ReLU(inplace=True),\n        }\n        if dropout > 0:\n            layers[f\"{name}_dropout\"] = nn.Dropout(p=dropout)\n\n        return nn.Sequential(OrderedDict(layers))\n\n    @staticmethod\n    def _block(in_channels, out_channels, kernel_size, name, dropout=0):\n        conv1 = EddyNet.conv_block(in_channels, out_channels, kernel_size, name, 1)\n        conv2 = EddyNet.conv_block(\n            out_channels, out_channels, kernel_size, name, 2, dropout=dropout\n        )\n        return nn.Sequential(conv1, conv2)\n\n    @staticmethod\n    def decoder_block(in_channels, out_channels, kernel_size, name, dropout=0):\n        return EddyNet._block(in_channels, out_channels, kernel_size, name, dropout)\n\n    def forward(self, x):\n        # encoder\n        enc1 = self.encoder1(x)\n        pool1 = self.pool1(enc1)\n\n        enc2 = self.encoder2(pool1)\n        pool2 = self.pool2(enc2)\n\n        enc3 = self.encoder3(pool2)\n        pool3 = self.pool3(enc3)\n\n        # bottleneck?\n        enc4 = self.encoder4(pool3)\n\n        # decoder\n        dec3 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(enc4)\n        dec3 = torch.cat((dec3, enc3), dim=1)\n        dec3 = self.decoder3(dec3)\n\n        dec2 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec3)\n        dec2 = torch.cat((dec2, enc2), dim=1)\n        dec2 = self.decoder2(dec2)\n\n        dec1 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec2)\n        dec1 = torch.cat((dec1, enc1), dim=1)\n        dec1 = self.decoder1(dec1)\n\n        # final layer\n        final = self.final_conv(dec1)\n\n        # softmax\n        final = nn.Softmax(dim=1)(final)\n\n        return final\n\n\nclass Conv2dSame(nn.Module):\n    \"\"\"Manual convolution with same padding\n    https://discuss.pytorch.org/t/same-padding-equivalent-in-pytorch/85121/9\n    Although PyTorch >= 1.10.0 supports ``padding='same'`` as a keyword\n    argument, this does not export to CoreML as of coremltools 5.1.0,\n    so we need to implement the internal torch logic manually.\n    Currently the ``RuntimeError`` is\n    \"PyTorch convert function for op '_convolution_mode' not implemented\"\n    \"\"\"\n\n    def __init__(\n        self, in_channels, out_channels, kernel_size, stride=1, dilation=1, **kwargs\n    ):\n        \"\"\"Wrap base convolution layer\n        See official PyTorch documentation for parameter details\n        https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n        \"\"\"\n        super().__init__()\n        self.conv = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            dilation=dilation,\n            **kwargs,\n        )\n\n        # Setup internal representations\n        kernel_size_ = _pair(kernel_size)\n        dilation_ = _pair(dilation)\n        self._reversed_padding_repeated_twice = [0, 0] * len(kernel_size_)\n\n        # Follow the logic from ``nn/modules/conv.py:_ConvNd``\n        for d, k, i in zip(\n            dilation_, kernel_size_, range(len(kernel_size_) - 1, -1, -1)\n        ):\n            total_padding = d * (k - 1)\n            left_pad = total_padding // 2\n            self._reversed_padding_repeated_twice[2 * i] = left_pad\n            self._reversed_padding_repeated_twice[2 * i + 1] = total_padding - left_pad\n\n    def forward(self, imgs):\n        \"\"\"Setup padding so same spatial dimensions are returned\n        All shapes (input/output) are ``(N, C, W, H)`` convention\n        :param torch.Tensor imgs:\n        :return torch.Tensor:\n        \"\"\"\n        padded = F.pad(imgs, self._reversed_padding_repeated_twice)\n        return self.conv(padded)\n\n\ndef _ntuple(n):\n    \"\"\"Copy from PyTorch since internal function is not importable\n    See ``nn/modules/utils.py:6``\n    \"\"\"\n\n    def parse(x):\n        if isinstance(x, collections.abc.Iterable):\n            return tuple(x)\n        return tuple(repeat(x, n))\n\n    return parse\n\n\n_pair = _ntuple(2)\nFooter",
  "history_output" : "",
  "history_begin_time" : 1665245533954,
  "history_end_time" : 1665253907671,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "7uzjmqgyx2s",
  "history_input" : "#Eddynet\nimport collections\nfrom itertools import repeat\nfrom typing import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass EddyNet(nn.Module):\n    \"\"\"\n    PyTorch implementation of EddyNet from Lguensat et al. (2018)\n    Original implementation in TensorFlow: https://github.com/redouanelg/EddyNet\n    \"\"\"\n    def __init__(self, num_classes, num_filters, kernel_size):\n        super(EddyNet, self).__init__()\n        # encoder\n        self.encoder1 = EddyNet._block(1, num_filters, kernel_size, \"enc1\", dropout=0.2)\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder2 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc2\", dropout=0.3\n        )\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder3 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc3\", dropout=0.4\n        )\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder4 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc4\", dropout=0.5\n        )\n\n        # decoder\n        self.decoder3 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec3\", dropout=0.4\n        )\n        self.decoder2 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec2\", dropout=0.3\n        )\n        self.decoder1 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec1\", dropout=0.2\n        )\n\n        # final layer\n        self.final_conv = nn.Conv2d(\n            num_filters, num_classes, kernel_size=1, padding=0, bias=False\n        )\n\n    @staticmethod\n    def conv_block(in_channels, out_channels, kernel_size, name, num, dropout=0):\n        layers = {\n            f\"{name}_conv{num}\": Conv2dSame(in_channels, out_channels, kernel_size),\n            f\"{name}_bn{num}\": nn.BatchNorm2d(out_channels),\n            f\"{name}_relu{num}\": nn.ReLU(inplace=True),\n        }\n        if dropout > 0:\n            layers[f\"{name}_dropout\"] = nn.Dropout(p=dropout)\n\n        return nn.Sequential(OrderedDict(layers))\n\n    @staticmethod\n    def _block(in_channels, out_channels, kernel_size, name, dropout=0):\n        conv1 = EddyNet.conv_block(in_channels, out_channels, kernel_size, name, 1)\n        conv2 = EddyNet.conv_block(\n            out_channels, out_channels, kernel_size, name, 2, dropout=dropout\n        )\n        return nn.Sequential(conv1, conv2)\n\n    @staticmethod\n    def decoder_block(in_channels, out_channels, kernel_size, name, dropout=0):\n        return EddyNet._block(in_channels, out_channels, kernel_size, name, dropout)\n\n    def forward(self, x):\n        # encoder\n        enc1 = self.encoder1(x)\n        pool1 = self.pool1(enc1)\n\n        enc2 = self.encoder2(pool1)\n        pool2 = self.pool2(enc2)\n\n        enc3 = self.encoder3(pool2)\n        pool3 = self.pool3(enc3)\n\n        # bottleneck?\n        enc4 = self.encoder4(pool3)\n\n        # decoder\n        dec3 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(enc4)\n        dec3 = torch.cat((dec3, enc3), dim=1)\n        dec3 = self.decoder3(dec3)\n\n        dec2 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec3)\n        dec2 = torch.cat((dec2, enc2), dim=1)\n        dec2 = self.decoder2(dec2)\n\n        dec1 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec2)\n        dec1 = torch.cat((dec1, enc1), dim=1)\n        dec1 = self.decoder1(dec1)\n\n        # final layer\n        final = self.final_conv(dec1)\n\n        # softmax\n        final = nn.Softmax(dim=1)(final)\n\n        return final\n\n\nclass Conv2dSame(nn.Module):\n    \"\"\"Manual convolution with same padding\n    https://discuss.pytorch.org/t/same-padding-equivalent-in-pytorch/85121/9\n    Although PyTorch >= 1.10.0 supports ``padding='same'`` as a keyword\n    argument, this does not export to CoreML as of coremltools 5.1.0,\n    so we need to implement the internal torch logic manually.\n    Currently the ``RuntimeError`` is\n    \"PyTorch convert function for op '_convolution_mode' not implemented\"\n    \"\"\"\n\n    def __init__(\n        self, in_channels, out_channels, kernel_size, stride=1, dilation=1, **kwargs\n    ):\n        \"\"\"Wrap base convolution layer\n        See official PyTorch documentation for parameter details\n        https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n        \"\"\"\n        super().__init__()\n        self.conv = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            dilation=dilation,\n            **kwargs,\n        )\n\n        # Setup internal representations\n        kernel_size_ = _pair(kernel_size)\n        dilation_ = _pair(dilation)\n        self._reversed_padding_repeated_twice = [0, 0] * len(kernel_size_)\n\n        # Follow the logic from ``nn/modules/conv.py:_ConvNd``\n        for d, k, i in zip(\n            dilation_, kernel_size_, range(len(kernel_size_) - 1, -1, -1)\n        ):\n            total_padding = d * (k - 1)\n            left_pad = total_padding // 2\n            self._reversed_padding_repeated_twice[2 * i] = left_pad\n            self._reversed_padding_repeated_twice[2 * i + 1] = total_padding - left_pad\n\n    def forward(self, imgs):\n        \"\"\"Setup padding so same spatial dimensions are returned\n        All shapes (input/output) are ``(N, C, W, H)`` convention\n        :param torch.Tensor imgs:\n        :return torch.Tensor:\n        \"\"\"\n        padded = F.pad(imgs, self._reversed_padding_repeated_twice)\n        return self.conv(padded)\n\n\ndef _ntuple(n):\n    \"\"\"Copy from PyTorch since internal function is not importable\n    See ``nn/modules/utils.py:6``\n    \"\"\"\n\n    def parse(x):\n        if isinstance(x, collections.abc.Iterable):\n            return tuple(x)\n        return tuple(repeat(x, n))\n\n    return parse\n\n\n_pair = _ntuple(2)\nFooter",
  "history_output" : "Traceback (most recent call last):\n  File \"/Users/lakshmichetana/gw-workspace/7uzjmqgyx2s/eddynet.py\", line 173, in <module>\n    Footer\nNameError: name 'Footer' is not defined\n",
  "history_begin_time" : 1665244638149,
  "history_end_time" : 1665245509411,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "q377e25zgua",
  "history_input" : "#Eddynet\nimport collections\nfrom itertools import repeat\nfrom typing import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass EddyNet(nn.Module):\n    \"\"\"\n    PyTorch implementation of EddyNet from Lguensat et al. (2018)\n    Original implementation in TensorFlow: https://github.com/redouanelg/EddyNet\n    \"\"\"\n    def __init__(self, num_classes, num_filters, kernel_size):\n        super(EddyNet, self).__init__()\n        # encoder\n        self.encoder1 = EddyNet._block(1, num_filters, kernel_size, \"enc1\", dropout=0.2)\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder2 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc2\", dropout=0.3\n        )\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder3 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc3\", dropout=0.4\n        )\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder4 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc4\", dropout=0.5\n        )\n\n        # decoder\n        self.decoder3 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec3\", dropout=0.4\n        )\n        self.decoder2 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec2\", dropout=0.3\n        )\n        self.decoder1 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec1\", dropout=0.2\n        )\n\n        # final layer\n        self.final_conv = nn.Conv2d(\n            num_filters, num_classes, kernel_size=1, padding=0, bias=False\n        )\n\n    @staticmethod\n    def conv_block(in_channels, out_channels, kernel_size, name, num, dropout=0):\n        layers = {\n            f\"{name}_conv{num}\": Conv2dSame(in_channels, out_channels, kernel_size),\n            f\"{name}_bn{num}\": nn.BatchNorm2d(out_channels),\n            f\"{name}_relu{num}\": nn.ReLU(inplace=True),\n        }\n        if dropout > 0:\n            layers[f\"{name}_dropout\"] = nn.Dropout(p=dropout)\n\n        return nn.Sequential(OrderedDict(layers))\n\n    @staticmethod\n    def _block(in_channels, out_channels, kernel_size, name, dropout=0):\n        conv1 = EddyNet.conv_block(in_channels, out_channels, kernel_size, name, 1)\n        conv2 = EddyNet.conv_block(\n            out_channels, out_channels, kernel_size, name, 2, dropout=dropout\n        )\n        return nn.Sequential(conv1, conv2)\n\n    @staticmethod\n    def decoder_block(in_channels, out_channels, kernel_size, name, dropout=0):\n        return EddyNet._block(in_channels, out_channels, kernel_size, name, dropout)\n\n    def forward(self, x):\n        # encoder\n        enc1 = self.encoder1(x)\n        pool1 = self.pool1(enc1)\n\n        enc2 = self.encoder2(pool1)\n        pool2 = self.pool2(enc2)\n\n        enc3 = self.encoder3(pool2)\n        pool3 = self.pool3(enc3)\n\n        # bottleneck?\n        enc4 = self.encoder4(pool3)\n\n        # decoder\n        dec3 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(enc4)\n        dec3 = torch.cat((dec3, enc3), dim=1)\n        dec3 = self.decoder3(dec3)\n\n        dec2 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec3)\n        dec2 = torch.cat((dec2, enc2), dim=1)\n        dec2 = self.decoder2(dec2)\n\n        dec1 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec2)\n        dec1 = torch.cat((dec1, enc1), dim=1)\n        dec1 = self.decoder1(dec1)\n\n        # final layer\n        final = self.final_conv(dec1)\n\n        # softmax\n        final = nn.Softmax(dim=1)(final)\n\n        return final\n\n\nclass Conv2dSame(nn.Module):\n    \"\"\"Manual convolution with same padding\n    https://discuss.pytorch.org/t/same-padding-equivalent-in-pytorch/85121/9\n    Although PyTorch >= 1.10.0 supports ``padding='same'`` as a keyword\n    argument, this does not export to CoreML as of coremltools 5.1.0,\n    so we need to implement the internal torch logic manually.\n    Currently the ``RuntimeError`` is\n    \"PyTorch convert function for op '_convolution_mode' not implemented\"\n    \"\"\"\n\n    def __init__(\n        self, in_channels, out_channels, kernel_size, stride=1, dilation=1, **kwargs\n    ):\n        \"\"\"Wrap base convolution layer\n        See official PyTorch documentation for parameter details\n        https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n        \"\"\"\n        super().__init__()\n        self.conv = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            dilation=dilation,\n            **kwargs,\n        )\n\n        # Setup internal representations\n        kernel_size_ = _pair(kernel_size)\n        dilation_ = _pair(dilation)\n        self._reversed_padding_repeated_twice = [0, 0] * len(kernel_size_)\n\n        # Follow the logic from ``nn/modules/conv.py:_ConvNd``\n        for d, k, i in zip(\n            dilation_, kernel_size_, range(len(kernel_size_) - 1, -1, -1)\n        ):\n            total_padding = d * (k - 1)\n            left_pad = total_padding // 2\n            self._reversed_padding_repeated_twice[2 * i] = left_pad\n            self._reversed_padding_repeated_twice[2 * i + 1] = total_padding - left_pad\n\n    def forward(self, imgs):\n        \"\"\"Setup padding so same spatial dimensions are returned\n        All shapes (input/output) are ``(N, C, W, H)`` convention\n        :param torch.Tensor imgs:\n        :return torch.Tensor:\n        \"\"\"\n        padded = F.pad(imgs, self._reversed_padding_repeated_twice)\n        return self.conv(padded)\n\n\ndef _ntuple(n):\n    \"\"\"Copy from PyTorch since internal function is not importable\n    See ``nn/modules/utils.py:6``\n    \"\"\"\n\n    def parse(x):\n        if isinstance(x, collections.abc.Iterable):\n            return tuple(x)\n        return tuple(repeat(x, n))\n\n    return parse\n\n\n_pair = _ntuple(2)\nFooter",
  "history_output" : "Traceback (most recent call last):\n  File \"/Users/lakshmichetana/gw-workspace/q377e25zgua/eddynet.py\", line 173, in <module>\n    Footer\nNameError: name 'Footer' is not defined\n",
  "history_begin_time" : 1665181360742,
  "history_end_time" : 1665244616485,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "cb0dot4dx2x",
  "history_input" : "#Eddynet\nimport collections\nfrom itertools import repeat\nfrom typing import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass EddyNet(nn.Module):\n    \"\"\"\n    PyTorch implementation of EddyNet from Lguensat et al. (2018)\n    Original implementation in TensorFlow: https://github.com/redouanelg/EddyNet\n    \"\"\"\n    def __init__(self, num_classes, num_filters, kernel_size):\n        super(EddyNet, self).__init__()\n        # encoder\n        self.encoder1 = EddyNet._block(1, num_filters, kernel_size, \"enc1\", dropout=0.2)\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder2 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc2\", dropout=0.3\n        )\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder3 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc3\", dropout=0.4\n        )\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder4 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc4\", dropout=0.5\n        )\n\n        # decoder\n        self.decoder3 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec3\", dropout=0.4\n        )\n        self.decoder2 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec2\", dropout=0.3\n        )\n        self.decoder1 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec1\", dropout=0.2\n        )\n\n        # final layer\n        self.final_conv = nn.Conv2d(\n            num_filters, num_classes, kernel_size=1, padding=0, bias=False\n        )\n\n    @staticmethod\n    def conv_block(in_channels, out_channels, kernel_size, name, num, dropout=0):\n        layers = {\n            f\"{name}_conv{num}\": Conv2dSame(in_channels, out_channels, kernel_size),\n            f\"{name}_bn{num}\": nn.BatchNorm2d(out_channels),\n            f\"{name}_relu{num}\": nn.ReLU(inplace=True),\n        }\n        if dropout > 0:\n            layers[f\"{name}_dropout\"] = nn.Dropout(p=dropout)\n\n        return nn.Sequential(OrderedDict(layers))\n\n    @staticmethod\n    def _block(in_channels, out_channels, kernel_size, name, dropout=0):\n        conv1 = EddyNet.conv_block(in_channels, out_channels, kernel_size, name, 1)\n        conv2 = EddyNet.conv_block(\n            out_channels, out_channels, kernel_size, name, 2, dropout=dropout\n        )\n        return nn.Sequential(conv1, conv2)\n\n    @staticmethod\n    def decoder_block(in_channels, out_channels, kernel_size, name, dropout=0):\n        return EddyNet._block(in_channels, out_channels, kernel_size, name, dropout)\n\n    def forward(self, x):\n        # encoder\n        enc1 = self.encoder1(x)\n        pool1 = self.pool1(enc1)\n\n        enc2 = self.encoder2(pool1)\n        pool2 = self.pool2(enc2)\n\n        enc3 = self.encoder3(pool2)\n        pool3 = self.pool3(enc3)\n\n        # bottleneck?\n        enc4 = self.encoder4(pool3)\n\n        # decoder\n        dec3 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(enc4)\n        dec3 = torch.cat((dec3, enc3), dim=1)\n        dec3 = self.decoder3(dec3)\n\n        dec2 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec3)\n        dec2 = torch.cat((dec2, enc2), dim=1)\n        dec2 = self.decoder2(dec2)\n\n        dec1 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec2)\n        dec1 = torch.cat((dec1, enc1), dim=1)\n        dec1 = self.decoder1(dec1)\n\n        # final layer\n        final = self.final_conv(dec1)\n\n        # softmax\n        final = nn.Softmax(dim=1)(final)\n\n        return final\n\n\nclass Conv2dSame(nn.Module):\n    \"\"\"Manual convolution with same padding\n    https://discuss.pytorch.org/t/same-padding-equivalent-in-pytorch/85121/9\n    Although PyTorch >= 1.10.0 supports ``padding='same'`` as a keyword\n    argument, this does not export to CoreML as of coremltools 5.1.0,\n    so we need to implement the internal torch logic manually.\n    Currently the ``RuntimeError`` is\n    \"PyTorch convert function for op '_convolution_mode' not implemented\"\n    \"\"\"\n\n    def __init__(\n        self, in_channels, out_channels, kernel_size, stride=1, dilation=1, **kwargs\n    ):\n        \"\"\"Wrap base convolution layer\n        See official PyTorch documentation for parameter details\n        https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n        \"\"\"\n        super().__init__()\n        self.conv = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            dilation=dilation,\n            **kwargs,\n        )\n\n        # Setup internal representations\n        kernel_size_ = _pair(kernel_size)\n        dilation_ = _pair(dilation)\n        self._reversed_padding_repeated_twice = [0, 0] * len(kernel_size_)\n\n        # Follow the logic from ``nn/modules/conv.py:_ConvNd``\n        for d, k, i in zip(\n            dilation_, kernel_size_, range(len(kernel_size_) - 1, -1, -1)\n        ):\n            total_padding = d * (k - 1)\n            left_pad = total_padding // 2\n            self._reversed_padding_repeated_twice[2 * i] = left_pad\n            self._reversed_padding_repeated_twice[2 * i + 1] = total_padding - left_pad\n\n    def forward(self, imgs):\n        \"\"\"Setup padding so same spatial dimensions are returned\n        All shapes (input/output) are ``(N, C, W, H)`` convention\n        :param torch.Tensor imgs:\n        :return torch.Tensor:\n        \"\"\"\n        padded = F.pad(imgs, self._reversed_padding_repeated_twice)\n        return self.conv(padded)\n\n\ndef _ntuple(n):\n    \"\"\"Copy from PyTorch since internal function is not importable\n    See ``nn/modules/utils.py:6``\n    \"\"\"\n\n    def parse(x):\n        if isinstance(x, collections.abc.Iterable):\n            return tuple(x)\n        return tuple(repeat(x, n))\n\n    return parse\n\n\n_pair = _ntuple(2)\nFooter",
  "history_output" : "Traceback (most recent call last):\n  File \"/Users/lakshmichetana/gw-workspace/cb0dot4dx2x/eddynet.py\", line 6, in <module>\n    import torch\nModuleNotFoundError: No module named 'torch'\n",
  "history_begin_time" : 1665180985320,
  "history_end_time" : 1665181356996,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "7pdth5sllmh",
  "history_input" : "#Eddynet\nimport collections\nfrom itertools import repeat\nfrom typing import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass EddyNet(nn.Module):\n    \"\"\"\n    PyTorch implementation of EddyNet from Lguensat et al. (2018)\n    Original implementation in TensorFlow: https://github.com/redouanelg/EddyNet\n    \"\"\"\n    def __init__(self, num_classes, num_filters, kernel_size):\n        super(EddyNet, self).__init__()\n        # encoder\n        self.encoder1 = EddyNet._block(1, num_filters, kernel_size, \"enc1\", dropout=0.2)\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder2 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc2\", dropout=0.3\n        )\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder3 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc3\", dropout=0.4\n        )\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder4 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc4\", dropout=0.5\n        )\n\n        # decoder\n        self.decoder3 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec3\", dropout=0.4\n        )\n        self.decoder2 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec2\", dropout=0.3\n        )\n        self.decoder1 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec1\", dropout=0.2\n        )\n\n        # final layer\n        self.final_conv = nn.Conv2d(\n            num_filters, num_classes, kernel_size=1, padding=0, bias=False\n        )\n\n    @staticmethod\n    def conv_block(in_channels, out_channels, kernel_size, name, num, dropout=0):\n        layers = {\n            f\"{name}_conv{num}\": Conv2dSame(in_channels, out_channels, kernel_size),\n            f\"{name}_bn{num}\": nn.BatchNorm2d(out_channels),\n            f\"{name}_relu{num}\": nn.ReLU(inplace=True),\n        }\n        if dropout > 0:\n            layers[f\"{name}_dropout\"] = nn.Dropout(p=dropout)\n\n        return nn.Sequential(OrderedDict(layers))\n\n    @staticmethod\n    def _block(in_channels, out_channels, kernel_size, name, dropout=0):\n        conv1 = EddyNet.conv_block(in_channels, out_channels, kernel_size, name, 1)\n        conv2 = EddyNet.conv_block(\n            out_channels, out_channels, kernel_size, name, 2, dropout=dropout\n        )\n        return nn.Sequential(conv1, conv2)\n\n    @staticmethod\n    def decoder_block(in_channels, out_channels, kernel_size, name, dropout=0):\n        return EddyNet._block(in_channels, out_channels, kernel_size, name, dropout)\n\n    def forward(self, x):\n        # encoder\n        enc1 = self.encoder1(x)\n        pool1 = self.pool1(enc1)\n\n        enc2 = self.encoder2(pool1)\n        pool2 = self.pool2(enc2)\n\n        enc3 = self.encoder3(pool2)\n        pool3 = self.pool3(enc3)\n\n        # bottleneck?\n        enc4 = self.encoder4(pool3)\n\n        # decoder\n        dec3 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(enc4)\n        dec3 = torch.cat((dec3, enc3), dim=1)\n        dec3 = self.decoder3(dec3)\n\n        dec2 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec3)\n        dec2 = torch.cat((dec2, enc2), dim=1)\n        dec2 = self.decoder2(dec2)\n\n        dec1 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec2)\n        dec1 = torch.cat((dec1, enc1), dim=1)\n        dec1 = self.decoder1(dec1)\n\n        # final layer\n        final = self.final_conv(dec1)\n\n        # softmax\n        final = nn.Softmax(dim=1)(final)\n\n        return final\n\n\nclass Conv2dSame(nn.Module):\n    \"\"\"Manual convolution with same padding\n    https://discuss.pytorch.org/t/same-padding-equivalent-in-pytorch/85121/9\n    Although PyTorch >= 1.10.0 supports ``padding='same'`` as a keyword\n    argument, this does not export to CoreML as of coremltools 5.1.0,\n    so we need to implement the internal torch logic manually.\n    Currently the ``RuntimeError`` is\n    \"PyTorch convert function for op '_convolution_mode' not implemented\"\n    \"\"\"\n\n    def __init__(\n        self, in_channels, out_channels, kernel_size, stride=1, dilation=1, **kwargs\n    ):\n        \"\"\"Wrap base convolution layer\n        See official PyTorch documentation for parameter details\n        https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n        \"\"\"\n        super().__init__()\n        self.conv = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            dilation=dilation,\n            **kwargs,\n        )\n\n        # Setup internal representations\n        kernel_size_ = _pair(kernel_size)\n        dilation_ = _pair(dilation)\n        self._reversed_padding_repeated_twice = [0, 0] * len(kernel_size_)\n\n        # Follow the logic from ``nn/modules/conv.py:_ConvNd``\n        for d, k, i in zip(\n            dilation_, kernel_size_, range(len(kernel_size_) - 1, -1, -1)\n        ):\n            total_padding = d * (k - 1)\n            left_pad = total_padding // 2\n            self._reversed_padding_repeated_twice[2 * i] = left_pad\n            self._reversed_padding_repeated_twice[2 * i + 1] = total_padding - left_pad\n\n    def forward(self, imgs):\n        \"\"\"Setup padding so same spatial dimensions are returned\n        All shapes (input/output) are ``(N, C, W, H)`` convention\n        :param torch.Tensor imgs:\n        :return torch.Tensor:\n        \"\"\"\n        padded = F.pad(imgs, self._reversed_padding_repeated_twice)\n        return self.conv(padded)\n\n\ndef _ntuple(n):\n    \"\"\"Copy from PyTorch since internal function is not importable\n    See ``nn/modules/utils.py:6``\n    \"\"\"\n\n    def parse(x):\n        if isinstance(x, collections.abc.Iterable):\n            return tuple(x)\n        return tuple(repeat(x, n))\n\n    return parse\n\n\n_pair = _ntuple(2)\nFooter",
  "history_output" : "Traceback (most recent call last):\n  File \"/Users/lakshmichetana/gw-workspace/7pdth5sllmh/eddynet.py\", line 6, in <module>\n    import torch\nModuleNotFoundError: No module named 'torch'\n",
  "history_begin_time" : 1665096326094,
  "history_end_time" : 1665096326180,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "zu788tpt0iv",
  "history_input" : "#Eddynet\nimport collections\nfrom itertools import repeat\nfrom typing import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass EddyNet(nn.Module):\n    \"\"\"\n    PyTorch implementation of EddyNet from Lguensat et al. (2018)\n    Original implementation in TensorFlow: https://github.com/redouanelg/EddyNet\n    \"\"\"\n    def __init__(self, num_classes, num_filters, kernel_size):\n        super(EddyNet, self).__init__()\n        # encoder\n        self.encoder1 = EddyNet._block(1, num_filters, kernel_size, \"enc1\", dropout=0.2)\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder2 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc2\", dropout=0.3\n        )\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder3 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc3\", dropout=0.4\n        )\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder4 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc4\", dropout=0.5\n        )\n\n        # decoder\n        self.decoder3 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec3\", dropout=0.4\n        )\n        self.decoder2 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec2\", dropout=0.3\n        )\n        self.decoder1 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec1\", dropout=0.2\n        )\n\n        # final layer\n        self.final_conv = nn.Conv2d(\n            num_filters, num_classes, kernel_size=1, padding=0, bias=False\n        )\n\n    @staticmethod\n    def conv_block(in_channels, out_channels, kernel_size, name, num, dropout=0):\n        layers = {\n            f\"{name}_conv{num}\": Conv2dSame(in_channels, out_channels, kernel_size),\n            f\"{name}_bn{num}\": nn.BatchNorm2d(out_channels),\n            f\"{name}_relu{num}\": nn.ReLU(inplace=True),\n        }\n        if dropout > 0:\n            layers[f\"{name}_dropout\"] = nn.Dropout(p=dropout)\n\n        return nn.Sequential(OrderedDict(layers))\n\n    @staticmethod\n    def _block(in_channels, out_channels, kernel_size, name, dropout=0):\n        conv1 = EddyNet.conv_block(in_channels, out_channels, kernel_size, name, 1)\n        conv2 = EddyNet.conv_block(\n            out_channels, out_channels, kernel_size, name, 2, dropout=dropout\n        )\n        return nn.Sequential(conv1, conv2)\n\n    @staticmethod\n    def decoder_block(in_channels, out_channels, kernel_size, name, dropout=0):\n        return EddyNet._block(in_channels, out_channels, kernel_size, name, dropout)\n\n    def forward(self, x):\n        # encoder\n        enc1 = self.encoder1(x)\n        pool1 = self.pool1(enc1)\n\n        enc2 = self.encoder2(pool1)\n        pool2 = self.pool2(enc2)\n\n        enc3 = self.encoder3(pool2)\n        pool3 = self.pool3(enc3)\n\n        # bottleneck?\n        enc4 = self.encoder4(pool3)\n\n        # decoder\n        dec3 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(enc4)\n        dec3 = torch.cat((dec3, enc3), dim=1)\n        dec3 = self.decoder3(dec3)\n\n        dec2 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec3)\n        dec2 = torch.cat((dec2, enc2), dim=1)\n        dec2 = self.decoder2(dec2)\n\n        dec1 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec2)\n        dec1 = torch.cat((dec1, enc1), dim=1)\n        dec1 = self.decoder1(dec1)\n\n        # final layer\n        final = self.final_conv(dec1)\n\n        # softmax\n        final = nn.Softmax(dim=1)(final)\n\n        return final\n\n\nclass Conv2dSame(nn.Module):\n    \"\"\"Manual convolution with same padding\n    https://discuss.pytorch.org/t/same-padding-equivalent-in-pytorch/85121/9\n    Although PyTorch >= 1.10.0 supports ``padding='same'`` as a keyword\n    argument, this does not export to CoreML as of coremltools 5.1.0,\n    so we need to implement the internal torch logic manually.\n    Currently the ``RuntimeError`` is\n    \"PyTorch convert function for op '_convolution_mode' not implemented\"\n    \"\"\"\n\n    def __init__(\n        self, in_channels, out_channels, kernel_size, stride=1, dilation=1, **kwargs\n    ):\n        \"\"\"Wrap base convolution layer\n        See official PyTorch documentation for parameter details\n        https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n        \"\"\"\n        super().__init__()\n        self.conv = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            dilation=dilation,\n            **kwargs,\n        )\n\n        # Setup internal representations\n        kernel_size_ = _pair(kernel_size)\n        dilation_ = _pair(dilation)\n        self._reversed_padding_repeated_twice = [0, 0] * len(kernel_size_)\n\n        # Follow the logic from ``nn/modules/conv.py:_ConvNd``\n        for d, k, i in zip(\n            dilation_, kernel_size_, range(len(kernel_size_) - 1, -1, -1)\n        ):\n            total_padding = d * (k - 1)\n            left_pad = total_padding // 2\n            self._reversed_padding_repeated_twice[2 * i] = left_pad\n            self._reversed_padding_repeated_twice[2 * i + 1] = total_padding - left_pad\n\n    def forward(self, imgs):\n        \"\"\"Setup padding so same spatial dimensions are returned\n        All shapes (input/output) are ``(N, C, W, H)`` convention\n        :param torch.Tensor imgs:\n        :return torch.Tensor:\n        \"\"\"\n        padded = F.pad(imgs, self._reversed_padding_repeated_twice)\n        return self.conv(padded)\n\n\ndef _ntuple(n):\n    \"\"\"Copy from PyTorch since internal function is not importable\n    See ``nn/modules/utils.py:6``\n    \"\"\"\n\n    def parse(x):\n        if isinstance(x, collections.abc.Iterable):\n            return tuple(x)\n        return tuple(repeat(x, n))\n\n    return parse\n\n\n_pair = _ntuple(2)\nFooter",
  "history_output" : "Traceback (most recent call last):\n  File \"/Users/lakshmichetana/gw-workspace/zu788tpt0iv/eddynet.py\", line 6, in <module>\n    import torch\nModuleNotFoundError: No module named 'torch'\n",
  "history_begin_time" : 1665089274165,
  "history_end_time" : 1665096317687,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "qbdvgua2sdz",
  "history_input" : "#Eddynet\nimport collections\nfrom itertools import repeat\nfrom typing import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass EddyNet(nn.Module):\n    \"\"\"\n    PyTorch implementation of EddyNet from Lguensat et al. (2018)\n    Original implementation in TensorFlow: https://github.com/redouanelg/EddyNet\n    \"\"\"\n    def __init__(self, num_classes, num_filters, kernel_size):\n        super(EddyNet, self).__init__()\n        # encoder\n        self.encoder1 = EddyNet._block(1, num_filters, kernel_size, \"enc1\", dropout=0.2)\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder2 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc2\", dropout=0.3\n        )\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder3 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc3\", dropout=0.4\n        )\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder4 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc4\", dropout=0.5\n        )\n\n        # decoder\n        self.decoder3 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec3\", dropout=0.4\n        )\n        self.decoder2 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec2\", dropout=0.3\n        )\n        self.decoder1 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec1\", dropout=0.2\n        )\n\n        # final layer\n        self.final_conv = nn.Conv2d(\n            num_filters, num_classes, kernel_size=1, padding=0, bias=False\n        )\n\n    @staticmethod\n    def conv_block(in_channels, out_channels, kernel_size, name, num, dropout=0):\n        layers = {\n            f\"{name}_conv{num}\": Conv2dSame(in_channels, out_channels, kernel_size),\n            f\"{name}_bn{num}\": nn.BatchNorm2d(out_channels),\n            f\"{name}_relu{num}\": nn.ReLU(inplace=True),\n        }\n        if dropout > 0:\n            layers[f\"{name}_dropout\"] = nn.Dropout(p=dropout)\n\n        return nn.Sequential(OrderedDict(layers))\n\n    @staticmethod\n    def _block(in_channels, out_channels, kernel_size, name, dropout=0):\n        conv1 = EddyNet.conv_block(in_channels, out_channels, kernel_size, name, 1)\n        conv2 = EddyNet.conv_block(\n            out_channels, out_channels, kernel_size, name, 2, dropout=dropout\n        )\n        return nn.Sequential(conv1, conv2)\n\n    @staticmethod\n    def decoder_block(in_channels, out_channels, kernel_size, name, dropout=0):\n        return EddyNet._block(in_channels, out_channels, kernel_size, name, dropout)\n\n    def forward(self, x):\n        # encoder\n        enc1 = self.encoder1(x)\n        pool1 = self.pool1(enc1)\n\n        enc2 = self.encoder2(pool1)\n        pool2 = self.pool2(enc2)\n\n        enc3 = self.encoder3(pool2)\n        pool3 = self.pool3(enc3)\n\n        # bottleneck?\n        enc4 = self.encoder4(pool3)\n\n        # decoder\n        dec3 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(enc4)\n        dec3 = torch.cat((dec3, enc3), dim=1)\n        dec3 = self.decoder3(dec3)\n\n        dec2 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec3)\n        dec2 = torch.cat((dec2, enc2), dim=1)\n        dec2 = self.decoder2(dec2)\n\n        dec1 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec2)\n        dec1 = torch.cat((dec1, enc1), dim=1)\n        dec1 = self.decoder1(dec1)\n\n        # final layer\n        final = self.final_conv(dec1)\n\n        # softmax\n        final = nn.Softmax(dim=1)(final)\n\n        return final\n\n\nclass Conv2dSame(nn.Module):\n    \"\"\"Manual convolution with same padding\n    https://discuss.pytorch.org/t/same-padding-equivalent-in-pytorch/85121/9\n    Although PyTorch >= 1.10.0 supports ``padding='same'`` as a keyword\n    argument, this does not export to CoreML as of coremltools 5.1.0,\n    so we need to implement the internal torch logic manually.\n    Currently the ``RuntimeError`` is\n    \"PyTorch convert function for op '_convolution_mode' not implemented\"\n    \"\"\"\n\n    def __init__(\n        self, in_channels, out_channels, kernel_size, stride=1, dilation=1, **kwargs\n    ):\n        \"\"\"Wrap base convolution layer\n        See official PyTorch documentation for parameter details\n        https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n        \"\"\"\n        super().__init__()\n        self.conv = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            dilation=dilation,\n            **kwargs,\n        )\n\n        # Setup internal representations\n        kernel_size_ = _pair(kernel_size)\n        dilation_ = _pair(dilation)\n        self._reversed_padding_repeated_twice = [0, 0] * len(kernel_size_)\n\n        # Follow the logic from ``nn/modules/conv.py:_ConvNd``\n        for d, k, i in zip(\n            dilation_, kernel_size_, range(len(kernel_size_) - 1, -1, -1)\n        ):\n            total_padding = d * (k - 1)\n            left_pad = total_padding // 2\n            self._reversed_padding_repeated_twice[2 * i] = left_pad\n            self._reversed_padding_repeated_twice[2 * i + 1] = total_padding - left_pad\n\n    def forward(self, imgs):\n        \"\"\"Setup padding so same spatial dimensions are returned\n        All shapes (input/output) are ``(N, C, W, H)`` convention\n        :param torch.Tensor imgs:\n        :return torch.Tensor:\n        \"\"\"\n        padded = F.pad(imgs, self._reversed_padding_repeated_twice)\n        return self.conv(padded)\n\n\ndef _ntuple(n):\n    \"\"\"Copy from PyTorch since internal function is not importable\n    See ``nn/modules/utils.py:6``\n    \"\"\"\n\n    def parse(x):\n        if isinstance(x, collections.abc.Iterable):\n            return tuple(x)\n        return tuple(repeat(x, n))\n\n    return parse\n\n\n_pair = _ntuple(2)\nFooter",
  "history_output" : "Traceback (most recent call last):\n  File \"/Users/lakshmichetana/gw-workspace/qbdvgua2sdz/eddynet.py\", line 6, in <module>\n    import torch\nModuleNotFoundError: No module named 'torch'\n",
  "history_begin_time" : 1665015398910,
  "history_end_time" : 1665015399658,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "msehyufoewt",
  "history_input" : "#Eddynet\nimport collections\nfrom itertools import repeat\nfrom typing import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass EddyNet(nn.Module):\n    \"\"\"\n    PyTorch implementation of EddyNet from Lguensat et al. (2018)\n    Original implementation in TensorFlow: https://github.com/redouanelg/EddyNet\n    \"\"\"\n    def __init__(self, num_classes, num_filters, kernel_size):\n        super(EddyNet, self).__init__()\n        # encoder\n        self.encoder1 = EddyNet._block(1, num_filters, kernel_size, \"enc1\", dropout=0.2)\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder2 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc2\", dropout=0.3\n        )\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder3 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc3\", dropout=0.4\n        )\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder4 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc4\", dropout=0.5\n        )\n\n        # decoder\n        self.decoder3 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec3\", dropout=0.4\n        )\n        self.decoder2 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec2\", dropout=0.3\n        )\n        self.decoder1 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec1\", dropout=0.2\n        )\n\n        # final layer\n        self.final_conv = nn.Conv2d(\n            num_filters, num_classes, kernel_size=1, padding=0, bias=False\n        )\n\n    @staticmethod\n    def conv_block(in_channels, out_channels, kernel_size, name, num, dropout=0):\n        layers = {\n            f\"{name}_conv{num}\": Conv2dSame(in_channels, out_channels, kernel_size),\n            f\"{name}_bn{num}\": nn.BatchNorm2d(out_channels),\n            f\"{name}_relu{num}\": nn.ReLU(inplace=True),\n        }\n        if dropout > 0:\n            layers[f\"{name}_dropout\"] = nn.Dropout(p=dropout)\n\n        return nn.Sequential(OrderedDict(layers))\n\n    @staticmethod\n    def _block(in_channels, out_channels, kernel_size, name, dropout=0):\n        conv1 = EddyNet.conv_block(in_channels, out_channels, kernel_size, name, 1)\n        conv2 = EddyNet.conv_block(\n            out_channels, out_channels, kernel_size, name, 2, dropout=dropout\n        )\n        return nn.Sequential(conv1, conv2)\n\n    @staticmethod\n    def decoder_block(in_channels, out_channels, kernel_size, name, dropout=0):\n        return EddyNet._block(in_channels, out_channels, kernel_size, name, dropout)\n\n    def forward(self, x):\n        # encoder\n        enc1 = self.encoder1(x)\n        pool1 = self.pool1(enc1)\n\n        enc2 = self.encoder2(pool1)\n        pool2 = self.pool2(enc2)\n\n        enc3 = self.encoder3(pool2)\n        pool3 = self.pool3(enc3)\n\n        # bottleneck?\n        enc4 = self.encoder4(pool3)\n\n        # decoder\n        dec3 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(enc4)\n        dec3 = torch.cat((dec3, enc3), dim=1)\n        dec3 = self.decoder3(dec3)\n\n        dec2 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec3)\n        dec2 = torch.cat((dec2, enc2), dim=1)\n        dec2 = self.decoder2(dec2)\n\n        dec1 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec2)\n        dec1 = torch.cat((dec1, enc1), dim=1)\n        dec1 = self.decoder1(dec1)\n\n        # final layer\n        final = self.final_conv(dec1)\n\n        # softmax\n        final = nn.Softmax(dim=1)(final)\n\n        return final\n\n\nclass Conv2dSame(nn.Module):\n    \"\"\"Manual convolution with same padding\n    https://discuss.pytorch.org/t/same-padding-equivalent-in-pytorch/85121/9\n    Although PyTorch >= 1.10.0 supports ``padding='same'`` as a keyword\n    argument, this does not export to CoreML as of coremltools 5.1.0,\n    so we need to implement the internal torch logic manually.\n    Currently the ``RuntimeError`` is\n    \"PyTorch convert function for op '_convolution_mode' not implemented\"\n    \"\"\"\n\n    def __init__(\n        self, in_channels, out_channels, kernel_size, stride=1, dilation=1, **kwargs\n    ):\n        \"\"\"Wrap base convolution layer\n        See official PyTorch documentation for parameter details\n        https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n        \"\"\"\n        super().__init__()\n        self.conv = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            dilation=dilation,\n            **kwargs,\n        )\n\n        # Setup internal representations\n        kernel_size_ = _pair(kernel_size)\n        dilation_ = _pair(dilation)\n        self._reversed_padding_repeated_twice = [0, 0] * len(kernel_size_)\n\n        # Follow the logic from ``nn/modules/conv.py:_ConvNd``\n        for d, k, i in zip(\n            dilation_, kernel_size_, range(len(kernel_size_) - 1, -1, -1)\n        ):\n            total_padding = d * (k - 1)\n            left_pad = total_padding // 2\n            self._reversed_padding_repeated_twice[2 * i] = left_pad\n            self._reversed_padding_repeated_twice[2 * i + 1] = total_padding - left_pad\n\n    def forward(self, imgs):\n        \"\"\"Setup padding so same spatial dimensions are returned\n        All shapes (input/output) are ``(N, C, W, H)`` convention\n        :param torch.Tensor imgs:\n        :return torch.Tensor:\n        \"\"\"\n        padded = F.pad(imgs, self._reversed_padding_repeated_twice)\n        return self.conv(padded)\n\n\ndef _ntuple(n):\n    \"\"\"Copy from PyTorch since internal function is not importable\n    See ``nn/modules/utils.py:6``\n    \"\"\"\n\n    def parse(x):\n        if isinstance(x, collections.abc.Iterable):\n            return tuple(x)\n        return tuple(repeat(x, n))\n\n    return parse\n\n\n_pair = _ntuple(2)\nFooter",
  "history_output" : "Traceback (most recent call last):\n  File \"/Users/lakshmichetana/gw-workspace/msehyufoewt/eddynet.py\", line 6, in <module>\n    import torch\nModuleNotFoundError: No module named 'torch'\n",
  "history_begin_time" : 1664976190333,
  "history_end_time" : 1664976190408,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "jc4ec955gtt",
  "history_input" : "#Eddynet\nimport collections\nfrom itertools import repeat\nfrom typing import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass EddyNet(nn.Module):\n    \"\"\"\n    PyTorch implementation of EddyNet from Lguensat et al. (2018)\n    Original implementation in TensorFlow: https://github.com/redouanelg/EddyNet\n    \"\"\"\n    def __init__(self, num_classes, num_filters, kernel_size):\n        super(EddyNet, self).__init__()\n        # encoder\n        self.encoder1 = EddyNet._block(1, num_filters, kernel_size, \"enc1\", dropout=0.2)\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder2 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc2\", dropout=0.3\n        )\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder3 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc3\", dropout=0.4\n        )\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder4 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc4\", dropout=0.5\n        )\n\n        # decoder\n        self.decoder3 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec3\", dropout=0.4\n        )\n        self.decoder2 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec2\", dropout=0.3\n        )\n        self.decoder1 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec1\", dropout=0.2\n        )\n\n        # final layer\n        self.final_conv = nn.Conv2d(\n            num_filters, num_classes, kernel_size=1, padding=0, bias=False\n        )\n\n    @staticmethod\n    def conv_block(in_channels, out_channels, kernel_size, name, num, dropout=0):\n        layers = {\n            f\"{name}_conv{num}\": Conv2dSame(in_channels, out_channels, kernel_size),\n            f\"{name}_bn{num}\": nn.BatchNorm2d(out_channels),\n            f\"{name}_relu{num}\": nn.ReLU(inplace=True),\n        }\n        if dropout > 0:\n            layers[f\"{name}_dropout\"] = nn.Dropout(p=dropout)\n\n        return nn.Sequential(OrderedDict(layers))\n\n    @staticmethod\n    def _block(in_channels, out_channels, kernel_size, name, dropout=0):\n        conv1 = EddyNet.conv_block(in_channels, out_channels, kernel_size, name, 1)\n        conv2 = EddyNet.conv_block(\n            out_channels, out_channels, kernel_size, name, 2, dropout=dropout\n        )\n        return nn.Sequential(conv1, conv2)\n\n    @staticmethod\n    def decoder_block(in_channels, out_channels, kernel_size, name, dropout=0):\n        return EddyNet._block(in_channels, out_channels, kernel_size, name, dropout)\n\n    def forward(self, x):\n        # encoder\n        enc1 = self.encoder1(x)\n        pool1 = self.pool1(enc1)\n\n        enc2 = self.encoder2(pool1)\n        pool2 = self.pool2(enc2)\n\n        enc3 = self.encoder3(pool2)\n        pool3 = self.pool3(enc3)\n\n        # bottleneck?\n        enc4 = self.encoder4(pool3)\n\n        # decoder\n        dec3 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(enc4)\n        dec3 = torch.cat((dec3, enc3), dim=1)\n        dec3 = self.decoder3(dec3)\n\n        dec2 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec3)\n        dec2 = torch.cat((dec2, enc2), dim=1)\n        dec2 = self.decoder2(dec2)\n\n        dec1 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec2)\n        dec1 = torch.cat((dec1, enc1), dim=1)\n        dec1 = self.decoder1(dec1)\n\n        # final layer\n        final = self.final_conv(dec1)\n\n        # softmax\n        final = nn.Softmax(dim=1)(final)\n\n        return final\n\n\nclass Conv2dSame(nn.Module):\n    \"\"\"Manual convolution with same padding\n    https://discuss.pytorch.org/t/same-padding-equivalent-in-pytorch/85121/9\n    Although PyTorch >= 1.10.0 supports ``padding='same'`` as a keyword\n    argument, this does not export to CoreML as of coremltools 5.1.0,\n    so we need to implement the internal torch logic manually.\n    Currently the ``RuntimeError`` is\n    \"PyTorch convert function for op '_convolution_mode' not implemented\"\n    \"\"\"\n\n    def __init__(\n        self, in_channels, out_channels, kernel_size, stride=1, dilation=1, **kwargs\n    ):\n        \"\"\"Wrap base convolution layer\n        See official PyTorch documentation for parameter details\n        https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n        \"\"\"\n        super().__init__()\n        self.conv = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            dilation=dilation,\n            **kwargs,\n        )\n\n        # Setup internal representations\n        kernel_size_ = _pair(kernel_size)\n        dilation_ = _pair(dilation)\n        self._reversed_padding_repeated_twice = [0, 0] * len(kernel_size_)\n\n        # Follow the logic from ``nn/modules/conv.py:_ConvNd``\n        for d, k, i in zip(\n            dilation_, kernel_size_, range(len(kernel_size_) - 1, -1, -1)\n        ):\n            total_padding = d * (k - 1)\n            left_pad = total_padding // 2\n            self._reversed_padding_repeated_twice[2 * i] = left_pad\n            self._reversed_padding_repeated_twice[2 * i + 1] = total_padding - left_pad\n\n    def forward(self, imgs):\n        \"\"\"Setup padding so same spatial dimensions are returned\n        All shapes (input/output) are ``(N, C, W, H)`` convention\n        :param torch.Tensor imgs:\n        :return torch.Tensor:\n        \"\"\"\n        padded = F.pad(imgs, self._reversed_padding_repeated_twice)\n        return self.conv(padded)\n\n\ndef _ntuple(n):\n    \"\"\"Copy from PyTorch since internal function is not importable\n    See ``nn/modules/utils.py:6``\n    \"\"\"\n\n    def parse(x):\n        if isinstance(x, collections.abc.Iterable):\n            return tuple(x)\n        return tuple(repeat(x, n))\n\n    return parse\n\n\n_pair = _ntuple(2)\nFooter",
  "history_output" : "Traceback (most recent call last):\n  File \"/Users/lakshmichetana/gw-workspace/jc4ec955gtt/eddynet.py\", line 6, in <module>\n    import torch\nModuleNotFoundError: No module named 'torch'\n",
  "history_begin_time" : 1664976067982,
  "history_end_time" : 1664976068464,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "t58jtd4jkrr",
  "history_input" : "#Eddynet\nimport collections\nfrom itertools import repeat\nfrom typing import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass EddyNet(nn.Module):\n    \"\"\"\n    PyTorch implementation of EddyNet from Lguensat et al. (2018)\n    Original implementation in TensorFlow: https://github.com/redouanelg/EddyNet\n    \"\"\"\n    def __init__(self, num_classes, num_filters, kernel_size):\n        super(EddyNet, self).__init__()\n        # encoder\n        self.encoder1 = EddyNet._block(1, num_filters, kernel_size, \"enc1\", dropout=0.2)\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder2 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc2\", dropout=0.3\n        )\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder3 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc3\", dropout=0.4\n        )\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder4 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc4\", dropout=0.5\n        )\n\n        # decoder\n        self.decoder3 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec3\", dropout=0.4\n        )\n        self.decoder2 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec2\", dropout=0.3\n        )\n        self.decoder1 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec1\", dropout=0.2\n        )\n\n        # final layer\n        self.final_conv = nn.Conv2d(\n            num_filters, num_classes, kernel_size=1, padding=0, bias=False\n        )\n\n    @staticmethod\n    def conv_block(in_channels, out_channels, kernel_size, name, num, dropout=0):\n        layers = {\n            f\"{name}_conv{num}\": Conv2dSame(in_channels, out_channels, kernel_size),\n            f\"{name}_bn{num}\": nn.BatchNorm2d(out_channels),\n            f\"{name}_relu{num}\": nn.ReLU(inplace=True),\n        }\n        if dropout > 0:\n            layers[f\"{name}_dropout\"] = nn.Dropout(p=dropout)\n\n        return nn.Sequential(OrderedDict(layers))\n\n    @staticmethod\n    def _block(in_channels, out_channels, kernel_size, name, dropout=0):\n        conv1 = EddyNet.conv_block(in_channels, out_channels, kernel_size, name, 1)\n        conv2 = EddyNet.conv_block(\n            out_channels, out_channels, kernel_size, name, 2, dropout=dropout\n        )\n        return nn.Sequential(conv1, conv2)\n\n    @staticmethod\n    def decoder_block(in_channels, out_channels, kernel_size, name, dropout=0):\n        return EddyNet._block(in_channels, out_channels, kernel_size, name, dropout)\n\n    def forward(self, x):\n        # encoder\n        enc1 = self.encoder1(x)\n        pool1 = self.pool1(enc1)\n\n        enc2 = self.encoder2(pool1)\n        pool2 = self.pool2(enc2)\n\n        enc3 = self.encoder3(pool2)\n        pool3 = self.pool3(enc3)\n\n        # bottleneck?\n        enc4 = self.encoder4(pool3)\n\n        # decoder\n        dec3 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(enc4)\n        dec3 = torch.cat((dec3, enc3), dim=1)\n        dec3 = self.decoder3(dec3)\n\n        dec2 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec3)\n        dec2 = torch.cat((dec2, enc2), dim=1)\n        dec2 = self.decoder2(dec2)\n\n        dec1 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec2)\n        dec1 = torch.cat((dec1, enc1), dim=1)\n        dec1 = self.decoder1(dec1)\n\n        # final layer\n        final = self.final_conv(dec1)\n\n        # softmax\n        final = nn.Softmax(dim=1)(final)\n\n        return final\n\n\nclass Conv2dSame(nn.Module):\n    \"\"\"Manual convolution with same padding\n    https://discuss.pytorch.org/t/same-padding-equivalent-in-pytorch/85121/9\n    Although PyTorch >= 1.10.0 supports ``padding='same'`` as a keyword\n    argument, this does not export to CoreML as of coremltools 5.1.0,\n    so we need to implement the internal torch logic manually.\n    Currently the ``RuntimeError`` is\n    \"PyTorch convert function for op '_convolution_mode' not implemented\"\n    \"\"\"\n\n    def __init__(\n        self, in_channels, out_channels, kernel_size, stride=1, dilation=1, **kwargs\n    ):\n        \"\"\"Wrap base convolution layer\n        See official PyTorch documentation for parameter details\n        https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n        \"\"\"\n        super().__init__()\n        self.conv = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            dilation=dilation,\n            **kwargs,\n        )\n\n        # Setup internal representations\n        kernel_size_ = _pair(kernel_size)\n        dilation_ = _pair(dilation)\n        self._reversed_padding_repeated_twice = [0, 0] * len(kernel_size_)\n\n        # Follow the logic from ``nn/modules/conv.py:_ConvNd``\n        for d, k, i in zip(\n            dilation_, kernel_size_, range(len(kernel_size_) - 1, -1, -1)\n        ):\n            total_padding = d * (k - 1)\n            left_pad = total_padding // 2\n            self._reversed_padding_repeated_twice[2 * i] = left_pad\n            self._reversed_padding_repeated_twice[2 * i + 1] = total_padding - left_pad\n\n    def forward(self, imgs):\n        \"\"\"Setup padding so same spatial dimensions are returned\n        All shapes (input/output) are ``(N, C, W, H)`` convention\n        :param torch.Tensor imgs:\n        :return torch.Tensor:\n        \"\"\"\n        padded = F.pad(imgs, self._reversed_padding_repeated_twice)\n        return self.conv(padded)\n\n\ndef _ntuple(n):\n    \"\"\"Copy from PyTorch since internal function is not importable\n    See ``nn/modules/utils.py:6``\n    \"\"\"\n\n    def parse(x):\n        if isinstance(x, collections.abc.Iterable):\n            return tuple(x)\n        return tuple(repeat(x, n))\n\n    return parse\n\n\n_pair = _ntuple(2)\nFooter",
  "history_output" : "Traceback (most recent call last):\n  File \"eddynet.py\", line 173, in <module>\n    Footer\nNameError: name 'Footer' is not defined\n",
  "history_begin_time" : 1664372230458,
  "history_end_time" : 1664373328933,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "yvy4wvz9x3l",
  "history_input" : "#Eddynet\nimport collections\nfrom itertools import repeat\nfrom typing import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass EddyNet(nn.Module):\n    \"\"\"\n    PyTorch implementation of EddyNet from Lguensat et al. (2018)\n    Original implementation in TensorFlow: https://github.com/redouanelg/EddyNet\n    \"\"\"\n    def __init__(self, num_classes, num_filters, kernel_size):\n        super(EddyNet, self).__init__()\n        # encoder\n        self.encoder1 = EddyNet._block(1, num_filters, kernel_size, \"enc1\", dropout=0.2)\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder2 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc2\", dropout=0.3\n        )\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder3 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc3\", dropout=0.4\n        )\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder4 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc4\", dropout=0.5\n        )\n\n        # decoder\n        self.decoder3 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec3\", dropout=0.4\n        )\n        self.decoder2 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec2\", dropout=0.3\n        )\n        self.decoder1 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec1\", dropout=0.2\n        )\n\n        # final layer\n        self.final_conv = nn.Conv2d(\n            num_filters, num_classes, kernel_size=1, padding=0, bias=False\n        )\n\n    @staticmethod\n    def conv_block(in_channels, out_channels, kernel_size, name, num, dropout=0):\n        layers = {\n            f\"{name}_conv{num}\": Conv2dSame(in_channels, out_channels, kernel_size),\n            f\"{name}_bn{num}\": nn.BatchNorm2d(out_channels),\n            f\"{name}_relu{num}\": nn.ReLU(inplace=True),\n        }\n        if dropout > 0:\n            layers[f\"{name}_dropout\"] = nn.Dropout(p=dropout)\n\n        return nn.Sequential(OrderedDict(layers))\n\n    @staticmethod\n    def _block(in_channels, out_channels, kernel_size, name, dropout=0):\n        conv1 = EddyNet.conv_block(in_channels, out_channels, kernel_size, name, 1)\n        conv2 = EddyNet.conv_block(\n            out_channels, out_channels, kernel_size, name, 2, dropout=dropout\n        )\n        return nn.Sequential(conv1, conv2)\n\n    @staticmethod\n    def decoder_block(in_channels, out_channels, kernel_size, name, dropout=0):\n        return EddyNet._block(in_channels, out_channels, kernel_size, name, dropout)\n\n    def forward(self, x):\n        # encoder\n        enc1 = self.encoder1(x)\n        pool1 = self.pool1(enc1)\n\n        enc2 = self.encoder2(pool1)\n        pool2 = self.pool2(enc2)\n\n        enc3 = self.encoder3(pool2)\n        pool3 = self.pool3(enc3)\n\n        # bottleneck?\n        enc4 = self.encoder4(pool3)\n\n        # decoder\n        dec3 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(enc4)\n        dec3 = torch.cat((dec3, enc3), dim=1)\n        dec3 = self.decoder3(dec3)\n\n        dec2 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec3)\n        dec2 = torch.cat((dec2, enc2), dim=1)\n        dec2 = self.decoder2(dec2)\n\n        dec1 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec2)\n        dec1 = torch.cat((dec1, enc1), dim=1)\n        dec1 = self.decoder1(dec1)\n\n        # final layer\n        final = self.final_conv(dec1)\n\n        # softmax\n        final = nn.Softmax(dim=1)(final)\n\n        return final\n\n\nclass Conv2dSame(nn.Module):\n    \"\"\"Manual convolution with same padding\n    https://discuss.pytorch.org/t/same-padding-equivalent-in-pytorch/85121/9\n    Although PyTorch >= 1.10.0 supports ``padding='same'`` as a keyword\n    argument, this does not export to CoreML as of coremltools 5.1.0,\n    so we need to implement the internal torch logic manually.\n    Currently the ``RuntimeError`` is\n    \"PyTorch convert function for op '_convolution_mode' not implemented\"\n    \"\"\"\n\n    def __init__(\n        self, in_channels, out_channels, kernel_size, stride=1, dilation=1, **kwargs\n    ):\n        \"\"\"Wrap base convolution layer\n        See official PyTorch documentation for parameter details\n        https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n        \"\"\"\n        super().__init__()\n        self.conv = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            dilation=dilation,\n            **kwargs,\n        )\n\n        # Setup internal representations\n        kernel_size_ = _pair(kernel_size)\n        dilation_ = _pair(dilation)\n        self._reversed_padding_repeated_twice = [0, 0] * len(kernel_size_)\n\n        # Follow the logic from ``nn/modules/conv.py:_ConvNd``\n        for d, k, i in zip(\n            dilation_, kernel_size_, range(len(kernel_size_) - 1, -1, -1)\n        ):\n            total_padding = d * (k - 1)\n            left_pad = total_padding // 2\n            self._reversed_padding_repeated_twice[2 * i] = left_pad\n            self._reversed_padding_repeated_twice[2 * i + 1] = total_padding - left_pad\n\n    def forward(self, imgs):\n        \"\"\"Setup padding so same spatial dimensions are returned\n        All shapes (input/output) are ``(N, C, W, H)`` convention\n        :param torch.Tensor imgs:\n        :return torch.Tensor:\n        \"\"\"\n        padded = F.pad(imgs, self._reversed_padding_repeated_twice)\n        return self.conv(padded)\n\n\ndef _ntuple(n):\n    \"\"\"Copy from PyTorch since internal function is not importable\n    See ``nn/modules/utils.py:6``\n    \"\"\"\n\n    def parse(x):\n        if isinstance(x, collections.abc.Iterable):\n            return tuple(x)\n        return tuple(repeat(x, n))\n\n    return parse\n\n\n_pair = _ntuple(2)\nFooter",
  "history_output" : "Traceback (most recent call last):\n  File \"eddynet.py\", line 173, in <module>\n    Footer\nNameError: name 'Footer' is not defined\n",
  "history_begin_time" : 1664371666395,
  "history_end_time" : 1664371680832,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "u4njvxadog3",
  "history_input" : "#Eddynet\nimport collections\nfrom itertools import repeat\nfrom typing import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass EddyNet(nn.Module):\n    \"\"\"\n    PyTorch implementation of EddyNet from Lguensat et al. (2018)\n    Original implementation in TensorFlow: https://github.com/redouanelg/EddyNet\n    \"\"\"\n    def __init__(self, num_classes, num_filters, kernel_size):\n        super(EddyNet, self).__init__()\n        # encoder\n        self.encoder1 = EddyNet._block(1, num_filters, kernel_size, \"enc1\", dropout=0.2)\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder2 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc2\", dropout=0.3\n        )\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder3 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc3\", dropout=0.4\n        )\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder4 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc4\", dropout=0.5\n        )\n\n        # decoder\n        self.decoder3 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec3\", dropout=0.4\n        )\n        self.decoder2 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec2\", dropout=0.3\n        )\n        self.decoder1 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec1\", dropout=0.2\n        )\n\n        # final layer\n        self.final_conv = nn.Conv2d(\n            num_filters, num_classes, kernel_size=1, padding=0, bias=False\n        )\n\n    @staticmethod\n    def conv_block(in_channels, out_channels, kernel_size, name, num, dropout=0):\n        layers = {\n            f\"{name}_conv{num}\": Conv2dSame(in_channels, out_channels, kernel_size),\n            f\"{name}_bn{num}\": nn.BatchNorm2d(out_channels),\n            f\"{name}_relu{num}\": nn.ReLU(inplace=True),\n        }\n        if dropout > 0:\n            layers[f\"{name}_dropout\"] = nn.Dropout(p=dropout)\n\n        return nn.Sequential(OrderedDict(layers))\n\n    @staticmethod\n    def _block(in_channels, out_channels, kernel_size, name, dropout=0):\n        conv1 = EddyNet.conv_block(in_channels, out_channels, kernel_size, name, 1)\n        conv2 = EddyNet.conv_block(\n            out_channels, out_channels, kernel_size, name, 2, dropout=dropout\n        )\n        return nn.Sequential(conv1, conv2)\n\n    @staticmethod\n    def decoder_block(in_channels, out_channels, kernel_size, name, dropout=0):\n        return EddyNet._block(in_channels, out_channels, kernel_size, name, dropout)\n\n    def forward(self, x):\n        # encoder\n        enc1 = self.encoder1(x)\n        pool1 = self.pool1(enc1)\n\n        enc2 = self.encoder2(pool1)\n        pool2 = self.pool2(enc2)\n\n        enc3 = self.encoder3(pool2)\n        pool3 = self.pool3(enc3)\n\n        # bottleneck?\n        enc4 = self.encoder4(pool3)\n\n        # decoder\n        dec3 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(enc4)\n        dec3 = torch.cat((dec3, enc3), dim=1)\n        dec3 = self.decoder3(dec3)\n\n        dec2 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec3)\n        dec2 = torch.cat((dec2, enc2), dim=1)\n        dec2 = self.decoder2(dec2)\n\n        dec1 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec2)\n        dec1 = torch.cat((dec1, enc1), dim=1)\n        dec1 = self.decoder1(dec1)\n\n        # final layer\n        final = self.final_conv(dec1)\n\n        # softmax\n        final = nn.Softmax(dim=1)(final)\n\n        return final\n\n\nclass Conv2dSame(nn.Module):\n    \"\"\"Manual convolution with same padding\n    https://discuss.pytorch.org/t/same-padding-equivalent-in-pytorch/85121/9\n    Although PyTorch >= 1.10.0 supports ``padding='same'`` as a keyword\n    argument, this does not export to CoreML as of coremltools 5.1.0,\n    so we need to implement the internal torch logic manually.\n    Currently the ``RuntimeError`` is\n    \"PyTorch convert function for op '_convolution_mode' not implemented\"\n    \"\"\"\n\n    def __init__(\n        self, in_channels, out_channels, kernel_size, stride=1, dilation=1, **kwargs\n    ):\n        \"\"\"Wrap base convolution layer\n        See official PyTorch documentation for parameter details\n        https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n        \"\"\"\n        super().__init__()\n        self.conv = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            dilation=dilation,\n            **kwargs,\n        )\n\n        # Setup internal representations\n        kernel_size_ = _pair(kernel_size)\n        dilation_ = _pair(dilation)\n        self._reversed_padding_repeated_twice = [0, 0] * len(kernel_size_)\n\n        # Follow the logic from ``nn/modules/conv.py:_ConvNd``\n        for d, k, i in zip(\n            dilation_, kernel_size_, range(len(kernel_size_) - 1, -1, -1)\n        ):\n            total_padding = d * (k - 1)\n            left_pad = total_padding // 2\n            self._reversed_padding_repeated_twice[2 * i] = left_pad\n            self._reversed_padding_repeated_twice[2 * i + 1] = total_padding - left_pad\n\n    def forward(self, imgs):\n        \"\"\"Setup padding so same spatial dimensions are returned\n        All shapes (input/output) are ``(N, C, W, H)`` convention\n        :param torch.Tensor imgs:\n        :return torch.Tensor:\n        \"\"\"\n        padded = F.pad(imgs, self._reversed_padding_repeated_twice)\n        return self.conv(padded)\n\n\ndef _ntuple(n):\n    \"\"\"Copy from PyTorch since internal function is not importable\n    See ``nn/modules/utils.py:6``\n    \"\"\"\n\n    def parse(x):\n        if isinstance(x, collections.abc.Iterable):\n            return tuple(x)\n        return tuple(repeat(x, n))\n\n    return parse\n\n\n_pair = _ntuple(2)\nFooter",
  "history_output" : "Cannot run program \"python3.8\" (in directory \"C:\\Users\\user\\gw-workspace\\u4njvxadog3\"): CreateProcess error=2, The system cannot find the file specified",
  "history_begin_time" : 1664371276748,
  "history_end_time" : 1664371277102,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "4mrz2ew18cs",
  "history_input" : "#Eddynet\nimport collections\nfrom itertools import repeat\nfrom typing import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass EddyNet(nn.Module):\n    \"\"\"\n    PyTorch implementation of EddyNet from Lguensat et al. (2018)\n    Original implementation in TensorFlow: https://github.com/redouanelg/EddyNet\n    \"\"\"\n    def __init__(self, num_classes, num_filters, kernel_size):\n        super(EddyNet, self).__init__()\n        # encoder\n        self.encoder1 = EddyNet._block(1, num_filters, kernel_size, \"enc1\", dropout=0.2)\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder2 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc2\", dropout=0.3\n        )\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder3 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc3\", dropout=0.4\n        )\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder4 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc4\", dropout=0.5\n        )\n\n        # decoder\n        self.decoder3 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec3\", dropout=0.4\n        )\n        self.decoder2 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec2\", dropout=0.3\n        )\n        self.decoder1 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec1\", dropout=0.2\n        )\n\n        # final layer\n        self.final_conv = nn.Conv2d(\n            num_filters, num_classes, kernel_size=1, padding=0, bias=False\n        )\n\n    @staticmethod\n    def conv_block(in_channels, out_channels, kernel_size, name, num, dropout=0):\n        layers = {\n            f\"{name}_conv{num}\": Conv2dSame(in_channels, out_channels, kernel_size),\n            f\"{name}_bn{num}\": nn.BatchNorm2d(out_channels),\n            f\"{name}_relu{num}\": nn.ReLU(inplace=True),\n        }\n        if dropout > 0:\n            layers[f\"{name}_dropout\"] = nn.Dropout(p=dropout)\n\n        return nn.Sequential(OrderedDict(layers))\n\n    @staticmethod\n    def _block(in_channels, out_channels, kernel_size, name, dropout=0):\n        conv1 = EddyNet.conv_block(in_channels, out_channels, kernel_size, name, 1)\n        conv2 = EddyNet.conv_block(\n            out_channels, out_channels, kernel_size, name, 2, dropout=dropout\n        )\n        return nn.Sequential(conv1, conv2)\n\n    @staticmethod\n    def decoder_block(in_channels, out_channels, kernel_size, name, dropout=0):\n        return EddyNet._block(in_channels, out_channels, kernel_size, name, dropout)\n\n    def forward(self, x):\n        # encoder\n        enc1 = self.encoder1(x)\n        pool1 = self.pool1(enc1)\n\n        enc2 = self.encoder2(pool1)\n        pool2 = self.pool2(enc2)\n\n        enc3 = self.encoder3(pool2)\n        pool3 = self.pool3(enc3)\n\n        # bottleneck?\n        enc4 = self.encoder4(pool3)\n\n        # decoder\n        dec3 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(enc4)\n        dec3 = torch.cat((dec3, enc3), dim=1)\n        dec3 = self.decoder3(dec3)\n\n        dec2 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec3)\n        dec2 = torch.cat((dec2, enc2), dim=1)\n        dec2 = self.decoder2(dec2)\n\n        dec1 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec2)\n        dec1 = torch.cat((dec1, enc1), dim=1)\n        dec1 = self.decoder1(dec1)\n\n        # final layer\n        final = self.final_conv(dec1)\n\n        # softmax\n        final = nn.Softmax(dim=1)(final)\n\n        return final\n\n\nclass Conv2dSame(nn.Module):\n    \"\"\"Manual convolution with same padding\n    https://discuss.pytorch.org/t/same-padding-equivalent-in-pytorch/85121/9\n    Although PyTorch >= 1.10.0 supports ``padding='same'`` as a keyword\n    argument, this does not export to CoreML as of coremltools 5.1.0,\n    so we need to implement the internal torch logic manually.\n    Currently the ``RuntimeError`` is\n    \"PyTorch convert function for op '_convolution_mode' not implemented\"\n    \"\"\"\n\n    def __init__(\n        self, in_channels, out_channels, kernel_size, stride=1, dilation=1, **kwargs\n    ):\n        \"\"\"Wrap base convolution layer\n        See official PyTorch documentation for parameter details\n        https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n        \"\"\"\n        super().__init__()\n        self.conv = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            dilation=dilation,\n            **kwargs,\n        )\n\n        # Setup internal representations\n        kernel_size_ = _pair(kernel_size)\n        dilation_ = _pair(dilation)\n        self._reversed_padding_repeated_twice = [0, 0] * len(kernel_size_)\n\n        # Follow the logic from ``nn/modules/conv.py:_ConvNd``\n        for d, k, i in zip(\n            dilation_, kernel_size_, range(len(kernel_size_) - 1, -1, -1)\n        ):\n            total_padding = d * (k - 1)\n            left_pad = total_padding // 2\n            self._reversed_padding_repeated_twice[2 * i] = left_pad\n            self._reversed_padding_repeated_twice[2 * i + 1] = total_padding - left_pad\n\n    def forward(self, imgs):\n        \"\"\"Setup padding so same spatial dimensions are returned\n        All shapes (input/output) are ``(N, C, W, H)`` convention\n        :param torch.Tensor imgs:\n        :return torch.Tensor:\n        \"\"\"\n        padded = F.pad(imgs, self._reversed_padding_repeated_twice)\n        return self.conv(padded)\n\n\ndef _ntuple(n):\n    \"\"\"Copy from PyTorch since internal function is not importable\n    See ``nn/modules/utils.py:6``\n    \"\"\"\n\n    def parse(x):\n        if isinstance(x, collections.abc.Iterable):\n            return tuple(x)\n        return tuple(repeat(x, n))\n\n    return parse\n\n\n_pair = _ntuple(2)\nFooter",
  "history_output" : "Traceback (most recent call last):\n  File \"eddynet.py\", line 173, in <module>\n    Footer\nNameError: name 'Footer' is not defined\n",
  "history_begin_time" : 1664370967165,
  "history_end_time" : 1664370971872,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "eaoagrirh42",
  "history_input" : "#Eddynet\nimport collections\nfrom itertools import repeat\nfrom typing import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass EddyNet(nn.Module):\n    \"\"\"\n    PyTorch implementation of EddyNet from Lguensat et al. (2018)\n    Original implementation in TensorFlow: https://github.com/redouanelg/EddyNet\n    \"\"\"\n    def __init__(self, num_classes, num_filters, kernel_size):\n        super(EddyNet, self).__init__()\n        # encoder\n        self.encoder1 = EddyNet._block(1, num_filters, kernel_size, \"enc1\", dropout=0.2)\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder2 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc2\", dropout=0.3\n        )\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder3 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc3\", dropout=0.4\n        )\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder4 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc4\", dropout=0.5\n        )\n\n        # decoder\n        self.decoder3 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec3\", dropout=0.4\n        )\n        self.decoder2 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec2\", dropout=0.3\n        )\n        self.decoder1 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec1\", dropout=0.2\n        )\n\n        # final layer\n        self.final_conv = nn.Conv2d(\n            num_filters, num_classes, kernel_size=1, padding=0, bias=False\n        )\n\n    @staticmethod\n    def conv_block(in_channels, out_channels, kernel_size, name, num, dropout=0):\n        layers = {\n            f\"{name}_conv{num}\": Conv2dSame(in_channels, out_channels, kernel_size),\n            f\"{name}_bn{num}\": nn.BatchNorm2d(out_channels),\n            f\"{name}_relu{num}\": nn.ReLU(inplace=True),\n        }\n        if dropout > 0:\n            layers[f\"{name}_dropout\"] = nn.Dropout(p=dropout)\n\n        return nn.Sequential(OrderedDict(layers))\n\n    @staticmethod\n    def _block(in_channels, out_channels, kernel_size, name, dropout=0):\n        conv1 = EddyNet.conv_block(in_channels, out_channels, kernel_size, name, 1)\n        conv2 = EddyNet.conv_block(\n            out_channels, out_channels, kernel_size, name, 2, dropout=dropout\n        )\n        return nn.Sequential(conv1, conv2)\n\n    @staticmethod\n    def decoder_block(in_channels, out_channels, kernel_size, name, dropout=0):\n        return EddyNet._block(in_channels, out_channels, kernel_size, name, dropout)\n\n    def forward(self, x):\n        # encoder\n        enc1 = self.encoder1(x)\n        pool1 = self.pool1(enc1)\n\n        enc2 = self.encoder2(pool1)\n        pool2 = self.pool2(enc2)\n\n        enc3 = self.encoder3(pool2)\n        pool3 = self.pool3(enc3)\n\n        # bottleneck?\n        enc4 = self.encoder4(pool3)\n\n        # decoder\n        dec3 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(enc4)\n        dec3 = torch.cat((dec3, enc3), dim=1)\n        dec3 = self.decoder3(dec3)\n\n        dec2 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec3)\n        dec2 = torch.cat((dec2, enc2), dim=1)\n        dec2 = self.decoder2(dec2)\n\n        dec1 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec2)\n        dec1 = torch.cat((dec1, enc1), dim=1)\n        dec1 = self.decoder1(dec1)\n\n        # final layer\n        final = self.final_conv(dec1)\n\n        # softmax\n        final = nn.Softmax(dim=1)(final)\n\n        return final\n\n\nclass Conv2dSame(nn.Module):\n    \"\"\"Manual convolution with same padding\n    https://discuss.pytorch.org/t/same-padding-equivalent-in-pytorch/85121/9\n    Although PyTorch >= 1.10.0 supports ``padding='same'`` as a keyword\n    argument, this does not export to CoreML as of coremltools 5.1.0,\n    so we need to implement the internal torch logic manually.\n    Currently the ``RuntimeError`` is\n    \"PyTorch convert function for op '_convolution_mode' not implemented\"\n    \"\"\"\n\n    def __init__(\n        self, in_channels, out_channels, kernel_size, stride=1, dilation=1, **kwargs\n    ):\n        \"\"\"Wrap base convolution layer\n        See official PyTorch documentation for parameter details\n        https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n        \"\"\"\n        super().__init__()\n        self.conv = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            dilation=dilation,\n            **kwargs,\n        )\n\n        # Setup internal representations\n        kernel_size_ = _pair(kernel_size)\n        dilation_ = _pair(dilation)\n        self._reversed_padding_repeated_twice = [0, 0] * len(kernel_size_)\n\n        # Follow the logic from ``nn/modules/conv.py:_ConvNd``\n        for d, k, i in zip(\n            dilation_, kernel_size_, range(len(kernel_size_) - 1, -1, -1)\n        ):\n            total_padding = d * (k - 1)\n            left_pad = total_padding // 2\n            self._reversed_padding_repeated_twice[2 * i] = left_pad\n            self._reversed_padding_repeated_twice[2 * i + 1] = total_padding - left_pad\n\n    def forward(self, imgs):\n        \"\"\"Setup padding so same spatial dimensions are returned\n        All shapes (input/output) are ``(N, C, W, H)`` convention\n        :param torch.Tensor imgs:\n        :return torch.Tensor:\n        \"\"\"\n        padded = F.pad(imgs, self._reversed_padding_repeated_twice)\n        return self.conv(padded)\n\n\ndef _ntuple(n):\n    \"\"\"Copy from PyTorch since internal function is not importable\n    See ``nn/modules/utils.py:6``\n    \"\"\"\n\n    def parse(x):\n        if isinstance(x, collections.abc.Iterable):\n            return tuple(x)\n        return tuple(repeat(x, n))\n\n    return parse\n\n\n_pair = _ntuple(2)\nFooter",
  "history_output" : "Traceback (most recent call last):\n  File \"eddynet.py\", line 173, in <module>\n    Footer\nNameError: name 'Footer' is not defined\n",
  "history_begin_time" : 1664370562709,
  "history_end_time" : 1664370574868,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "gxhcaxo4dmh",
  "history_input" : "#Eddynet\nimport collections\nfrom itertools import repeat\nfrom typing import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass EddyNet(nn.Module):\n    \"\"\"\n    PyTorch implementation of EddyNet from Lguensat et al. (2018)\n    Original implementation in TensorFlow: https://github.com/redouanelg/EddyNet\n    \"\"\"\n    def __init__(self, num_classes, num_filters, kernel_size):\n        super(EddyNet, self).__init__()\n        # encoder\n        self.encoder1 = EddyNet._block(1, num_filters, kernel_size, \"enc1\", dropout=0.2)\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder2 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc2\", dropout=0.3\n        )\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder3 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc3\", dropout=0.4\n        )\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder4 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc4\", dropout=0.5\n        )\n\n        # decoder\n        self.decoder3 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec3\", dropout=0.4\n        )\n        self.decoder2 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec2\", dropout=0.3\n        )\n        self.decoder1 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec1\", dropout=0.2\n        )\n\n        # final layer\n        self.final_conv = nn.Conv2d(\n            num_filters, num_classes, kernel_size=1, padding=0, bias=False\n        )\n\n    @staticmethod\n    def conv_block(in_channels, out_channels, kernel_size, name, num, dropout=0):\n        layers = {\n            f\"{name}_conv{num}\": Conv2dSame(in_channels, out_channels, kernel_size),\n            f\"{name}_bn{num}\": nn.BatchNorm2d(out_channels),\n            f\"{name}_relu{num}\": nn.ReLU(inplace=True),\n        }\n        if dropout > 0:\n            layers[f\"{name}_dropout\"] = nn.Dropout(p=dropout)\n\n        return nn.Sequential(OrderedDict(layers))\n\n    @staticmethod\n    def _block(in_channels, out_channels, kernel_size, name, dropout=0):\n        conv1 = EddyNet.conv_block(in_channels, out_channels, kernel_size, name, 1)\n        conv2 = EddyNet.conv_block(\n            out_channels, out_channels, kernel_size, name, 2, dropout=dropout\n        )\n        return nn.Sequential(conv1, conv2)\n\n    @staticmethod\n    def decoder_block(in_channels, out_channels, kernel_size, name, dropout=0):\n        return EddyNet._block(in_channels, out_channels, kernel_size, name, dropout)\n\n    def forward(self, x):\n        # encoder\n        enc1 = self.encoder1(x)\n        pool1 = self.pool1(enc1)\n\n        enc2 = self.encoder2(pool1)\n        pool2 = self.pool2(enc2)\n\n        enc3 = self.encoder3(pool2)\n        pool3 = self.pool3(enc3)\n\n        # bottleneck?\n        enc4 = self.encoder4(pool3)\n\n        # decoder\n        dec3 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(enc4)\n        dec3 = torch.cat((dec3, enc3), dim=1)\n        dec3 = self.decoder3(dec3)\n\n        dec2 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec3)\n        dec2 = torch.cat((dec2, enc2), dim=1)\n        dec2 = self.decoder2(dec2)\n\n        dec1 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec2)\n        dec1 = torch.cat((dec1, enc1), dim=1)\n        dec1 = self.decoder1(dec1)\n\n        # final layer\n        final = self.final_conv(dec1)\n\n        # softmax\n        final = nn.Softmax(dim=1)(final)\n\n        return final\n\n\nclass Conv2dSame(nn.Module):\n    \"\"\"Manual convolution with same padding\n    https://discuss.pytorch.org/t/same-padding-equivalent-in-pytorch/85121/9\n    Although PyTorch >= 1.10.0 supports ``padding='same'`` as a keyword\n    argument, this does not export to CoreML as of coremltools 5.1.0,\n    so we need to implement the internal torch logic manually.\n    Currently the ``RuntimeError`` is\n    \"PyTorch convert function for op '_convolution_mode' not implemented\"\n    \"\"\"\n\n    def __init__(\n        self, in_channels, out_channels, kernel_size, stride=1, dilation=1, **kwargs\n    ):\n        \"\"\"Wrap base convolution layer\n        See official PyTorch documentation for parameter details\n        https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n        \"\"\"\n        super().__init__()\n        self.conv = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            dilation=dilation,\n            **kwargs,\n        )\n\n        # Setup internal representations\n        kernel_size_ = _pair(kernel_size)\n        dilation_ = _pair(dilation)\n        self._reversed_padding_repeated_twice = [0, 0] * len(kernel_size_)\n\n        # Follow the logic from ``nn/modules/conv.py:_ConvNd``\n        for d, k, i in zip(\n            dilation_, kernel_size_, range(len(kernel_size_) - 1, -1, -1)\n        ):\n            total_padding = d * (k - 1)\n            left_pad = total_padding // 2\n            self._reversed_padding_repeated_twice[2 * i] = left_pad\n            self._reversed_padding_repeated_twice[2 * i + 1] = total_padding - left_pad\n\n    def forward(self, imgs):\n        \"\"\"Setup padding so same spatial dimensions are returned\n        All shapes (input/output) are ``(N, C, W, H)`` convention\n        :param torch.Tensor imgs:\n        :return torch.Tensor:\n        \"\"\"\n        padded = F.pad(imgs, self._reversed_padding_repeated_twice)\n        return self.conv(padded)\n\n\ndef _ntuple(n):\n    \"\"\"Copy from PyTorch since internal function is not importable\n    See ``nn/modules/utils.py:6``\n    \"\"\"\n\n    def parse(x):\n        if isinstance(x, collections.abc.Iterable):\n            return tuple(x)\n        return tuple(repeat(x, n))\n\n    return parse\n\n\n_pair = _ntuple(2)\nFooter",
  "history_output" : "Traceback (most recent call last):\n  File \"eddynet.py\", line 173, in <module>\n    Footer\nNameError: name 'Footer' is not defined\n",
  "history_begin_time" : 1664363392212,
  "history_end_time" : 1664364123188,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "eshq7lc8hpg",
  "history_input" : "#Eddynet\nimport collections\nfrom itertools import repeat\nfrom typing import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass EddyNet(nn.Module):\n    \"\"\"\n    PyTorch implementation of EddyNet from Lguensat et al. (2018)\n    Original implementation in TensorFlow: https://github.com/redouanelg/EddyNet\n    \"\"\"\n    def __init__(self, num_classes, num_filters, kernel_size):\n        super(EddyNet, self).__init__()\n        # encoder\n        self.encoder1 = EddyNet._block(1, num_filters, kernel_size, \"enc1\", dropout=0.2)\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder2 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc2\", dropout=0.3\n        )\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder3 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc3\", dropout=0.4\n        )\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder4 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc4\", dropout=0.5\n        )\n\n        # decoder\n        self.decoder3 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec3\", dropout=0.4\n        )\n        self.decoder2 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec2\", dropout=0.3\n        )\n        self.decoder1 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec1\", dropout=0.2\n        )\n\n        # final layer\n        self.final_conv = nn.Conv2d(\n            num_filters, num_classes, kernel_size=1, padding=0, bias=False\n        )\n\n    @staticmethod\n    def conv_block(in_channels, out_channels, kernel_size, name, num, dropout=0):\n        layers = {\n            f\"{name}_conv{num}\": Conv2dSame(in_channels, out_channels, kernel_size),\n            f\"{name}_bn{num}\": nn.BatchNorm2d(out_channels),\n            f\"{name}_relu{num}\": nn.ReLU(inplace=True),\n        }\n        if dropout > 0:\n            layers[f\"{name}_dropout\"] = nn.Dropout(p=dropout)\n\n        return nn.Sequential(OrderedDict(layers))\n\n    @staticmethod\n    def _block(in_channels, out_channels, kernel_size, name, dropout=0):\n        conv1 = EddyNet.conv_block(in_channels, out_channels, kernel_size, name, 1)\n        conv2 = EddyNet.conv_block(\n            out_channels, out_channels, kernel_size, name, 2, dropout=dropout\n        )\n        return nn.Sequential(conv1, conv2)\n\n    @staticmethod\n    def decoder_block(in_channels, out_channels, kernel_size, name, dropout=0):\n        return EddyNet._block(in_channels, out_channels, kernel_size, name, dropout)\n\n    def forward(self, x):\n        # encoder\n        enc1 = self.encoder1(x)\n        pool1 = self.pool1(enc1)\n\n        enc2 = self.encoder2(pool1)\n        pool2 = self.pool2(enc2)\n\n        enc3 = self.encoder3(pool2)\n        pool3 = self.pool3(enc3)\n\n        # bottleneck?\n        enc4 = self.encoder4(pool3)\n\n        # decoder\n        dec3 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(enc4)\n        dec3 = torch.cat((dec3, enc3), dim=1)\n        dec3 = self.decoder3(dec3)\n\n        dec2 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec3)\n        dec2 = torch.cat((dec2, enc2), dim=1)\n        dec2 = self.decoder2(dec2)\n\n        dec1 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec2)\n        dec1 = torch.cat((dec1, enc1), dim=1)\n        dec1 = self.decoder1(dec1)\n\n        # final layer\n        final = self.final_conv(dec1)\n\n        # softmax\n        final = nn.Softmax(dim=1)(final)\n\n        return final\n\n\nclass Conv2dSame(nn.Module):\n    \"\"\"Manual convolution with same padding\n    https://discuss.pytorch.org/t/same-padding-equivalent-in-pytorch/85121/9\n    Although PyTorch >= 1.10.0 supports ``padding='same'`` as a keyword\n    argument, this does not export to CoreML as of coremltools 5.1.0,\n    so we need to implement the internal torch logic manually.\n    Currently the ``RuntimeError`` is\n    \"PyTorch convert function for op '_convolution_mode' not implemented\"\n    \"\"\"\n\n    def __init__(\n        self, in_channels, out_channels, kernel_size, stride=1, dilation=1, **kwargs\n    ):\n        \"\"\"Wrap base convolution layer\n        See official PyTorch documentation for parameter details\n        https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n        \"\"\"\n        super().__init__()\n        self.conv = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            dilation=dilation,\n            **kwargs,\n        )\n\n        # Setup internal representations\n        kernel_size_ = _pair(kernel_size)\n        dilation_ = _pair(dilation)\n        self._reversed_padding_repeated_twice = [0, 0] * len(kernel_size_)\n\n        # Follow the logic from ``nn/modules/conv.py:_ConvNd``\n        for d, k, i in zip(\n            dilation_, kernel_size_, range(len(kernel_size_) - 1, -1, -1)\n        ):\n            total_padding = d * (k - 1)\n            left_pad = total_padding // 2\n            self._reversed_padding_repeated_twice[2 * i] = left_pad\n            self._reversed_padding_repeated_twice[2 * i + 1] = total_padding - left_pad\n\n    def forward(self, imgs):\n        \"\"\"Setup padding so same spatial dimensions are returned\n        All shapes (input/output) are ``(N, C, W, H)`` convention\n        :param torch.Tensor imgs:\n        :return torch.Tensor:\n        \"\"\"\n        padded = F.pad(imgs, self._reversed_padding_repeated_twice)\n        return self.conv(padded)\n\n\ndef _ntuple(n):\n    \"\"\"Copy from PyTorch since internal function is not importable\n    See ``nn/modules/utils.py:6``\n    \"\"\"\n\n    def parse(x):\n        if isinstance(x, collections.abc.Iterable):\n            return tuple(x)\n        return tuple(repeat(x, n))\n\n    return parse\n\n\n_pair = _ntuple(2)\nFooter",
  "history_output" : "Traceback (most recent call last):\n  File \"eddynet.py\", line 173, in <module>\n    Footer\nNameError: name 'Footer' is not defined\n",
  "history_begin_time" : 1664362585711,
  "history_end_time" : 1664363357388,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "5ftipjg87hp",
  "history_input" : "#Eddynet\nimport collections\nfrom itertools import repeat\nfrom typing import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass EddyNet(nn.Module):\n    \"\"\"\n    PyTorch implementation of EddyNet from Lguensat et al. (2018)\n    Original implementation in TensorFlow: https://github.com/redouanelg/EddyNet\n    \"\"\"\n    def __init__(self, num_classes, num_filters, kernel_size):\n        super(EddyNet, self).__init__()\n        # encoder\n        self.encoder1 = EddyNet._block(1, num_filters, kernel_size, \"enc1\", dropout=0.2)\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder2 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc2\", dropout=0.3\n        )\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder3 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc3\", dropout=0.4\n        )\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder4 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc4\", dropout=0.5\n        )\n\n        # decoder\n        self.decoder3 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec3\", dropout=0.4\n        )\n        self.decoder2 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec2\", dropout=0.3\n        )\n        self.decoder1 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec1\", dropout=0.2\n        )\n\n        # final layer\n        self.final_conv = nn.Conv2d(\n            num_filters, num_classes, kernel_size=1, padding=0, bias=False\n        )\n\n    @staticmethod\n    def conv_block(in_channels, out_channels, kernel_size, name, num, dropout=0):\n        layers = {\n            f\"{name}_conv{num}\": Conv2dSame(in_channels, out_channels, kernel_size),\n            f\"{name}_bn{num}\": nn.BatchNorm2d(out_channels),\n            f\"{name}_relu{num}\": nn.ReLU(inplace=True),\n        }\n        if dropout > 0:\n            layers[f\"{name}_dropout\"] = nn.Dropout(p=dropout)\n\n        return nn.Sequential(OrderedDict(layers))\n\n    @staticmethod\n    def _block(in_channels, out_channels, kernel_size, name, dropout=0):\n        conv1 = EddyNet.conv_block(in_channels, out_channels, kernel_size, name, 1)\n        conv2 = EddyNet.conv_block(\n            out_channels, out_channels, kernel_size, name, 2, dropout=dropout\n        )\n        return nn.Sequential(conv1, conv2)\n\n    @staticmethod\n    def decoder_block(in_channels, out_channels, kernel_size, name, dropout=0):\n        return EddyNet._block(in_channels, out_channels, kernel_size, name, dropout)\n\n    def forward(self, x):\n        # encoder\n        enc1 = self.encoder1(x)\n        pool1 = self.pool1(enc1)\n\n        enc2 = self.encoder2(pool1)\n        pool2 = self.pool2(enc2)\n\n        enc3 = self.encoder3(pool2)\n        pool3 = self.pool3(enc3)\n\n        # bottleneck?\n        enc4 = self.encoder4(pool3)\n\n        # decoder\n        dec3 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(enc4)\n        dec3 = torch.cat((dec3, enc3), dim=1)\n        dec3 = self.decoder3(dec3)\n\n        dec2 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec3)\n        dec2 = torch.cat((dec2, enc2), dim=1)\n        dec2 = self.decoder2(dec2)\n\n        dec1 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec2)\n        dec1 = torch.cat((dec1, enc1), dim=1)\n        dec1 = self.decoder1(dec1)\n\n        # final layer\n        final = self.final_conv(dec1)\n\n        # softmax\n        final = nn.Softmax(dim=1)(final)\n\n        return final\n\n\nclass Conv2dSame(nn.Module):\n    \"\"\"Manual convolution with same padding\n    https://discuss.pytorch.org/t/same-padding-equivalent-in-pytorch/85121/9\n    Although PyTorch >= 1.10.0 supports ``padding='same'`` as a keyword\n    argument, this does not export to CoreML as of coremltools 5.1.0,\n    so we need to implement the internal torch logic manually.\n    Currently the ``RuntimeError`` is\n    \"PyTorch convert function for op '_convolution_mode' not implemented\"\n    \"\"\"\n\n    def __init__(\n        self, in_channels, out_channels, kernel_size, stride=1, dilation=1, **kwargs\n    ):\n        \"\"\"Wrap base convolution layer\n        See official PyTorch documentation for parameter details\n        https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n        \"\"\"\n        super().__init__()\n        self.conv = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            dilation=dilation,\n            **kwargs,\n        )\n\n        # Setup internal representations\n        kernel_size_ = _pair(kernel_size)\n        dilation_ = _pair(dilation)\n        self._reversed_padding_repeated_twice = [0, 0] * len(kernel_size_)\n\n        # Follow the logic from ``nn/modules/conv.py:_ConvNd``\n        for d, k, i in zip(\n            dilation_, kernel_size_, range(len(kernel_size_) - 1, -1, -1)\n        ):\n            total_padding = d * (k - 1)\n            left_pad = total_padding // 2\n            self._reversed_padding_repeated_twice[2 * i] = left_pad\n            self._reversed_padding_repeated_twice[2 * i + 1] = total_padding - left_pad\n\n    def forward(self, imgs):\n        \"\"\"Setup padding so same spatial dimensions are returned\n        All shapes (input/output) are ``(N, C, W, H)`` convention\n        :param torch.Tensor imgs:\n        :return torch.Tensor:\n        \"\"\"\n        padded = F.pad(imgs, self._reversed_padding_repeated_twice)\n        return self.conv(padded)\n\n\ndef _ntuple(n):\n    \"\"\"Copy from PyTorch since internal function is not importable\n    See ``nn/modules/utils.py:6``\n    \"\"\"\n\n    def parse(x):\n        if isinstance(x, collections.abc.Iterable):\n            return tuple(x)\n        return tuple(repeat(x, n))\n\n    return parse\n\n\n_pair = _ntuple(2)\nFooter",
  "history_output" : "Traceback (most recent call last):\n  File \"eddynet.py\", line 173, in <module>\n    Footer\nNameError: name 'Footer' is not defined\n",
  "history_begin_time" : 1664334057543,
  "history_end_time" : 1664362462504,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "g1mrvko47by",
  "history_input" : "#Eddynet\nimport collections\nfrom itertools import repeat\nfrom typing import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass EddyNet(nn.Module):\n    \"\"\"\n    PyTorch implementation of EddyNet from Lguensat et al. (2018)\n    Original implementation in TensorFlow: https://github.com/redouanelg/EddyNet\n    \"\"\"\n    def __init__(self, num_classes, num_filters, kernel_size):\n        super(EddyNet, self).__init__()\n        # encoder\n        self.encoder1 = EddyNet._block(1, num_filters, kernel_size, \"enc1\", dropout=0.2)\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder2 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc2\", dropout=0.3\n        )\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder3 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc3\", dropout=0.4\n        )\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder4 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc4\", dropout=0.5\n        )\n\n        # decoder\n        self.decoder3 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec3\", dropout=0.4\n        )\n        self.decoder2 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec2\", dropout=0.3\n        )\n        self.decoder1 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec1\", dropout=0.2\n        )\n\n        # final layer\n        self.final_conv = nn.Conv2d(\n            num_filters, num_classes, kernel_size=1, padding=0, bias=False\n        )\n\n    @staticmethod\n    def conv_block(in_channels, out_channels, kernel_size, name, num, dropout=0):\n        layers = {\n            f\"{name}_conv{num}\": Conv2dSame(in_channels, out_channels, kernel_size),\n            f\"{name}_bn{num}\": nn.BatchNorm2d(out_channels),\n            f\"{name}_relu{num}\": nn.ReLU(inplace=True),\n        }\n        if dropout > 0:\n            layers[f\"{name}_dropout\"] = nn.Dropout(p=dropout)\n\n        return nn.Sequential(OrderedDict(layers))\n\n    @staticmethod\n    def _block(in_channels, out_channels, kernel_size, name, dropout=0):\n        conv1 = EddyNet.conv_block(in_channels, out_channels, kernel_size, name, 1)\n        conv2 = EddyNet.conv_block(\n            out_channels, out_channels, kernel_size, name, 2, dropout=dropout\n        )\n        return nn.Sequential(conv1, conv2)\n\n    @staticmethod\n    def decoder_block(in_channels, out_channels, kernel_size, name, dropout=0):\n        return EddyNet._block(in_channels, out_channels, kernel_size, name, dropout)\n\n    def forward(self, x):\n        # encoder\n        enc1 = self.encoder1(x)\n        pool1 = self.pool1(enc1)\n\n        enc2 = self.encoder2(pool1)\n        pool2 = self.pool2(enc2)\n\n        enc3 = self.encoder3(pool2)\n        pool3 = self.pool3(enc3)\n\n        # bottleneck?\n        enc4 = self.encoder4(pool3)\n\n        # decoder\n        dec3 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(enc4)\n        dec3 = torch.cat((dec3, enc3), dim=1)\n        dec3 = self.decoder3(dec3)\n\n        dec2 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec3)\n        dec2 = torch.cat((dec2, enc2), dim=1)\n        dec2 = self.decoder2(dec2)\n\n        dec1 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec2)\n        dec1 = torch.cat((dec1, enc1), dim=1)\n        dec1 = self.decoder1(dec1)\n\n        # final layer\n        final = self.final_conv(dec1)\n\n        # softmax\n        final = nn.Softmax(dim=1)(final)\n\n        return final\n\n\nclass Conv2dSame(nn.Module):\n    \"\"\"Manual convolution with same padding\n    https://discuss.pytorch.org/t/same-padding-equivalent-in-pytorch/85121/9\n    Although PyTorch >= 1.10.0 supports ``padding='same'`` as a keyword\n    argument, this does not export to CoreML as of coremltools 5.1.0,\n    so we need to implement the internal torch logic manually.\n    Currently the ``RuntimeError`` is\n    \"PyTorch convert function for op '_convolution_mode' not implemented\"\n    \"\"\"\n\n    def __init__(\n        self, in_channels, out_channels, kernel_size, stride=1, dilation=1, **kwargs\n    ):\n        \"\"\"Wrap base convolution layer\n        See official PyTorch documentation for parameter details\n        https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n        \"\"\"\n        super().__init__()\n        self.conv = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            dilation=dilation,\n            **kwargs,\n        )\n\n        # Setup internal representations\n        kernel_size_ = _pair(kernel_size)\n        dilation_ = _pair(dilation)\n        self._reversed_padding_repeated_twice = [0, 0] * len(kernel_size_)\n\n        # Follow the logic from ``nn/modules/conv.py:_ConvNd``\n        for d, k, i in zip(\n            dilation_, kernel_size_, range(len(kernel_size_) - 1, -1, -1)\n        ):\n            total_padding = d * (k - 1)\n            left_pad = total_padding // 2\n            self._reversed_padding_repeated_twice[2 * i] = left_pad\n            self._reversed_padding_repeated_twice[2 * i + 1] = total_padding - left_pad\n\n    def forward(self, imgs):\n        \"\"\"Setup padding so same spatial dimensions are returned\n        All shapes (input/output) are ``(N, C, W, H)`` convention\n        :param torch.Tensor imgs:\n        :return torch.Tensor:\n        \"\"\"\n        padded = F.pad(imgs, self._reversed_padding_repeated_twice)\n        return self.conv(padded)\n\n\ndef _ntuple(n):\n    \"\"\"Copy from PyTorch since internal function is not importable\n    See ``nn/modules/utils.py:6``\n    \"\"\"\n\n    def parse(x):\n        if isinstance(x, collections.abc.Iterable):\n            return tuple(x)\n        return tuple(repeat(x, n))\n\n    return parse\n\n\n_pair = _ntuple(2)\nFooter",
  "history_output" : "Traceback (most recent call last):\n  File \"eddynet.py\", line 173, in <module>\n    Footer\nNameError: name 'Footer' is not defined\n",
  "history_begin_time" : 1664327882647,
  "history_end_time" : 1664333649317,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "capemx8hsde",
  "history_input" : "#Eddynet\nimport collections\nfrom itertools import repeat\nfrom typing import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass EddyNet(nn.Module):\n    \"\"\"\n    PyTorch implementation of EddyNet from Lguensat et al. (2018)\n    Original implementation in TensorFlow: https://github.com/redouanelg/EddyNet\n    \"\"\"\n    def __init__(self, num_classes, num_filters, kernel_size):\n        super(EddyNet, self).__init__()\n        # encoder\n        self.encoder1 = EddyNet._block(1, num_filters, kernel_size, \"enc1\", dropout=0.2)\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder2 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc2\", dropout=0.3\n        )\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder3 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc3\", dropout=0.4\n        )\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder4 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc4\", dropout=0.5\n        )\n\n        # decoder\n        self.decoder3 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec3\", dropout=0.4\n        )\n        self.decoder2 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec2\", dropout=0.3\n        )\n        self.decoder1 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec1\", dropout=0.2\n        )\n\n        # final layer\n        self.final_conv = nn.Conv2d(\n            num_filters, num_classes, kernel_size=1, padding=0, bias=False\n        )\n\n    @staticmethod\n    def conv_block(in_channels, out_channels, kernel_size, name, num, dropout=0):\n        layers = {\n            f\"{name}_conv{num}\": Conv2dSame(in_channels, out_channels, kernel_size),\n            f\"{name}_bn{num}\": nn.BatchNorm2d(out_channels),\n            f\"{name}_relu{num}\": nn.ReLU(inplace=True),\n        }\n        if dropout > 0:\n            layers[f\"{name}_dropout\"] = nn.Dropout(p=dropout)\n\n        return nn.Sequential(OrderedDict(layers))\n\n    @staticmethod\n    def _block(in_channels, out_channels, kernel_size, name, dropout=0):\n        conv1 = EddyNet.conv_block(in_channels, out_channels, kernel_size, name, 1)\n        conv2 = EddyNet.conv_block(\n            out_channels, out_channels, kernel_size, name, 2, dropout=dropout\n        )\n        return nn.Sequential(conv1, conv2)\n\n    @staticmethod\n    def decoder_block(in_channels, out_channels, kernel_size, name, dropout=0):\n        return EddyNet._block(in_channels, out_channels, kernel_size, name, dropout)\n\n    def forward(self, x):\n        # encoder\n        enc1 = self.encoder1(x)\n        pool1 = self.pool1(enc1)\n\n        enc2 = self.encoder2(pool1)\n        pool2 = self.pool2(enc2)\n\n        enc3 = self.encoder3(pool2)\n        pool3 = self.pool3(enc3)\n\n        # bottleneck?\n        enc4 = self.encoder4(pool3)\n\n        # decoder\n        dec3 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(enc4)\n        dec3 = torch.cat((dec3, enc3), dim=1)\n        dec3 = self.decoder3(dec3)\n\n        dec2 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec3)\n        dec2 = torch.cat((dec2, enc2), dim=1)\n        dec2 = self.decoder2(dec2)\n\n        dec1 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec2)\n        dec1 = torch.cat((dec1, enc1), dim=1)\n        dec1 = self.decoder1(dec1)\n\n        # final layer\n        final = self.final_conv(dec1)\n\n        # softmax\n        final = nn.Softmax(dim=1)(final)\n\n        return final\n\n\nclass Conv2dSame(nn.Module):\n    \"\"\"Manual convolution with same padding\n    https://discuss.pytorch.org/t/same-padding-equivalent-in-pytorch/85121/9\n    Although PyTorch >= 1.10.0 supports ``padding='same'`` as a keyword\n    argument, this does not export to CoreML as of coremltools 5.1.0,\n    so we need to implement the internal torch logic manually.\n    Currently the ``RuntimeError`` is\n    \"PyTorch convert function for op '_convolution_mode' not implemented\"\n    \"\"\"\n\n    def __init__(\n        self, in_channels, out_channels, kernel_size, stride=1, dilation=1, **kwargs\n    ):\n        \"\"\"Wrap base convolution layer\n        See official PyTorch documentation for parameter details\n        https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n        \"\"\"\n        super().__init__()\n        self.conv = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            dilation=dilation,\n            **kwargs,\n        )\n\n        # Setup internal representations\n        kernel_size_ = _pair(kernel_size)\n        dilation_ = _pair(dilation)\n        self._reversed_padding_repeated_twice = [0, 0] * len(kernel_size_)\n\n        # Follow the logic from ``nn/modules/conv.py:_ConvNd``\n        for d, k, i in zip(\n            dilation_, kernel_size_, range(len(kernel_size_) - 1, -1, -1)\n        ):\n            total_padding = d * (k - 1)\n            left_pad = total_padding // 2\n            self._reversed_padding_repeated_twice[2 * i] = left_pad\n            self._reversed_padding_repeated_twice[2 * i + 1] = total_padding - left_pad\n\n    def forward(self, imgs):\n        \"\"\"Setup padding so same spatial dimensions are returned\n        All shapes (input/output) are ``(N, C, W, H)`` convention\n        :param torch.Tensor imgs:\n        :return torch.Tensor:\n        \"\"\"\n        padded = F.pad(imgs, self._reversed_padding_repeated_twice)\n        return self.conv(padded)\n\n\ndef _ntuple(n):\n    \"\"\"Copy from PyTorch since internal function is not importable\n    See ``nn/modules/utils.py:6``\n    \"\"\"\n\n    def parse(x):\n        if isinstance(x, collections.abc.Iterable):\n            return tuple(x)\n        return tuple(repeat(x, n))\n\n    return parse\n\n\n_pair = _ntuple(2)\nFooter",
  "history_output" : "Traceback (most recent call last):\n  File \"eddynet.py\", line 173, in <module>\n    Footer\nNameError: name 'Footer' is not defined\n",
  "history_begin_time" : 1664327413033,
  "history_end_time" : 1664327829448,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "cu18iab04g2",
  "history_input" : "#Eddynet\nimport collections\nfrom itertools import repeat\nfrom typing import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass EddyNet(nn.Module):\n    \"\"\"\n    PyTorch implementation of EddyNet from Lguensat et al. (2018)\n    Original implementation in TensorFlow: https://github.com/redouanelg/EddyNet\n    \"\"\"\n    def __init__(self, num_classes, num_filters, kernel_size):\n        super(EddyNet, self).__init__()\n        # encoder\n        self.encoder1 = EddyNet._block(1, num_filters, kernel_size, \"enc1\", dropout=0.2)\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder2 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc2\", dropout=0.3\n        )\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder3 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc3\", dropout=0.4\n        )\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder4 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc4\", dropout=0.5\n        )\n\n        # decoder\n        self.decoder3 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec3\", dropout=0.4\n        )\n        self.decoder2 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec2\", dropout=0.3\n        )\n        self.decoder1 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec1\", dropout=0.2\n        )\n\n        # final layer\n        self.final_conv = nn.Conv2d(\n            num_filters, num_classes, kernel_size=1, padding=0, bias=False\n        )\n\n    @staticmethod\n    def conv_block(in_channels, out_channels, kernel_size, name, num, dropout=0):\n        layers = {\n            f\"{name}_conv{num}\": Conv2dSame(in_channels, out_channels, kernel_size),\n            f\"{name}_bn{num}\": nn.BatchNorm2d(out_channels),\n            f\"{name}_relu{num}\": nn.ReLU(inplace=True),\n        }\n        if dropout > 0:\n            layers[f\"{name}_dropout\"] = nn.Dropout(p=dropout)\n\n        return nn.Sequential(OrderedDict(layers))\n\n    @staticmethod\n    def _block(in_channels, out_channels, kernel_size, name, dropout=0):\n        conv1 = EddyNet.conv_block(in_channels, out_channels, kernel_size, name, 1)\n        conv2 = EddyNet.conv_block(\n            out_channels, out_channels, kernel_size, name, 2, dropout=dropout\n        )\n        return nn.Sequential(conv1, conv2)\n\n    @staticmethod\n    def decoder_block(in_channels, out_channels, kernel_size, name, dropout=0):\n        return EddyNet._block(in_channels, out_channels, kernel_size, name, dropout)\n\n    def forward(self, x):\n        # encoder\n        enc1 = self.encoder1(x)\n        pool1 = self.pool1(enc1)\n\n        enc2 = self.encoder2(pool1)\n        pool2 = self.pool2(enc2)\n\n        enc3 = self.encoder3(pool2)\n        pool3 = self.pool3(enc3)\n\n        # bottleneck?\n        enc4 = self.encoder4(pool3)\n\n        # decoder\n        dec3 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(enc4)\n        dec3 = torch.cat((dec3, enc3), dim=1)\n        dec3 = self.decoder3(dec3)\n\n        dec2 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec3)\n        dec2 = torch.cat((dec2, enc2), dim=1)\n        dec2 = self.decoder2(dec2)\n\n        dec1 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec2)\n        dec1 = torch.cat((dec1, enc1), dim=1)\n        dec1 = self.decoder1(dec1)\n\n        # final layer\n        final = self.final_conv(dec1)\n\n        # softmax\n        final = nn.Softmax(dim=1)(final)\n\n        return final\n\n\nclass Conv2dSame(nn.Module):\n    \"\"\"Manual convolution with same padding\n    https://discuss.pytorch.org/t/same-padding-equivalent-in-pytorch/85121/9\n    Although PyTorch >= 1.10.0 supports ``padding='same'`` as a keyword\n    argument, this does not export to CoreML as of coremltools 5.1.0,\n    so we need to implement the internal torch logic manually.\n    Currently the ``RuntimeError`` is\n    \"PyTorch convert function for op '_convolution_mode' not implemented\"\n    \"\"\"\n\n    def __init__(\n        self, in_channels, out_channels, kernel_size, stride=1, dilation=1, **kwargs\n    ):\n        \"\"\"Wrap base convolution layer\n        See official PyTorch documentation for parameter details\n        https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n        \"\"\"\n        super().__init__()\n        self.conv = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            dilation=dilation,\n            **kwargs,\n        )\n\n        # Setup internal representations\n        kernel_size_ = _pair(kernel_size)\n        dilation_ = _pair(dilation)\n        self._reversed_padding_repeated_twice = [0, 0] * len(kernel_size_)\n\n        # Follow the logic from ``nn/modules/conv.py:_ConvNd``\n        for d, k, i in zip(\n            dilation_, kernel_size_, range(len(kernel_size_) - 1, -1, -1)\n        ):\n            total_padding = d * (k - 1)\n            left_pad = total_padding // 2\n            self._reversed_padding_repeated_twice[2 * i] = left_pad\n            self._reversed_padding_repeated_twice[2 * i + 1] = total_padding - left_pad\n\n    def forward(self, imgs):\n        \"\"\"Setup padding so same spatial dimensions are returned\n        All shapes (input/output) are ``(N, C, W, H)`` convention\n        :param torch.Tensor imgs:\n        :return torch.Tensor:\n        \"\"\"\n        padded = F.pad(imgs, self._reversed_padding_repeated_twice)\n        return self.conv(padded)\n\n\ndef _ntuple(n):\n    \"\"\"Copy from PyTorch since internal function is not importable\n    See ``nn/modules/utils.py:6``\n    \"\"\"\n\n    def parse(x):\n        if isinstance(x, collections.abc.Iterable):\n            return tuple(x)\n        return tuple(repeat(x, n))\n\n    return parse\n\n\n_pair = _ntuple(2)\nFooter",
  "history_output" : "Traceback (most recent call last):\n  File \"eddynet.py\", line 173, in <module>\n    Footer\nNameError: name 'Footer' is not defined\n",
  "history_begin_time" : 1664326957664,
  "history_end_time" : 1664326961834,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "9an750ajrk5",
  "history_input" : "#Eddynet\nimport collections\nfrom itertools import repeat\nfrom typing import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass EddyNet(nn.Module):\n    \"\"\"\n    PyTorch implementation of EddyNet from Lguensat et al. (2018)\n    Original implementation in TensorFlow: https://github.com/redouanelg/EddyNet\n    \"\"\"\n    def __init__(self, num_classes, num_filters, kernel_size):\n        super(EddyNet, self).__init__()\n        # encoder\n        self.encoder1 = EddyNet._block(1, num_filters, kernel_size, \"enc1\", dropout=0.2)\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder2 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc2\", dropout=0.3\n        )\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder3 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc3\", dropout=0.4\n        )\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder4 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc4\", dropout=0.5\n        )\n\n        # decoder\n        self.decoder3 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec3\", dropout=0.4\n        )\n        self.decoder2 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec2\", dropout=0.3\n        )\n        self.decoder1 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec1\", dropout=0.2\n        )\n\n        # final layer\n        self.final_conv = nn.Conv2d(\n            num_filters, num_classes, kernel_size=1, padding=0, bias=False\n        )\n\n    @staticmethod\n    def conv_block(in_channels, out_channels, kernel_size, name, num, dropout=0):\n        layers = {\n            f\"{name}_conv{num}\": Conv2dSame(in_channels, out_channels, kernel_size),\n            f\"{name}_bn{num}\": nn.BatchNorm2d(out_channels),\n            f\"{name}_relu{num}\": nn.ReLU(inplace=True),\n        }\n        if dropout > 0:\n            layers[f\"{name}_dropout\"] = nn.Dropout(p=dropout)\n\n        return nn.Sequential(OrderedDict(layers))\n\n    @staticmethod\n    def _block(in_channels, out_channels, kernel_size, name, dropout=0):\n        conv1 = EddyNet.conv_block(in_channels, out_channels, kernel_size, name, 1)\n        conv2 = EddyNet.conv_block(\n            out_channels, out_channels, kernel_size, name, 2, dropout=dropout\n        )\n        return nn.Sequential(conv1, conv2)\n\n    @staticmethod\n    def decoder_block(in_channels, out_channels, kernel_size, name, dropout=0):\n        return EddyNet._block(in_channels, out_channels, kernel_size, name, dropout)\n\n    def forward(self, x):\n        # encoder\n        enc1 = self.encoder1(x)\n        pool1 = self.pool1(enc1)\n\n        enc2 = self.encoder2(pool1)\n        pool2 = self.pool2(enc2)\n\n        enc3 = self.encoder3(pool2)\n        pool3 = self.pool3(enc3)\n\n        # bottleneck?\n        enc4 = self.encoder4(pool3)\n\n        # decoder\n        dec3 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(enc4)\n        dec3 = torch.cat((dec3, enc3), dim=1)\n        dec3 = self.decoder3(dec3)\n\n        dec2 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec3)\n        dec2 = torch.cat((dec2, enc2), dim=1)\n        dec2 = self.decoder2(dec2)\n\n        dec1 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec2)\n        dec1 = torch.cat((dec1, enc1), dim=1)\n        dec1 = self.decoder1(dec1)\n\n        # final layer\n        final = self.final_conv(dec1)\n\n        # softmax\n        final = nn.Softmax(dim=1)(final)\n\n        return final\n\n\nclass Conv2dSame(nn.Module):\n    \"\"\"Manual convolution with same padding\n    https://discuss.pytorch.org/t/same-padding-equivalent-in-pytorch/85121/9\n    Although PyTorch >= 1.10.0 supports ``padding='same'`` as a keyword\n    argument, this does not export to CoreML as of coremltools 5.1.0,\n    so we need to implement the internal torch logic manually.\n    Currently the ``RuntimeError`` is\n    \"PyTorch convert function for op '_convolution_mode' not implemented\"\n    \"\"\"\n\n    def __init__(\n        self, in_channels, out_channels, kernel_size, stride=1, dilation=1, **kwargs\n    ):\n        \"\"\"Wrap base convolution layer\n        See official PyTorch documentation for parameter details\n        https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n        \"\"\"\n        super().__init__()\n        self.conv = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            dilation=dilation,\n            **kwargs,\n        )\n\n        # Setup internal representations\n        kernel_size_ = _pair(kernel_size)\n        dilation_ = _pair(dilation)\n        self._reversed_padding_repeated_twice = [0, 0] * len(kernel_size_)\n\n        # Follow the logic from ``nn/modules/conv.py:_ConvNd``\n        for d, k, i in zip(\n            dilation_, kernel_size_, range(len(kernel_size_) - 1, -1, -1)\n        ):\n            total_padding = d * (k - 1)\n            left_pad = total_padding // 2\n            self._reversed_padding_repeated_twice[2 * i] = left_pad\n            self._reversed_padding_repeated_twice[2 * i + 1] = total_padding - left_pad\n\n    def forward(self, imgs):\n        \"\"\"Setup padding so same spatial dimensions are returned\n        All shapes (input/output) are ``(N, C, W, H)`` convention\n        :param torch.Tensor imgs:\n        :return torch.Tensor:\n        \"\"\"\n        padded = F.pad(imgs, self._reversed_padding_repeated_twice)\n        return self.conv(padded)\n\n\ndef _ntuple(n):\n    \"\"\"Copy from PyTorch since internal function is not importable\n    See ``nn/modules/utils.py:6``\n    \"\"\"\n\n    def parse(x):\n        if isinstance(x, collections.abc.Iterable):\n            return tuple(x)\n        return tuple(repeat(x, n))\n\n    return parse\n\n\n_pair = _ntuple(2)\nFooter",
  "history_output" : "Traceback (most recent call last):\n  File \"eddynet.py\", line 173, in <module>\n    Footer\nNameError: name 'Footer' is not defined\n",
  "history_begin_time" : 1664326783152,
  "history_end_time" : 1664326787710,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "2i9c0cum7zh",
  "history_input" : "#Eddynet\nimport collections\nfrom itertools import repeat\nfrom typing import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass EddyNet(nn.Module):\n    \"\"\"\n    PyTorch implementation of EddyNet from Lguensat et al. (2018)\n    Original implementation in TensorFlow: https://github.com/redouanelg/EddyNet\n    \"\"\"\n    def __init__(self, num_classes, num_filters, kernel_size):\n        super(EddyNet, self).__init__()\n        # encoder\n        self.encoder1 = EddyNet._block(1, num_filters, kernel_size, \"enc1\", dropout=0.2)\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder2 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc2\", dropout=0.3\n        )\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder3 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc3\", dropout=0.4\n        )\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder4 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc4\", dropout=0.5\n        )\n\n        # decoder\n        self.decoder3 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec3\", dropout=0.4\n        )\n        self.decoder2 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec2\", dropout=0.3\n        )\n        self.decoder1 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec1\", dropout=0.2\n        )\n\n        # final layer\n        self.final_conv = nn.Conv2d(\n            num_filters, num_classes, kernel_size=1, padding=0, bias=False\n        )\n\n    @staticmethod\n    def conv_block(in_channels, out_channels, kernel_size, name, num, dropout=0):\n        layers = {\n            f\"{name}_conv{num}\": Conv2dSame(in_channels, out_channels, kernel_size),\n            f\"{name}_bn{num}\": nn.BatchNorm2d(out_channels),\n            f\"{name}_relu{num}\": nn.ReLU(inplace=True),\n        }\n        if dropout > 0:\n            layers[f\"{name}_dropout\"] = nn.Dropout(p=dropout)\n\n        return nn.Sequential(OrderedDict(layers))\n\n    @staticmethod\n    def _block(in_channels, out_channels, kernel_size, name, dropout=0):\n        conv1 = EddyNet.conv_block(in_channels, out_channels, kernel_size, name, 1)\n        conv2 = EddyNet.conv_block(\n            out_channels, out_channels, kernel_size, name, 2, dropout=dropout\n        )\n        return nn.Sequential(conv1, conv2)\n\n    @staticmethod\n    def decoder_block(in_channels, out_channels, kernel_size, name, dropout=0):\n        return EddyNet._block(in_channels, out_channels, kernel_size, name, dropout)\n\n    def forward(self, x):\n        # encoder\n        enc1 = self.encoder1(x)\n        pool1 = self.pool1(enc1)\n\n        enc2 = self.encoder2(pool1)\n        pool2 = self.pool2(enc2)\n\n        enc3 = self.encoder3(pool2)\n        pool3 = self.pool3(enc3)\n\n        # bottleneck?\n        enc4 = self.encoder4(pool3)\n\n        # decoder\n        dec3 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(enc4)\n        dec3 = torch.cat((dec3, enc3), dim=1)\n        dec3 = self.decoder3(dec3)\n\n        dec2 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec3)\n        dec2 = torch.cat((dec2, enc2), dim=1)\n        dec2 = self.decoder2(dec2)\n\n        dec1 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec2)\n        dec1 = torch.cat((dec1, enc1), dim=1)\n        dec1 = self.decoder1(dec1)\n\n        # final layer\n        final = self.final_conv(dec1)\n\n        # softmax\n        final = nn.Softmax(dim=1)(final)\n\n        return final\n\n\nclass Conv2dSame(nn.Module):\n    \"\"\"Manual convolution with same padding\n    https://discuss.pytorch.org/t/same-padding-equivalent-in-pytorch/85121/9\n    Although PyTorch >= 1.10.0 supports ``padding='same'`` as a keyword\n    argument, this does not export to CoreML as of coremltools 5.1.0,\n    so we need to implement the internal torch logic manually.\n    Currently the ``RuntimeError`` is\n    \"PyTorch convert function for op '_convolution_mode' not implemented\"\n    \"\"\"\n\n    def __init__(\n        self, in_channels, out_channels, kernel_size, stride=1, dilation=1, **kwargs\n    ):\n        \"\"\"Wrap base convolution layer\n        See official PyTorch documentation for parameter details\n        https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n        \"\"\"\n        super().__init__()\n        self.conv = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            dilation=dilation,\n            **kwargs,\n        )\n\n        # Setup internal representations\n        kernel_size_ = _pair(kernel_size)\n        dilation_ = _pair(dilation)\n        self._reversed_padding_repeated_twice = [0, 0] * len(kernel_size_)\n\n        # Follow the logic from ``nn/modules/conv.py:_ConvNd``\n        for d, k, i in zip(\n            dilation_, kernel_size_, range(len(kernel_size_) - 1, -1, -1)\n        ):\n            total_padding = d * (k - 1)\n            left_pad = total_padding // 2\n            self._reversed_padding_repeated_twice[2 * i] = left_pad\n            self._reversed_padding_repeated_twice[2 * i + 1] = total_padding - left_pad\n\n    def forward(self, imgs):\n        \"\"\"Setup padding so same spatial dimensions are returned\n        All shapes (input/output) are ``(N, C, W, H)`` convention\n        :param torch.Tensor imgs:\n        :return torch.Tensor:\n        \"\"\"\n        padded = F.pad(imgs, self._reversed_padding_repeated_twice)\n        return self.conv(padded)\n\n\ndef _ntuple(n):\n    \"\"\"Copy from PyTorch since internal function is not importable\n    See ``nn/modules/utils.py:6``\n    \"\"\"\n\n    def parse(x):\n        if isinstance(x, collections.abc.Iterable):\n            return tuple(x)\n        return tuple(repeat(x, n))\n\n    return parse\n\n\n_pair = _ntuple(2)\nFooter",
  "history_output" : "Traceback (most recent call last):\n  File \"eddynet.py\", line 173, in <module>\n    Footer\nNameError: name 'Footer' is not defined\n",
  "history_begin_time" : 1664326234928,
  "history_end_time" : 1664326242784,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "ljnk91iilnp",
  "history_input" : "#Eddynet\nimport collections\nfrom itertools import repeat\nfrom typing import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass EddyNet(nn.Module):\n    \"\"\"\n    PyTorch implementation of EddyNet from Lguensat et al. (2018)\n    Original implementation in TensorFlow: https://github.com/redouanelg/EddyNet\n    \"\"\"\n    def __init__(self, num_classes, num_filters, kernel_size):\n        super(EddyNet, self).__init__()\n        # encoder\n        self.encoder1 = EddyNet._block(1, num_filters, kernel_size, \"enc1\", dropout=0.2)\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder2 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc2\", dropout=0.3\n        )\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder3 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc3\", dropout=0.4\n        )\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder4 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc4\", dropout=0.5\n        )\n\n        # decoder\n        self.decoder3 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec3\", dropout=0.4\n        )\n        self.decoder2 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec2\", dropout=0.3\n        )\n        self.decoder1 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec1\", dropout=0.2\n        )\n\n        # final layer\n        self.final_conv = nn.Conv2d(\n            num_filters, num_classes, kernel_size=1, padding=0, bias=False\n        )\n\n    @staticmethod\n    def conv_block(in_channels, out_channels, kernel_size, name, num, dropout=0):\n        layers = {\n            f\"{name}_conv{num}\": Conv2dSame(in_channels, out_channels, kernel_size),\n            f\"{name}_bn{num}\": nn.BatchNorm2d(out_channels),\n            f\"{name}_relu{num}\": nn.ReLU(inplace=True),\n        }\n        if dropout > 0:\n            layers[f\"{name}_dropout\"] = nn.Dropout(p=dropout)\n\n        return nn.Sequential(OrderedDict(layers))\n\n    @staticmethod\n    def _block(in_channels, out_channels, kernel_size, name, dropout=0):\n        conv1 = EddyNet.conv_block(in_channels, out_channels, kernel_size, name, 1)\n        conv2 = EddyNet.conv_block(\n            out_channels, out_channels, kernel_size, name, 2, dropout=dropout\n        )\n        return nn.Sequential(conv1, conv2)\n\n    @staticmethod\n    def decoder_block(in_channels, out_channels, kernel_size, name, dropout=0):\n        return EddyNet._block(in_channels, out_channels, kernel_size, name, dropout)\n\n    def forward(self, x):\n        # encoder\n        enc1 = self.encoder1(x)\n        pool1 = self.pool1(enc1)\n\n        enc2 = self.encoder2(pool1)\n        pool2 = self.pool2(enc2)\n\n        enc3 = self.encoder3(pool2)\n        pool3 = self.pool3(enc3)\n\n        # bottleneck?\n        enc4 = self.encoder4(pool3)\n\n        # decoder\n        dec3 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(enc4)\n        dec3 = torch.cat((dec3, enc3), dim=1)\n        dec3 = self.decoder3(dec3)\n\n        dec2 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec3)\n        dec2 = torch.cat((dec2, enc2), dim=1)\n        dec2 = self.decoder2(dec2)\n\n        dec1 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec2)\n        dec1 = torch.cat((dec1, enc1), dim=1)\n        dec1 = self.decoder1(dec1)\n\n        # final layer\n        final = self.final_conv(dec1)\n\n        # softmax\n        final = nn.Softmax(dim=1)(final)\n\n        return final\n\n\nclass Conv2dSame(nn.Module):\n    \"\"\"Manual convolution with same padding\n    https://discuss.pytorch.org/t/same-padding-equivalent-in-pytorch/85121/9\n    Although PyTorch >= 1.10.0 supports ``padding='same'`` as a keyword\n    argument, this does not export to CoreML as of coremltools 5.1.0,\n    so we need to implement the internal torch logic manually.\n    Currently the ``RuntimeError`` is\n    \"PyTorch convert function for op '_convolution_mode' not implemented\"\n    \"\"\"\n\n    def __init__(\n        self, in_channels, out_channels, kernel_size, stride=1, dilation=1, **kwargs\n    ):\n        \"\"\"Wrap base convolution layer\n        See official PyTorch documentation for parameter details\n        https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n        \"\"\"\n        super().__init__()\n        self.conv = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            dilation=dilation,\n            **kwargs,\n        )\n\n        # Setup internal representations\n        kernel_size_ = _pair(kernel_size)\n        dilation_ = _pair(dilation)\n        self._reversed_padding_repeated_twice = [0, 0] * len(kernel_size_)\n\n        # Follow the logic from ``nn/modules/conv.py:_ConvNd``\n        for d, k, i in zip(\n            dilation_, kernel_size_, range(len(kernel_size_) - 1, -1, -1)\n        ):\n            total_padding = d * (k - 1)\n            left_pad = total_padding // 2\n            self._reversed_padding_repeated_twice[2 * i] = left_pad\n            self._reversed_padding_repeated_twice[2 * i + 1] = total_padding - left_pad\n\n    def forward(self, imgs):\n        \"\"\"Setup padding so same spatial dimensions are returned\n        All shapes (input/output) are ``(N, C, W, H)`` convention\n        :param torch.Tensor imgs:\n        :return torch.Tensor:\n        \"\"\"\n        padded = F.pad(imgs, self._reversed_padding_repeated_twice)\n        return self.conv(padded)\n\n\ndef _ntuple(n):\n    \"\"\"Copy from PyTorch since internal function is not importable\n    See ``nn/modules/utils.py:6``\n    \"\"\"\n\n    def parse(x):\n        if isinstance(x, collections.abc.Iterable):\n            return tuple(x)\n        return tuple(repeat(x, n))\n\n    return parse\n\n\n_pair = _ntuple(2)\nFooter",
  "history_output" : "Traceback (most recent call last):\n  File \"eddynet.py\", line 173, in <module>\n    Footer\nNameError: name 'Footer' is not defined\n",
  "history_begin_time" : 1664325219305,
  "history_end_time" : 1664325227245,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "k0lrayl3h90",
  "history_input" : "#Eddynet\nimport collections\nfrom itertools import repeat\nfrom typing import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass EddyNet(nn.Module):\n    \"\"\"\n    PyTorch implementation of EddyNet from Lguensat et al. (2018)\n    Original implementation in TensorFlow: https://github.com/redouanelg/EddyNet\n    \"\"\"\n    def __init__(self, num_classes, num_filters, kernel_size):\n        super(EddyNet, self).__init__()\n        # encoder\n        self.encoder1 = EddyNet._block(1, num_filters, kernel_size, \"enc1\", dropout=0.2)\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder2 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc2\", dropout=0.3\n        )\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder3 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc3\", dropout=0.4\n        )\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder4 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc4\", dropout=0.5\n        )\n\n        # decoder\n        self.decoder3 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec3\", dropout=0.4\n        )\n        self.decoder2 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec2\", dropout=0.3\n        )\n        self.decoder1 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec1\", dropout=0.2\n        )\n\n        # final layer\n        self.final_conv = nn.Conv2d(\n            num_filters, num_classes, kernel_size=1, padding=0, bias=False\n        )\n\n    @staticmethod\n    def conv_block(in_channels, out_channels, kernel_size, name, num, dropout=0):\n        layers = {\n            f\"{name}_conv{num}\": Conv2dSame(in_channels, out_channels, kernel_size),\n            f\"{name}_bn{num}\": nn.BatchNorm2d(out_channels),\n            f\"{name}_relu{num}\": nn.ReLU(inplace=True),\n        }\n        if dropout > 0:\n            layers[f\"{name}_dropout\"] = nn.Dropout(p=dropout)\n\n        return nn.Sequential(OrderedDict(layers))\n\n    @staticmethod\n    def _block(in_channels, out_channels, kernel_size, name, dropout=0):\n        conv1 = EddyNet.conv_block(in_channels, out_channels, kernel_size, name, 1)\n        conv2 = EddyNet.conv_block(\n            out_channels, out_channels, kernel_size, name, 2, dropout=dropout\n        )\n        return nn.Sequential(conv1, conv2)\n\n    @staticmethod\n    def decoder_block(in_channels, out_channels, kernel_size, name, dropout=0):\n        return EddyNet._block(in_channels, out_channels, kernel_size, name, dropout)\n\n    def forward(self, x):\n        # encoder\n        enc1 = self.encoder1(x)\n        pool1 = self.pool1(enc1)\n\n        enc2 = self.encoder2(pool1)\n        pool2 = self.pool2(enc2)\n\n        enc3 = self.encoder3(pool2)\n        pool3 = self.pool3(enc3)\n\n        # bottleneck?\n        enc4 = self.encoder4(pool3)\n\n        # decoder\n        dec3 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(enc4)\n        dec3 = torch.cat((dec3, enc3), dim=1)\n        dec3 = self.decoder3(dec3)\n\n        dec2 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec3)\n        dec2 = torch.cat((dec2, enc2), dim=1)\n        dec2 = self.decoder2(dec2)\n\n        dec1 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec2)\n        dec1 = torch.cat((dec1, enc1), dim=1)\n        dec1 = self.decoder1(dec1)\n\n        # final layer\n        final = self.final_conv(dec1)\n\n        # softmax\n        final = nn.Softmax(dim=1)(final)\n\n        return final\n\n\nclass Conv2dSame(nn.Module):\n    \"\"\"Manual convolution with same padding\n    https://discuss.pytorch.org/t/same-padding-equivalent-in-pytorch/85121/9\n    Although PyTorch >= 1.10.0 supports ``padding='same'`` as a keyword\n    argument, this does not export to CoreML as of coremltools 5.1.0,\n    so we need to implement the internal torch logic manually.\n    Currently the ``RuntimeError`` is\n    \"PyTorch convert function for op '_convolution_mode' not implemented\"\n    \"\"\"\n\n    def __init__(\n        self, in_channels, out_channels, kernel_size, stride=1, dilation=1, **kwargs\n    ):\n        \"\"\"Wrap base convolution layer\n        See official PyTorch documentation for parameter details\n        https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n        \"\"\"\n        super().__init__()\n        self.conv = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            dilation=dilation,\n            **kwargs,\n        )\n\n        # Setup internal representations\n        kernel_size_ = _pair(kernel_size)\n        dilation_ = _pair(dilation)\n        self._reversed_padding_repeated_twice = [0, 0] * len(kernel_size_)\n\n        # Follow the logic from ``nn/modules/conv.py:_ConvNd``\n        for d, k, i in zip(\n            dilation_, kernel_size_, range(len(kernel_size_) - 1, -1, -1)\n        ):\n            total_padding = d * (k - 1)\n            left_pad = total_padding // 2\n            self._reversed_padding_repeated_twice[2 * i] = left_pad\n            self._reversed_padding_repeated_twice[2 * i + 1] = total_padding - left_pad\n\n    def forward(self, imgs):\n        \"\"\"Setup padding so same spatial dimensions are returned\n        All shapes (input/output) are ``(N, C, W, H)`` convention\n        :param torch.Tensor imgs:\n        :return torch.Tensor:\n        \"\"\"\n        padded = F.pad(imgs, self._reversed_padding_repeated_twice)\n        return self.conv(padded)\n\n\ndef _ntuple(n):\n    \"\"\"Copy from PyTorch since internal function is not importable\n    See ``nn/modules/utils.py:6``\n    \"\"\"\n\n    def parse(x):\n        if isinstance(x, collections.abc.Iterable):\n            return tuple(x)\n        return tuple(repeat(x, n))\n\n    return parse\n\n\n_pair = _ntuple(2)\nFooter",
  "history_output" : "Traceback (most recent call last):\n  File \"eddynet.py\", line 173, in <module>\n    Footer\nNameError: name 'Footer' is not defined\n",
  "history_begin_time" : 1664324673724,
  "history_end_time" : 1664324682205,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "ky89zi6gjsw",
  "history_input" : "#Eddynet\nimport collections\nfrom itertools import repeat\nfrom typing import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass EddyNet(nn.Module):\n    \"\"\"\n    PyTorch implementation of EddyNet from Lguensat et al. (2018)\n    Original implementation in TensorFlow: https://github.com/redouanelg/EddyNet\n    \"\"\"\n    def __init__(self, num_classes, num_filters, kernel_size):\n        super(EddyNet, self).__init__()\n        # encoder\n        self.encoder1 = EddyNet._block(1, num_filters, kernel_size, \"enc1\", dropout=0.2)\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder2 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc2\", dropout=0.3\n        )\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder3 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc3\", dropout=0.4\n        )\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder4 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc4\", dropout=0.5\n        )\n\n        # decoder\n        self.decoder3 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec3\", dropout=0.4\n        )\n        self.decoder2 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec2\", dropout=0.3\n        )\n        self.decoder1 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec1\", dropout=0.2\n        )\n\n        # final layer\n        self.final_conv = nn.Conv2d(\n            num_filters, num_classes, kernel_size=1, padding=0, bias=False\n        )\n\n    @staticmethod\n    def conv_block(in_channels, out_channels, kernel_size, name, num, dropout=0):\n        layers = {\n            f\"{name}_conv{num}\": Conv2dSame(in_channels, out_channels, kernel_size),\n            f\"{name}_bn{num}\": nn.BatchNorm2d(out_channels),\n            f\"{name}_relu{num}\": nn.ReLU(inplace=True),\n        }\n        if dropout > 0:\n            layers[f\"{name}_dropout\"] = nn.Dropout(p=dropout)\n\n        return nn.Sequential(OrderedDict(layers))\n\n    @staticmethod\n    def _block(in_channels, out_channels, kernel_size, name, dropout=0):\n        conv1 = EddyNet.conv_block(in_channels, out_channels, kernel_size, name, 1)\n        conv2 = EddyNet.conv_block(\n            out_channels, out_channels, kernel_size, name, 2, dropout=dropout\n        )\n        return nn.Sequential(conv1, conv2)\n\n    @staticmethod\n    def decoder_block(in_channels, out_channels, kernel_size, name, dropout=0):\n        return EddyNet._block(in_channels, out_channels, kernel_size, name, dropout)\n\n    def forward(self, x):\n        # encoder\n        enc1 = self.encoder1(x)\n        pool1 = self.pool1(enc1)\n\n        enc2 = self.encoder2(pool1)\n        pool2 = self.pool2(enc2)\n\n        enc3 = self.encoder3(pool2)\n        pool3 = self.pool3(enc3)\n\n        # bottleneck?\n        enc4 = self.encoder4(pool3)\n\n        # decoder\n        dec3 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(enc4)\n        dec3 = torch.cat((dec3, enc3), dim=1)\n        dec3 = self.decoder3(dec3)\n\n        dec2 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec3)\n        dec2 = torch.cat((dec2, enc2), dim=1)\n        dec2 = self.decoder2(dec2)\n\n        dec1 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec2)\n        dec1 = torch.cat((dec1, enc1), dim=1)\n        dec1 = self.decoder1(dec1)\n\n        # final layer\n        final = self.final_conv(dec1)\n\n        # softmax\n        final = nn.Softmax(dim=1)(final)\n\n        return final\n\n\nclass Conv2dSame(nn.Module):\n    \"\"\"Manual convolution with same padding\n    https://discuss.pytorch.org/t/same-padding-equivalent-in-pytorch/85121/9\n    Although PyTorch >= 1.10.0 supports ``padding='same'`` as a keyword\n    argument, this does not export to CoreML as of coremltools 5.1.0,\n    so we need to implement the internal torch logic manually.\n    Currently the ``RuntimeError`` is\n    \"PyTorch convert function for op '_convolution_mode' not implemented\"\n    \"\"\"\n\n    def __init__(\n        self, in_channels, out_channels, kernel_size, stride=1, dilation=1, **kwargs\n    ):\n        \"\"\"Wrap base convolution layer\n        See official PyTorch documentation for parameter details\n        https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n        \"\"\"\n        super().__init__()\n        self.conv = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            dilation=dilation,\n            **kwargs,\n        )\n\n        # Setup internal representations\n        kernel_size_ = _pair(kernel_size)\n        dilation_ = _pair(dilation)\n        self._reversed_padding_repeated_twice = [0, 0] * len(kernel_size_)\n\n        # Follow the logic from ``nn/modules/conv.py:_ConvNd``\n        for d, k, i in zip(\n            dilation_, kernel_size_, range(len(kernel_size_) - 1, -1, -1)\n        ):\n            total_padding = d * (k - 1)\n            left_pad = total_padding // 2\n            self._reversed_padding_repeated_twice[2 * i] = left_pad\n            self._reversed_padding_repeated_twice[2 * i + 1] = total_padding - left_pad\n\n    def forward(self, imgs):\n        \"\"\"Setup padding so same spatial dimensions are returned\n        All shapes (input/output) are ``(N, C, W, H)`` convention\n        :param torch.Tensor imgs:\n        :return torch.Tensor:\n        \"\"\"\n        padded = F.pad(imgs, self._reversed_padding_repeated_twice)\n        return self.conv(padded)\n\n\ndef _ntuple(n):\n    \"\"\"Copy from PyTorch since internal function is not importable\n    See ``nn/modules/utils.py:6``\n    \"\"\"\n\n    def parse(x):\n        if isinstance(x, collections.abc.Iterable):\n            return tuple(x)\n        return tuple(repeat(x, n))\n\n    return parse\n\n\n_pair = _ntuple(2)\nFooter",
  "history_output" : "Traceback (most recent call last):\n  File \"eddynet.py\", line 173, in <module>\n    Footer\nNameError: name 'Footer' is not defined\n",
  "history_begin_time" : 1664324441660,
  "history_end_time" : 1664324446144,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "jab02srhhhn",
  "history_input" : "#Eddynet\nimport collections\nfrom itertools import repeat\nfrom typing import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass EddyNet(nn.Module):\n    \"\"\"\n    PyTorch implementation of EddyNet from Lguensat et al. (2018)\n    Original implementation in TensorFlow: https://github.com/redouanelg/EddyNet\n    \"\"\"\n    def __init__(self, num_classes, num_filters, kernel_size):\n        super(EddyNet, self).__init__()\n        # encoder\n        self.encoder1 = EddyNet._block(1, num_filters, kernel_size, \"enc1\", dropout=0.2)\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder2 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc2\", dropout=0.3\n        )\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder3 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc3\", dropout=0.4\n        )\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder4 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc4\", dropout=0.5\n        )\n\n        # decoder\n        self.decoder3 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec3\", dropout=0.4\n        )\n        self.decoder2 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec2\", dropout=0.3\n        )\n        self.decoder1 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec1\", dropout=0.2\n        )\n\n        # final layer\n        self.final_conv = nn.Conv2d(\n            num_filters, num_classes, kernel_size=1, padding=0, bias=False\n        )\n\n    @staticmethod\n    def conv_block(in_channels, out_channels, kernel_size, name, num, dropout=0):\n        layers = {\n            f\"{name}_conv{num}\": Conv2dSame(in_channels, out_channels, kernel_size),\n            f\"{name}_bn{num}\": nn.BatchNorm2d(out_channels),\n            f\"{name}_relu{num}\": nn.ReLU(inplace=True),\n        }\n        if dropout > 0:\n            layers[f\"{name}_dropout\"] = nn.Dropout(p=dropout)\n\n        return nn.Sequential(OrderedDict(layers))\n\n    @staticmethod\n    def _block(in_channels, out_channels, kernel_size, name, dropout=0):\n        conv1 = EddyNet.conv_block(in_channels, out_channels, kernel_size, name, 1)\n        conv2 = EddyNet.conv_block(\n            out_channels, out_channels, kernel_size, name, 2, dropout=dropout\n        )\n        return nn.Sequential(conv1, conv2)\n\n    @staticmethod\n    def decoder_block(in_channels, out_channels, kernel_size, name, dropout=0):\n        return EddyNet._block(in_channels, out_channels, kernel_size, name, dropout)\n\n    def forward(self, x):\n        # encoder\n        enc1 = self.encoder1(x)\n        pool1 = self.pool1(enc1)\n\n        enc2 = self.encoder2(pool1)\n        pool2 = self.pool2(enc2)\n\n        enc3 = self.encoder3(pool2)\n        pool3 = self.pool3(enc3)\n\n        # bottleneck?\n        enc4 = self.encoder4(pool3)\n\n        # decoder\n        dec3 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(enc4)\n        dec3 = torch.cat((dec3, enc3), dim=1)\n        dec3 = self.decoder3(dec3)\n\n        dec2 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec3)\n        dec2 = torch.cat((dec2, enc2), dim=1)\n        dec2 = self.decoder2(dec2)\n\n        dec1 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec2)\n        dec1 = torch.cat((dec1, enc1), dim=1)\n        dec1 = self.decoder1(dec1)\n\n        # final layer\n        final = self.final_conv(dec1)\n\n        # softmax\n        final = nn.Softmax(dim=1)(final)\n\n        return final\n\n\nclass Conv2dSame(nn.Module):\n    \"\"\"Manual convolution with same padding\n    https://discuss.pytorch.org/t/same-padding-equivalent-in-pytorch/85121/9\n    Although PyTorch >= 1.10.0 supports ``padding='same'`` as a keyword\n    argument, this does not export to CoreML as of coremltools 5.1.0,\n    so we need to implement the internal torch logic manually.\n    Currently the ``RuntimeError`` is\n    \"PyTorch convert function for op '_convolution_mode' not implemented\"\n    \"\"\"\n\n    def __init__(\n        self, in_channels, out_channels, kernel_size, stride=1, dilation=1, **kwargs\n    ):\n        \"\"\"Wrap base convolution layer\n        See official PyTorch documentation for parameter details\n        https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n        \"\"\"\n        super().__init__()\n        self.conv = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            dilation=dilation,\n            **kwargs,\n        )\n\n        # Setup internal representations\n        kernel_size_ = _pair(kernel_size)\n        dilation_ = _pair(dilation)\n        self._reversed_padding_repeated_twice = [0, 0] * len(kernel_size_)\n\n        # Follow the logic from ``nn/modules/conv.py:_ConvNd``\n        for d, k, i in zip(\n            dilation_, kernel_size_, range(len(kernel_size_) - 1, -1, -1)\n        ):\n            total_padding = d * (k - 1)\n            left_pad = total_padding // 2\n            self._reversed_padding_repeated_twice[2 * i] = left_pad\n            self._reversed_padding_repeated_twice[2 * i + 1] = total_padding - left_pad\n\n    def forward(self, imgs):\n        \"\"\"Setup padding so same spatial dimensions are returned\n        All shapes (input/output) are ``(N, C, W, H)`` convention\n        :param torch.Tensor imgs:\n        :return torch.Tensor:\n        \"\"\"\n        padded = F.pad(imgs, self._reversed_padding_repeated_twice)\n        return self.conv(padded)\n\n\ndef _ntuple(n):\n    \"\"\"Copy from PyTorch since internal function is not importable\n    See ``nn/modules/utils.py:6``\n    \"\"\"\n\n    def parse(x):\n        if isinstance(x, collections.abc.Iterable):\n            return tuple(x)\n        return tuple(repeat(x, n))\n\n    return parse\n\n\n_pair = _ntuple(2)\nFooter",
  "history_output" : "Traceback (most recent call last):\n  File \"eddynet.py\", line 173, in <module>\n    Footer\nNameError: name 'Footer' is not defined\n",
  "history_begin_time" : 1664324423467,
  "history_end_time" : 1664324427740,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "uatqci38ha2",
  "history_input" : "#Eddynet\nimport collections\nfrom itertools import repeat\nfrom typing import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass EddyNet(nn.Module):\n    \"\"\"\n    PyTorch implementation of EddyNet from Lguensat et al. (2018)\n    Original implementation in TensorFlow: https://github.com/redouanelg/EddyNet\n    \"\"\"\n    def __init__(self, num_classes, num_filters, kernel_size):\n        super(EddyNet, self).__init__()\n        # encoder\n        self.encoder1 = EddyNet._block(1, num_filters, kernel_size, \"enc1\", dropout=0.2)\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder2 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc2\", dropout=0.3\n        )\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder3 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc3\", dropout=0.4\n        )\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder4 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc4\", dropout=0.5\n        )\n\n        # decoder\n        self.decoder3 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec3\", dropout=0.4\n        )\n        self.decoder2 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec2\", dropout=0.3\n        )\n        self.decoder1 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec1\", dropout=0.2\n        )\n\n        # final layer\n        self.final_conv = nn.Conv2d(\n            num_filters, num_classes, kernel_size=1, padding=0, bias=False\n        )\n\n    @staticmethod\n    def conv_block(in_channels, out_channels, kernel_size, name, num, dropout=0):\n        layers = {\n            f\"{name}_conv{num}\": Conv2dSame(in_channels, out_channels, kernel_size),\n            f\"{name}_bn{num}\": nn.BatchNorm2d(out_channels),\n            f\"{name}_relu{num}\": nn.ReLU(inplace=True),\n        }\n        if dropout > 0:\n            layers[f\"{name}_dropout\"] = nn.Dropout(p=dropout)\n\n        return nn.Sequential(OrderedDict(layers))\n\n    @staticmethod\n    def _block(in_channels, out_channels, kernel_size, name, dropout=0):\n        conv1 = EddyNet.conv_block(in_channels, out_channels, kernel_size, name, 1)\n        conv2 = EddyNet.conv_block(\n            out_channels, out_channels, kernel_size, name, 2, dropout=dropout\n        )\n        return nn.Sequential(conv1, conv2)\n\n    @staticmethod\n    def decoder_block(in_channels, out_channels, kernel_size, name, dropout=0):\n        return EddyNet._block(in_channels, out_channels, kernel_size, name, dropout)\n\n    def forward(self, x):\n        # encoder\n        enc1 = self.encoder1(x)\n        pool1 = self.pool1(enc1)\n\n        enc2 = self.encoder2(pool1)\n        pool2 = self.pool2(enc2)\n\n        enc3 = self.encoder3(pool2)\n        pool3 = self.pool3(enc3)\n\n        # bottleneck?\n        enc4 = self.encoder4(pool3)\n\n        # decoder\n        dec3 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(enc4)\n        dec3 = torch.cat((dec3, enc3), dim=1)\n        dec3 = self.decoder3(dec3)\n\n        dec2 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec3)\n        dec2 = torch.cat((dec2, enc2), dim=1)\n        dec2 = self.decoder2(dec2)\n\n        dec1 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec2)\n        dec1 = torch.cat((dec1, enc1), dim=1)\n        dec1 = self.decoder1(dec1)\n\n        # final layer\n        final = self.final_conv(dec1)\n\n        # softmax\n        final = nn.Softmax(dim=1)(final)\n\n        return final\n\n\nclass Conv2dSame(nn.Module):\n    \"\"\"Manual convolution with same padding\n    https://discuss.pytorch.org/t/same-padding-equivalent-in-pytorch/85121/9\n    Although PyTorch >= 1.10.0 supports ``padding='same'`` as a keyword\n    argument, this does not export to CoreML as of coremltools 5.1.0,\n    so we need to implement the internal torch logic manually.\n    Currently the ``RuntimeError`` is\n    \"PyTorch convert function for op '_convolution_mode' not implemented\"\n    \"\"\"\n\n    def __init__(\n        self, in_channels, out_channels, kernel_size, stride=1, dilation=1, **kwargs\n    ):\n        \"\"\"Wrap base convolution layer\n        See official PyTorch documentation for parameter details\n        https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n        \"\"\"\n        super().__init__()\n        self.conv = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            dilation=dilation,\n            **kwargs,\n        )\n\n        # Setup internal representations\n        kernel_size_ = _pair(kernel_size)\n        dilation_ = _pair(dilation)\n        self._reversed_padding_repeated_twice = [0, 0] * len(kernel_size_)\n\n        # Follow the logic from ``nn/modules/conv.py:_ConvNd``\n        for d, k, i in zip(\n            dilation_, kernel_size_, range(len(kernel_size_) - 1, -1, -1)\n        ):\n            total_padding = d * (k - 1)\n            left_pad = total_padding // 2\n            self._reversed_padding_repeated_twice[2 * i] = left_pad\n            self._reversed_padding_repeated_twice[2 * i + 1] = total_padding - left_pad\n\n    def forward(self, imgs):\n        \"\"\"Setup padding so same spatial dimensions are returned\n        All shapes (input/output) are ``(N, C, W, H)`` convention\n        :param torch.Tensor imgs:\n        :return torch.Tensor:\n        \"\"\"\n        padded = F.pad(imgs, self._reversed_padding_repeated_twice)\n        return self.conv(padded)\n\n\ndef _ntuple(n):\n    \"\"\"Copy from PyTorch since internal function is not importable\n    See ``nn/modules/utils.py:6``\n    \"\"\"\n\n    def parse(x):\n        if isinstance(x, collections.abc.Iterable):\n            return tuple(x)\n        return tuple(repeat(x, n))\n\n    return parse\n\n\n_pair = _ntuple(2)\nFooter",
  "history_output" : "",
  "history_begin_time" : 1664324289196,
  "history_end_time" : 1664324291480,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Failed"
},{
  "history_id" : "r0biq5rhsep",
  "history_input" : "#Eddynet\nimport collections\nfrom itertools import repeat\nfrom typing import OrderedDict\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass EddyNet(nn.Module):\n    \"\"\"\n    PyTorch implementation of EddyNet from Lguensat et al. (2018)\n    Original implementation in TensorFlow: https://github.com/redouanelg/EddyNet\n    \"\"\"\n    def __init__(self, num_classes, num_filters, kernel_size):\n        super(EddyNet, self).__init__()\n        # encoder\n        self.encoder1 = EddyNet._block(1, num_filters, kernel_size, \"enc1\", dropout=0.2)\n        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder2 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc2\", dropout=0.3\n        )\n        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder3 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc3\", dropout=0.4\n        )\n        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.encoder4 = EddyNet._block(\n            num_filters, num_filters, kernel_size, \"enc4\", dropout=0.5\n        )\n\n        # decoder\n        self.decoder3 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec3\", dropout=0.4\n        )\n        self.decoder2 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec2\", dropout=0.3\n        )\n        self.decoder1 = EddyNet.decoder_block(\n            num_filters * 2, num_filters, kernel_size, \"dec1\", dropout=0.2\n        )\n\n        # final layer\n        self.final_conv = nn.Conv2d(\n            num_filters, num_classes, kernel_size=1, padding=0, bias=False\n        )\n\n    @staticmethod\n    def conv_block(in_channels, out_channels, kernel_size, name, num, dropout=0):\n        layers = {\n            f\"{name}_conv{num}\": Conv2dSame(in_channels, out_channels, kernel_size),\n            f\"{name}_bn{num}\": nn.BatchNorm2d(out_channels),\n            f\"{name}_relu{num}\": nn.ReLU(inplace=True),\n        }\n        if dropout > 0:\n            layers[f\"{name}_dropout\"] = nn.Dropout(p=dropout)\n\n        return nn.Sequential(OrderedDict(layers))\n\n    @staticmethod\n    def _block(in_channels, out_channels, kernel_size, name, dropout=0):\n        conv1 = EddyNet.conv_block(in_channels, out_channels, kernel_size, name, 1)\n        conv2 = EddyNet.conv_block(\n            out_channels, out_channels, kernel_size, name, 2, dropout=dropout\n        )\n        return nn.Sequential(conv1, conv2)\n\n    @staticmethod\n    def decoder_block(in_channels, out_channels, kernel_size, name, dropout=0):\n        return EddyNet._block(in_channels, out_channels, kernel_size, name, dropout)\n\n    def forward(self, x):\n        # encoder\n        enc1 = self.encoder1(x)\n        pool1 = self.pool1(enc1)\n\n        enc2 = self.encoder2(pool1)\n        pool2 = self.pool2(enc2)\n\n        enc3 = self.encoder3(pool2)\n        pool3 = self.pool3(enc3)\n\n        # bottleneck?\n        enc4 = self.encoder4(pool3)\n\n        # decoder\n        dec3 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(enc4)\n        dec3 = torch.cat((dec3, enc3), dim=1)\n        dec3 = self.decoder3(dec3)\n\n        dec2 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec3)\n        dec2 = torch.cat((dec2, enc2), dim=1)\n        dec2 = self.decoder2(dec2)\n\n        dec1 = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=False)(dec2)\n        dec1 = torch.cat((dec1, enc1), dim=1)\n        dec1 = self.decoder1(dec1)\n\n        # final layer\n        final = self.final_conv(dec1)\n\n        # softmax\n        final = nn.Softmax(dim=1)(final)\n\n        return final\n\n\nclass Conv2dSame(nn.Module):\n    \"\"\"Manual convolution with same padding\n    https://discuss.pytorch.org/t/same-padding-equivalent-in-pytorch/85121/9\n    Although PyTorch >= 1.10.0 supports ``padding='same'`` as a keyword\n    argument, this does not export to CoreML as of coremltools 5.1.0,\n    so we need to implement the internal torch logic manually.\n    Currently the ``RuntimeError`` is\n    \"PyTorch convert function for op '_convolution_mode' not implemented\"\n    \"\"\"\n\n    def __init__(\n        self, in_channels, out_channels, kernel_size, stride=1, dilation=1, **kwargs\n    ):\n        \"\"\"Wrap base convolution layer\n        See official PyTorch documentation for parameter details\n        https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html\n        \"\"\"\n        super().__init__()\n        self.conv = nn.Conv2d(\n            in_channels=in_channels,\n            out_channels=out_channels,\n            kernel_size=kernel_size,\n            stride=stride,\n            dilation=dilation,\n            **kwargs,\n        )\n\n        # Setup internal representations\n        kernel_size_ = _pair(kernel_size)\n        dilation_ = _pair(dilation)\n        self._reversed_padding_repeated_twice = [0, 0] * len(kernel_size_)\n\n        # Follow the logic from ``nn/modules/conv.py:_ConvNd``\n        for d, k, i in zip(\n            dilation_, kernel_size_, range(len(kernel_size_) - 1, -1, -1)\n        ):\n            total_padding = d * (k - 1)\n            left_pad = total_padding // 2\n            self._reversed_padding_repeated_twice[2 * i] = left_pad\n            self._reversed_padding_repeated_twice[2 * i + 1] = total_padding - left_pad\n\n    def forward(self, imgs):\n        \"\"\"Setup padding so same spatial dimensions are returned\n        All shapes (input/output) are ``(N, C, W, H)`` convention\n        :param torch.Tensor imgs:\n        :return torch.Tensor:\n        \"\"\"\n        padded = F.pad(imgs, self._reversed_padding_repeated_twice)\n        return self.conv(padded)\n\n\ndef _ntuple(n):\n    \"\"\"Copy from PyTorch since internal function is not importable\n    See ``nn/modules/utils.py:6``\n    \"\"\"\n\n    def parse(x):\n        if isinstance(x, collections.abc.Iterable):\n            return tuple(x)\n        return tuple(repeat(x, n))\n\n    return parse\n\n\n_pair = _ntuple(2)\nFooter",
  "history_output" : "Traceback (most recent call last):\n  File \"eddynet.py\", line 173, in <module>\n    Footer\nNameError: name 'Footer' is not defined\n",
  "history_begin_time" : 1664324192177,
  "history_end_time" : 1664324237743,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "iqf65am2yao",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1664324597741,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "3sa33591m1e",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1664325180368,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Stopped"
},{
  "history_id" : "2lwrsl5j8wy",
  "history_input" : null,
  "history_output" : null,
  "history_begin_time" : null,
  "history_end_time" : 1664372085473,
  "history_notes" : null,
  "history_process" : "bzgeyy",
  "host_id" : "100001",
  "indicator" : "Stopped"
},]
